{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-12 10:19:05.0000001</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2015-04-12 10:19:05 UTC</td>\n",
       "      <td>-73.979279</td>\n",
       "      <td>40.723438</td>\n",
       "      <td>-74.004608</td>\n",
       "      <td>40.746948</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-05 17:39:00.00000040</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2011-01-05 17:39:00 UTC</td>\n",
       "      <td>-73.966957</td>\n",
       "      <td>40.761268</td>\n",
       "      <td>-73.967912</td>\n",
       "      <td>40.765535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-17 04:22:00.0000006</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2013-09-17 04:22:00 UTC</td>\n",
       "      <td>-73.987210</td>\n",
       "      <td>40.729325</td>\n",
       "      <td>-73.931985</td>\n",
       "      <td>40.697207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-09 22:21:25.0000001</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2011-03-09 22:21:25 UTC</td>\n",
       "      <td>-73.977829</td>\n",
       "      <td>40.788979</td>\n",
       "      <td>-73.967935</td>\n",
       "      <td>40.760508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-10-16 10:32:00.000000191</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2009-10-16 10:32:00 UTC</td>\n",
       "      <td>-73.990575</td>\n",
       "      <td>40.746117</td>\n",
       "      <td>-74.003227</td>\n",
       "      <td>40.751447</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0    2015-04-12 10:19:05.0000001         11.5  2015-04-12 10:19:05 UTC   \n",
       "1   2011-01-05 17:39:00.00000040          3.7  2011-01-05 17:39:00 UTC   \n",
       "2    2013-09-17 04:22:00.0000006         19.0  2013-09-17 04:22:00 UTC   \n",
       "3    2011-03-09 22:21:25.0000001          9.7  2011-03-09 22:21:25 UTC   \n",
       "4  2009-10-16 10:32:00.000000191          7.3  2009-10-16 10:32:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.979279        40.723438         -74.004608         40.746948   \n",
       "1        -73.966957        40.761268         -73.967912         40.765535   \n",
       "2        -73.987210        40.729325         -73.931985         40.697207   \n",
       "3        -73.977829        40.788979         -73.967935         40.760508   \n",
       "4        -73.990575        40.746117         -74.003227         40.751447   \n",
       "\n",
       "   passenger_count  \n",
       "0                6  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "skiprows = np.random.rand(55 * 10 ** 7) > 0.02\n",
    "skiprows[0] = False\n",
    "df = pd.read_csv('train.csv', skiprows=lambda x: skiprows[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1108485, 8)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.973320</td>\n",
       "      <td>40.763805</td>\n",
       "      <td>-73.981430</td>\n",
       "      <td>40.743835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.986862</td>\n",
       "      <td>40.719383</td>\n",
       "      <td>-73.998886</td>\n",
       "      <td>40.739201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>2011-10-08 11:53:44 UTC</td>\n",
       "      <td>-73.982524</td>\n",
       "      <td>40.751260</td>\n",
       "      <td>-73.979654</td>\n",
       "      <td>40.746139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.981160</td>\n",
       "      <td>40.767807</td>\n",
       "      <td>-73.990448</td>\n",
       "      <td>40.751635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.966046</td>\n",
       "      <td>40.789775</td>\n",
       "      <td>-73.988565</td>\n",
       "      <td>40.744427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key          pickup_datetime  pickup_longitude  \\\n",
       "0  2015-01-27 13:08:24.0000002  2015-01-27 13:08:24 UTC        -73.973320   \n",
       "1  2015-01-27 13:08:24.0000003  2015-01-27 13:08:24 UTC        -73.986862   \n",
       "2  2011-10-08 11:53:44.0000002  2011-10-08 11:53:44 UTC        -73.982524   \n",
       "3  2012-12-01 21:12:12.0000002  2012-12-01 21:12:12 UTC        -73.981160   \n",
       "4  2012-12-01 21:12:12.0000003  2012-12-01 21:12:12 UTC        -73.966046   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.763805         -73.981430         40.743835                1  \n",
       "1        40.719383         -73.998886         40.739201                1  \n",
       "2        40.751260         -73.979654         40.746139                1  \n",
       "3        40.767807         -73.990448         40.751635                1  \n",
       "4        40.789775         -73.988565         40.744427                1  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],utc=True)\n",
    "df1['pickup_datetime'] = pd.to_datetime(df1['pickup_datetime'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,2:]\n",
    "Y_train = df['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df1.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['pickup_datetime'][3].dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting holidays\n",
      "  Downloading holidays-0.10.5.2.tar.gz (121 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from holidays) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from holidays) (1.15.0)\n",
      "Collecting convertdate>=2.3.0\n",
      "  Downloading convertdate-2.3.2-py3-none-any.whl (47 kB)\n",
      "Collecting korean_lunar_calendar\n",
      "  Downloading korean_lunar_calendar-0.2.1-py3-none-any.whl (8.0 kB)\n",
      "Collecting hijri_converter\n",
      "  Downloading hijri_converter-2.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pytz>=2014.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from convertdate>=2.3.0->holidays) (2020.1)\n",
      "Collecting pymeeus<=1,>=0.3.13\n",
      "  Downloading PyMeeus-0.5.11.tar.gz (5.4 MB)\n",
      "Building wheels for collected packages: holidays, pymeeus\n",
      "  Building wheel for holidays (setup.py): started\n",
      "  Building wheel for holidays (setup.py): finished with status 'done'\n",
      "  Created wheel for holidays: filename=holidays-0.10.5.2-py3-none-any.whl size=126818 sha256=e3b6c605924bd9e3e5b8e0100aef14af70b13b0c4d7f0a8124b9e6255af180fe\n",
      "  Stored in directory: c:\\users\\rahul\\appdata\\local\\pip\\cache\\wheels\\6a\\cf\\7a\\3ffc6ba5930fd024335b4a12b0be8bc3ba5a8fa63ca9a849ef\n",
      "  Building wheel for pymeeus (setup.py): started\n",
      "  Building wheel for pymeeus (setup.py): finished with status 'done'\n",
      "  Created wheel for pymeeus: filename=PyMeeus-0.5.11-py3-none-any.whl size=730977 sha256=c2d11d38dac7511f48f748ccc2588babc0ab35494a536e7ad9435ed5665da52d\n",
      "  Stored in directory: c:\\users\\rahul\\appdata\\local\\pip\\cache\\wheels\\a0\\8b\\b2\\810ae5a6f970c8be4725353400d643c90de1c0f023a9884ee7\n",
      "Successfully built holidays pymeeus\n",
      "Installing collected packages: pymeeus, convertdate, korean-lunar-calendar, hijri-converter, holidays\n",
      "Successfully installed convertdate-2.3.2 hijri-converter-2.1.1 holidays-0.10.5.2 korean-lunar-calendar-0.2.1 pymeeus-0.5.11\n"
     ]
    }
   ],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haversine\n",
      "  Downloading haversine-2.3.0-py2.py3-none-any.whl (5.5 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/1110767 [00:00<?, ?it/s]<ipython-input-188-5c51f4faf8d5>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['distance_travelled'][i] = hs.haversine(df['pickup_location'][i],df['dropoff_location'][i])\n",
      "<ipython-input-188-5c51f4faf8d5>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_a_holiday'][i] = df['pickup_datetime'][i] in us_holidays\n",
      "  0%|▏                                                                       | 2566/1110767 [00:16<2:01:06, 152.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-5c51f4faf8d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pickup_location'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickup_latitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickup_longitude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropoff_location'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropoff_latitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropoff_longitude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'distance_travelled'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaversine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pickup_location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropoff_location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_a_holiday'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pickup_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mus_holidays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                         \u001b[1;31m# If no `miniters` was specified, adjust automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1336\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import haversine as hs\n",
    "from tqdm import tqdm\n",
    "df['distance_travelled'] = None\n",
    "df['pickup_location'] = list(zip(df.pickup_latitude, df.pickup_longitude))\n",
    "df['dropoff_location'] = list(zip(df.dropoff_latitude, df.dropoff_longitude))\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    df['distance_travelled'][i] = hs.haversine(df['pickup_location'][i],df['dropoff_location'][i])\n",
    "    df['is_a_holiday'][i] = df['pickup_datetime'][i] in us_holidays\n",
    "df['is_a_holiday'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haversine as hs\n",
    "from tqdm import tqdm\n",
    "df['distance'] = list(zip(list(zip(df.pickup_latitude, df.pickup_longitude)),list(zip(df.dropoff_latitude, df.dropoff_longitude))))\n",
    "df['distance_travelled'] = df['distance'].apply(lambda x: hs.haversine(x[0],x[1]))\n",
    "df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "df['month'] = df['pickup_datetime'].apply(lambda x:x.month)\n",
    "df['day'] = df['pickup_datetime'].apply(lambda x: x.dayofweek)\n",
    "df[\"befor_shock\"] = ((df[\"year\"] <= 2011) | ((df[\"year\"] <= 2012) & (df[\"month\"] <= 8))).apply(int)\n",
    "df['is_a_holiday'] = (df['pickup_datetime'].apply(lambda x: x in us_holidays)).apply(int)\n",
    "df['passenger_count'] = (df['passenger_count']<5).apply(int)\n",
    "df[\"time\"] = df[\"pickup_datetime\"].apply(lambda x: x.hour * 60 + x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['distance'] = list(zip(list(zip(df1.pickup_latitude, df1.pickup_longitude)),list(zip(df1.dropoff_latitude, df1.dropoff_longitude))))\n",
    "df1['distance_travelled'] = df1['distance'].apply(lambda x: hs.haversine(x[0],x[1]))\n",
    "df1['year'] = df1['pickup_datetime'].apply(lambda x: x.year)\n",
    "df1['month'] = df1['pickup_datetime'].apply(lambda x:x.month)\n",
    "df1['day'] = df1['pickup_datetime'].apply(lambda x: x.dayofweek)\n",
    "df1[\"befor_shock\"] = ((df1[\"year\"] <= 2011) | ((df1[\"year\"] <= 2012) & (df1[\"month\"] <= 8))).apply(int)\n",
    "df1['is_a_holiday'] = (df1['pickup_datetime'].apply(lambda x: x in us_holidays)).apply(int)\n",
    "df1['passenger_count'] = (df1['passenger_count']<5).apply(int)\n",
    "df1[\"time\"] = df1[\"pickup_datetime\"].apply(lambda x: x.hour * 60 + x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>distance</th>\n",
       "      <th>distance_travelled</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>befor_shock</th>\n",
       "      <th>is_a_holiday</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-12 10:19:05.0000001</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2015-04-12 10:19:05+00:00</td>\n",
       "      <td>-73.979279</td>\n",
       "      <td>40.723438</td>\n",
       "      <td>-74.004608</td>\n",
       "      <td>40.746948</td>\n",
       "      <td>0</td>\n",
       "      <td>((40.72343826293945, -73.97927856445312), (40....</td>\n",
       "      <td>3.374718</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-05 17:39:00.00000040</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2011-01-05 17:39:00+00:00</td>\n",
       "      <td>-73.966957</td>\n",
       "      <td>40.761268</td>\n",
       "      <td>-73.967912</td>\n",
       "      <td>40.765535</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.761268, -73.966957), (40.765535, -73.9679...</td>\n",
       "      <td>0.481238</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-17 04:22:00.0000006</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2013-09-17 04:22:00+00:00</td>\n",
       "      <td>-73.987210</td>\n",
       "      <td>40.729325</td>\n",
       "      <td>-73.931985</td>\n",
       "      <td>40.697207</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.729325, -73.98721), (40.697207, -73.931985))</td>\n",
       "      <td>5.866839</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-09 22:21:25.0000001</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2011-03-09 22:21:25+00:00</td>\n",
       "      <td>-73.977829</td>\n",
       "      <td>40.788979</td>\n",
       "      <td>-73.967935</td>\n",
       "      <td>40.760508</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.788979, -73.977829), (40.760508, -73.9679...</td>\n",
       "      <td>3.273626</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-10-16 10:32:00.000000191</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2009-10-16 10:32:00+00:00</td>\n",
       "      <td>-73.990575</td>\n",
       "      <td>40.746117</td>\n",
       "      <td>-74.003227</td>\n",
       "      <td>40.751447</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.746117, -73.990575), (40.751447, -74.0032...</td>\n",
       "      <td>1.219496</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108480</th>\n",
       "      <td>2009-04-30 21:37:00.00000040</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2009-04-30 21:37:00+00:00</td>\n",
       "      <td>-73.997763</td>\n",
       "      <td>40.741155</td>\n",
       "      <td>-73.984755</td>\n",
       "      <td>40.717655</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.741155, -73.997763), (40.717655, -73.9847...</td>\n",
       "      <td>2.833663</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108481</th>\n",
       "      <td>2012-08-03 23:31:00.000000184</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2012-08-03 23:31:00+00:00</td>\n",
       "      <td>-73.967117</td>\n",
       "      <td>40.756930</td>\n",
       "      <td>-73.973147</td>\n",
       "      <td>40.755955</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.75693, -73.967117), (40.755955, -73.973147))</td>\n",
       "      <td>0.519345</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108482</th>\n",
       "      <td>2011-02-20 14:03:00.00000051</td>\n",
       "      <td>15.3</td>\n",
       "      <td>2011-02-20 14:03:00+00:00</td>\n",
       "      <td>-73.991902</td>\n",
       "      <td>40.738360</td>\n",
       "      <td>-73.967693</td>\n",
       "      <td>40.792793</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.73836, -73.991902), (40.792792999999996, ...</td>\n",
       "      <td>6.386844</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108483</th>\n",
       "      <td>2011-08-06 18:36:47.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2011-08-06 18:36:47+00:00</td>\n",
       "      <td>-73.992143</td>\n",
       "      <td>40.770590</td>\n",
       "      <td>-73.988171</td>\n",
       "      <td>40.758195</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.770590000000006, -73.992143), (40.758195,...</td>\n",
       "      <td>1.418278</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108484</th>\n",
       "      <td>2011-04-02 22:04:24.0000004</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2011-04-02 22:04:24+00:00</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.752325, -73.970505), (40.797342, -73.9605...</td>\n",
       "      <td>5.075555</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1108485 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  fare_amount           pickup_datetime  \\\n",
       "0          2015-04-12 10:19:05.0000001         11.5 2015-04-12 10:19:05+00:00   \n",
       "1         2011-01-05 17:39:00.00000040          3.7 2011-01-05 17:39:00+00:00   \n",
       "2          2013-09-17 04:22:00.0000006         19.0 2013-09-17 04:22:00+00:00   \n",
       "3          2011-03-09 22:21:25.0000001          9.7 2011-03-09 22:21:25+00:00   \n",
       "4        2009-10-16 10:32:00.000000191          7.3 2009-10-16 10:32:00+00:00   \n",
       "...                                ...          ...                       ...   \n",
       "1108480   2009-04-30 21:37:00.00000040         10.5 2009-04-30 21:37:00+00:00   \n",
       "1108481  2012-08-03 23:31:00.000000184          3.3 2012-08-03 23:31:00+00:00   \n",
       "1108482   2011-02-20 14:03:00.00000051         15.3 2011-02-20 14:03:00+00:00   \n",
       "1108483    2011-08-06 18:36:47.0000001          7.7 2011-08-06 18:36:47+00:00   \n",
       "1108484    2011-04-02 22:04:24.0000004         14.1 2011-04-02 22:04:24+00:00   \n",
       "\n",
       "         pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0              -73.979279        40.723438         -74.004608   \n",
       "1              -73.966957        40.761268         -73.967912   \n",
       "2              -73.987210        40.729325         -73.931985   \n",
       "3              -73.977829        40.788979         -73.967935   \n",
       "4              -73.990575        40.746117         -74.003227   \n",
       "...                   ...              ...                ...   \n",
       "1108480        -73.997763        40.741155         -73.984755   \n",
       "1108481        -73.967117        40.756930         -73.973147   \n",
       "1108482        -73.991902        40.738360         -73.967693   \n",
       "1108483        -73.992143        40.770590         -73.988171   \n",
       "1108484        -73.970505        40.752325         -73.960537   \n",
       "\n",
       "         dropoff_latitude  passenger_count  \\\n",
       "0               40.746948                0   \n",
       "1               40.765535                1   \n",
       "2               40.697207                1   \n",
       "3               40.760508                1   \n",
       "4               40.751447                1   \n",
       "...                   ...              ...   \n",
       "1108480         40.717655                1   \n",
       "1108481         40.755955                1   \n",
       "1108482         40.792793                1   \n",
       "1108483         40.758195                1   \n",
       "1108484         40.797342                1   \n",
       "\n",
       "                                                  distance  \\\n",
       "0        ((40.72343826293945, -73.97927856445312), (40....   \n",
       "1        ((40.761268, -73.966957), (40.765535, -73.9679...   \n",
       "2        ((40.729325, -73.98721), (40.697207, -73.931985))   \n",
       "3        ((40.788979, -73.977829), (40.760508, -73.9679...   \n",
       "4        ((40.746117, -73.990575), (40.751447, -74.0032...   \n",
       "...                                                    ...   \n",
       "1108480  ((40.741155, -73.997763), (40.717655, -73.9847...   \n",
       "1108481  ((40.75693, -73.967117), (40.755955, -73.973147))   \n",
       "1108482  ((40.73836, -73.991902), (40.792792999999996, ...   \n",
       "1108483  ((40.770590000000006, -73.992143), (40.758195,...   \n",
       "1108484  ((40.752325, -73.970505), (40.797342, -73.9605...   \n",
       "\n",
       "         distance_travelled  year  month  day  befor_shock  is_a_holiday  time  \n",
       "0                  3.374718  2015      4    6            0             0   619  \n",
       "1                  0.481238  2011      1    2            1             0  1059  \n",
       "2                  5.866839  2013      9    1            0             0   262  \n",
       "3                  3.273626  2011      3    2            1             0  1341  \n",
       "4                  1.219496  2009     10    4            1             0   632  \n",
       "...                     ...   ...    ...  ...          ...           ...   ...  \n",
       "1108480            2.833663  2009      4    3            1             0  1297  \n",
       "1108481            0.519345  2012      8    4            1             0  1411  \n",
       "1108482            6.386844  2011      2    6            1             0   843  \n",
       "1108483            1.418278  2011      8    5            1             0  1116  \n",
       "1108484            5.075555  2011      4    5            1             0  1324  \n",
       "\n",
       "[1108485 rows x 16 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dummy=pd.get_dummies(df['day'])\n",
    "\n",
    "#df=pd.concat([df,train_dummy],axis=1)\n",
    "#holiday_dummy = pd.get_dummies(df['is_a_holiday'])\n",
    "#passenger_dummy = pd.get_dummies(df['passeger_count'])\n",
    "#df=pd.concat([df,day_dummy,holiday_dummy, passenger_dummy],axis=1)\n",
    "df=pd.concat([df,day_dummy],axis=1)\n",
    "\n",
    "day1_dummy=pd.get_dummies(df1['day'])\n",
    "#df=pd.concat([df,train_dummy],axis=1)\n",
    "#holiday_dummy = pd.get_dummies(df['is_a_holiday'])\n",
    "#passenger_dummy = pd.get_dummies(df['passeger_count'])\n",
    "#df=pd.concat([df,day_dummy,holiday_dummy, passenger_dummy],axis=1)\n",
    "df1=pd.concat([df1,day1_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 16 columns before encoding categorical features\n",
      "There are 42 columns after encoding categorical features\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "        #drop the encoded column\n",
    "        df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "    \n",
    "cat_cols = ['is_a_holiday','befor_shock','day','month','year','passenger_count']\n",
    "print('There were {} columns before encoding categorical features'.format(df.shape[1]))\n",
    "df = oneHotEncode(df, cat_cols)\n",
    "df1 = oneHotEncode(df1, cat_cols)\n",
    "print('There are {} columns after encoding categorical features'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns = ['key','fare_amount','distance','pickup_datetime'])\n",
    "y_train = df['fare_amount']\n",
    "X_test = df1.drop(columns = ['key','distance','pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          11.5\n",
       "1           3.7\n",
       "2          19.0\n",
       "3           9.7\n",
       "4           7.3\n",
       "           ... \n",
       "1108480    10.5\n",
       "1108481     3.3\n",
       "1108482    15.3\n",
       "1108483     7.7\n",
       "1108484    14.1\n",
       "Name: fare_amount, Length: 1108472, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = df1.iloc[:,-13:]\n",
    "dftest.drop(columns = ['day','dropoff_location','pickup_location'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(dftrain,df['fare_amount'])\n",
    "y_pred = regr.predict(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key  fare_amount\n",
       "0  2015-01-27 13:08:24.0000002        11.35\n",
       "1  2015-01-27 13:08:24.0000003        11.35\n",
       "2  2011-10-08 11:53:44.0000002        11.35\n",
       "3  2012-12-01 21:12:12.0000002        11.35\n",
       "4  2012-12-01 21:12:12.0000003        11.35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('sample_submission.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3711217078712763"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission=pd.DataFrame(data=y_pred,columns=['fare_amount'])\n",
    "Submission['key']=df1['key']\n",
    "Submission=Submission[['key','fare_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = Submission.set_index('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.to_csv('y1_knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-27 13:08:24.0000002</th>\n",
       "      <td>11.292572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-27 13:08:24.0000003</th>\n",
       "      <td>10.649523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-08 11:53:44.0000002</th>\n",
       "      <td>4.736473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-01 21:12:12.0000002</th>\n",
       "      <td>10.116229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-01 21:12:12.0000003</th>\n",
       "      <td>15.810888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             fare_amount\n",
       "key                                     \n",
       "2015-01-27 13:08:24.0000002    11.292572\n",
       "2015-01-27 13:08:24.0000003    10.649523\n",
       "2011-10-08 11:53:44.0000002     4.736473\n",
       "2012-12-01 21:12:12.0000002    10.116229\n",
       "2012-12-01 21:12:12.0000003    15.810888"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(max_depth=30, n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               4992      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 169,857\n",
      "Trainable params: 169,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0851 - mean_absolute_error: 2.0851\n",
      "Epoch 00001: val_loss improved from inf to 2.10555, saving model to Weights-001--2.10555.hdf5\n",
      "27712/27712 [==============================] - 36s 1ms/step - loss: 2.0851 - mean_absolute_error: 2.0851 - val_loss: 2.1055 - val_mean_absolute_error: 2.1055\n",
      "Epoch 2/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0852 - mean_absolute_error: 2.0852\n",
      "Epoch 00002: val_loss did not improve from 2.10555\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0852 - mean_absolute_error: 2.0852 - val_loss: 2.1553 - val_mean_absolute_error: 2.1553\n",
      "Epoch 3/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0866 - mean_absolute_error: 2.0866\n",
      "Epoch 00003: val_loss improved from 2.10555 to 2.09226, saving model to Weights-003--2.09226.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0866 - mean_absolute_error: 2.0866 - val_loss: 2.0923 - val_mean_absolute_error: 2.0923\n",
      "Epoch 4/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0896 - mean_absolute_error: 2.0896\n",
      "Epoch 00004: val_loss did not improve from 2.09226\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0898 - mean_absolute_error: 2.0898 - val_loss: 2.0979 - val_mean_absolute_error: 2.0979\n",
      "Epoch 5/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0881 - mean_absolute_error: 2.0881\n",
      "Epoch 00005: val_loss improved from 2.09226 to 2.08297, saving model to Weights-005--2.08297.hdf5\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0882 - mean_absolute_error: 2.0882 - val_loss: 2.0830 - val_mean_absolute_error: 2.0830\n",
      "Epoch 6/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0889 - mean_absolute_error: 2.0889\n",
      "Epoch 00006: val_loss did not improve from 2.08297\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0891 - mean_absolute_error: 2.0891 - val_loss: 2.0989 - val_mean_absolute_error: 2.0989\n",
      "Epoch 7/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0882 - mean_absolute_error: 2.0882\n",
      "Epoch 00007: val_loss improved from 2.08297 to 2.07841, saving model to Weights-007--2.07841.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0883 - mean_absolute_error: 2.0883 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n",
      "Epoch 8/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0866 - mean_absolute_error: 2.0866\n",
      "Epoch 00008: val_loss did not improve from 2.07841\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0867 - mean_absolute_error: 2.0867 - val_loss: 2.1217 - val_mean_absolute_error: 2.1217\n",
      "Epoch 9/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0819 - mean_absolute_error: 2.0819\n",
      "Epoch 00009: val_loss did not improve from 2.07841\n",
      "27712/27712 [==============================] - 42s 1ms/step - loss: 2.0817 - mean_absolute_error: 2.0817 - val_loss: 2.1020 - val_mean_absolute_error: 2.1020\n",
      "Epoch 10/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00010: val_loss improved from 2.07841 to 2.07691, saving model to Weights-010--2.07691.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 11/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00011: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0830 - mean_absolute_error: 2.0830 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 12/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0825 - mean_absolute_error: 2.0825\n",
      "Epoch 00012: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0824 - mean_absolute_error: 2.0824 - val_loss: 2.1270 - val_mean_absolute_error: 2.1270\n",
      "Epoch 13/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00013: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.1002 - val_mean_absolute_error: 2.1002\n",
      "Epoch 14/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0845 - mean_absolute_error: 2.0845\n",
      "Epoch 00014: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0845 - mean_absolute_error: 2.0845 - val_loss: 2.1035 - val_mean_absolute_error: 2.1035\n",
      "Epoch 15/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0853 - mean_absolute_error: 2.0853\n",
      "Epoch 00015: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0855 - mean_absolute_error: 2.0855 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 16/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00016: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0847 - mean_absolute_error: 2.0847 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 17/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0853 - mean_absolute_error: 2.0853\n",
      "Epoch 00017: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0855 - mean_absolute_error: 2.0855 - val_loss: 2.1032 - val_mean_absolute_error: 2.1032\n",
      "Epoch 18/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0871 - mean_absolute_error: 2.0871\n",
      "Epoch 00018: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0871 - mean_absolute_error: 2.0871 - val_loss: 2.1052 - val_mean_absolute_error: 2.1052\n",
      "Epoch 19/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00019: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0864 - val_mean_absolute_error: 2.0864\n",
      "Epoch 20/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0859 - mean_absolute_error: 2.0859\n",
      "Epoch 00020: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0858 - mean_absolute_error: 2.0858 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 21/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0860 - mean_absolute_error: 2.0860\n",
      "Epoch 00021: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0860 - mean_absolute_error: 2.0860 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 22/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0872 - mean_absolute_error: 2.0872\n",
      "Epoch 00022: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0871 - mean_absolute_error: 2.0871 - val_loss: 2.1062 - val_mean_absolute_error: 2.1062\n",
      "Epoch 23/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00023: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0848 - mean_absolute_error: 2.0848 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 24/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0863 - mean_absolute_error: 2.0863\n",
      "Epoch 00024: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0863 - mean_absolute_error: 2.0863 - val_loss: 2.0924 - val_mean_absolute_error: 2.0924\n",
      "Epoch 25/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00025: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0896 - val_mean_absolute_error: 2.0896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00026: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.1512 - val_mean_absolute_error: 2.1512\n",
      "Epoch 27/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0860 - mean_absolute_error: 2.0860\n",
      "Epoch 00027: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0860 - mean_absolute_error: 2.0860 - val_loss: 2.1021 - val_mean_absolute_error: 2.1021\n",
      "Epoch 28/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0852 - mean_absolute_error: 2.0852\n",
      "Epoch 00028: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0852 - mean_absolute_error: 2.0852 - val_loss: 2.1117 - val_mean_absolute_error: 2.1117\n",
      "Epoch 29/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0844 - mean_absolute_error: 2.0844\n",
      "Epoch 00029: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0844 - mean_absolute_error: 2.0844 - val_loss: 2.1010 - val_mean_absolute_error: 2.1010\n",
      "Epoch 30/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0859 - mean_absolute_error: 2.0859\n",
      "Epoch 00030: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0859 - mean_absolute_error: 2.0859 - val_loss: 2.0815 - val_mean_absolute_error: 2.0815\n",
      "Epoch 31/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0871 - mean_absolute_error: 2.0871\n",
      "Epoch 00031: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0873 - mean_absolute_error: 2.0873 - val_loss: 2.1015 - val_mean_absolute_error: 2.1015\n",
      "Epoch 32/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0841 - mean_absolute_error: 2.0841\n",
      "Epoch 00032: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0841 - mean_absolute_error: 2.0841 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 33/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00033: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 34/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00034: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0774 - val_mean_absolute_error: 2.0774\n",
      "Epoch 35/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00035: val_loss improved from 2.07691 to 2.07578, saving model to Weights-035--2.07578.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0758 - val_mean_absolute_error: 2.0758\n",
      "Epoch 36/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0851 - mean_absolute_error: 2.0851\n",
      "Epoch 00036: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 37/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0833 - mean_absolute_error: 2.0833\n",
      "Epoch 00037: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0831 - mean_absolute_error: 2.0831 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 38/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0843 - mean_absolute_error: 2.0843\n",
      "Epoch 00038: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0844 - mean_absolute_error: 2.0844 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 39/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00039: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0848 - mean_absolute_error: 2.0848 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 40/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0833 - mean_absolute_error: 2.0833\n",
      "Epoch 00040: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0832 - mean_absolute_error: 2.0832 - val_loss: 2.1318 - val_mean_absolute_error: 2.1318\n",
      "Epoch 41/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00041: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 42/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0836 - mean_absolute_error: 2.0836\n",
      "Epoch 00042: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0837 - mean_absolute_error: 2.0837 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 43/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0855 - mean_absolute_error: 2.0855\n",
      "Epoch 00043: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0854 - mean_absolute_error: 2.0854 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 44/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00044: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0843 - val_mean_absolute_error: 2.0843\n",
      "Epoch 45/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00045: val_loss improved from 2.07578 to 2.07546, saving model to Weights-045--2.07546.hdf5\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0829 - mean_absolute_error: 2.0829 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 46/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00046: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0838 - mean_absolute_error: 2.0838 - val_loss: 2.1350 - val_mean_absolute_error: 2.1350\n",
      "Epoch 47/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00047: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 48/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0827 - mean_absolute_error: 2.0827\n",
      "Epoch 00048: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.1220 - val_mean_absolute_error: 2.1220\n",
      "Epoch 49/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00049: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.1425 - val_mean_absolute_error: 2.1425\n",
      "Epoch 50/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0836 - mean_absolute_error: 2.0836\n",
      "Epoch 00050: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.1112 - val_mean_absolute_error: 2.1112\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0821 - mean_absolute_error: 2.0821\n",
      "Epoch 00051: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0823 - mean_absolute_error: 2.0823 - val_loss: 2.0992 - val_mean_absolute_error: 2.0992\n",
      "Epoch 52/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00052: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0817 - val_mean_absolute_error: 2.0817\n",
      "Epoch 53/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0827 - mean_absolute_error: 2.0827\n",
      "Epoch 00053: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0859 - val_mean_absolute_error: 2.0859\n",
      "Epoch 54/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00054: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0827 - val_mean_absolute_error: 2.0827\n",
      "Epoch 55/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00055: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0950 - val_mean_absolute_error: 2.0950\n",
      "Epoch 56/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00056: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0839 - val_mean_absolute_error: 2.0839\n",
      "Epoch 57/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00057: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0934 - val_mean_absolute_error: 2.0934\n",
      "Epoch 58/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00058: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0815 - val_mean_absolute_error: 2.0815\n",
      "Epoch 59/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00059: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 60/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00060: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.0931 - val_mean_absolute_error: 2.0931\n",
      "Epoch 61/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00061: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 62/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00062: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 63/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00063: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 64/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00064: val_loss improved from 2.07546 to 2.07296, saving model to Weights-064--2.07296.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0730 - val_mean_absolute_error: 2.0730\n",
      "Epoch 65/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00065: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0977 - val_mean_absolute_error: 2.0977\n",
      "Epoch 66/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00066: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 67/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00067: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 68/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00068: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0800 - mean_absolute_error: 2.0800 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 69/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00069: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0731 - val_mean_absolute_error: 2.0731\n",
      "Epoch 70/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00070: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 71/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00071: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0736 - val_mean_absolute_error: 2.0736\n",
      "Epoch 72/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00072: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.1021 - val_mean_absolute_error: 2.1021\n",
      "Epoch 73/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00073: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 74/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00074: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0794 - val_mean_absolute_error: 2.0794\n",
      "Epoch 75/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00075: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0799 - mean_absolute_error: 2.0799 - val_loss: 2.0852 - val_mean_absolute_error: 2.0852\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00076: val_loss improved from 2.07296 to 2.06861, saving model to Weights-076--2.06861.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0686 - val_mean_absolute_error: 2.0686\n",
      "Epoch 77/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00077: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0704 - val_mean_absolute_error: 2.0704\n",
      "Epoch 78/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00078: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 79/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0814 - mean_absolute_error: 2.0814\n",
      "Epoch 00079: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0811 - mean_absolute_error: 2.0811 - val_loss: 2.1209 - val_mean_absolute_error: 2.1209\n",
      "Epoch 80/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0801 - mean_absolute_error: 2.0801\n",
      "Epoch 00080: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0754 - val_mean_absolute_error: 2.0754\n",
      "Epoch 81/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00081: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 82/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00082: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 83/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00083: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 84/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00084: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 85/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00085: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1072 - val_mean_absolute_error: 2.1072\n",
      "Epoch 86/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00086: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0958 - val_mean_absolute_error: 2.0958\n",
      "Epoch 87/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00087: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0799 - mean_absolute_error: 2.0799 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 88/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00088: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0933 - val_mean_absolute_error: 2.0933\n",
      "Epoch 89/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00089: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 90/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00090: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0884 - val_mean_absolute_error: 2.0884\n",
      "Epoch 91/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00091: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 92/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00092: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0860 - val_mean_absolute_error: 2.0860\n",
      "Epoch 93/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00093: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 94/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0807 - mean_absolute_error: 2.0807\n",
      "Epoch 00094: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0808 - mean_absolute_error: 2.0808 - val_loss: 2.0712 - val_mean_absolute_error: 2.0712\n",
      "Epoch 95/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00095: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0809 - mean_absolute_error: 2.0809 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 96/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00096: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 97/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00097: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0743 - val_mean_absolute_error: 2.0743\n",
      "Epoch 98/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00098: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0890 - val_mean_absolute_error: 2.0890\n",
      "Epoch 99/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00099: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.1049 - val_mean_absolute_error: 2.1049\n",
      "Epoch 100/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00100: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0875 - val_mean_absolute_error: 2.0875\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00101: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0721 - val_mean_absolute_error: 2.0721\n",
      "Epoch 102/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00102: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 103/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00103: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0757 - val_mean_absolute_error: 2.0757\n",
      "Epoch 104/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00104: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 105/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00105: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 106/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00106: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0902 - val_mean_absolute_error: 2.0902\n",
      "Epoch 107/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00107: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0828 - val_mean_absolute_error: 2.0828\n",
      "Epoch 108/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00108: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0800 - val_mean_absolute_error: 2.0800\n",
      "Epoch 109/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00109: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 110/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0802 - mean_absolute_error: 2.0802\n",
      "Epoch 00110: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0800 - mean_absolute_error: 2.0800 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 111/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00111: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0701 - val_mean_absolute_error: 2.0701\n",
      "Epoch 112/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00112: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 113/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00113: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0761 - val_mean_absolute_error: 2.0761\n",
      "Epoch 114/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00114: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0840 - val_mean_absolute_error: 2.0840\n",
      "Epoch 115/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00115: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 116/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00116: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1087 - val_mean_absolute_error: 2.1087\n",
      "Epoch 117/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00117: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0848 - val_mean_absolute_error: 2.0848\n",
      "Epoch 118/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00118: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0876 - val_mean_absolute_error: 2.0876\n",
      "Epoch 119/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00119: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 120/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00120: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.0877 - val_mean_absolute_error: 2.0877\n",
      "Epoch 121/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00121: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.1455 - val_mean_absolute_error: 2.1455\n",
      "Epoch 122/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00122: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 123/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00123: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0975 - val_mean_absolute_error: 2.0975\n",
      "Epoch 124/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00124: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 125/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00125: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 126/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00126: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0762 - val_mean_absolute_error: 2.0762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00127: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 128/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00128: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0880 - val_mean_absolute_error: 2.0880\n",
      "Epoch 129/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00129: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0996 - val_mean_absolute_error: 2.0996\n",
      "Epoch 130/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00130: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 131/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00131: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 132/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00132: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 133/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00133: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 134/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00134: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 135/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00135: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 136/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00136: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0803 - val_mean_absolute_error: 2.0803\n",
      "Epoch 137/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00137: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 138/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00138: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 139/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00139: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0880 - val_mean_absolute_error: 2.0880\n",
      "Epoch 140/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00140: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0943 - val_mean_absolute_error: 2.0943\n",
      "Epoch 141/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00141: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 142/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00142: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0805 - val_mean_absolute_error: 2.0805\n",
      "Epoch 143/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00143: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 144/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00144: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.1101 - val_mean_absolute_error: 2.1101\n",
      "Epoch 145/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00145: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 146/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00146: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 147/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00147: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0964 - val_mean_absolute_error: 2.0964\n",
      "Epoch 148/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00148: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 149/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00149: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0835 - val_mean_absolute_error: 2.0835\n",
      "Epoch 150/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00150: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0788 - val_mean_absolute_error: 2.0788\n",
      "Epoch 151/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00151: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0823 - val_mean_absolute_error: 2.0823\n",
      "Epoch 152/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00152: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0775 - val_mean_absolute_error: 2.0775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00153: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0817 - val_mean_absolute_error: 2.0817\n",
      "Epoch 154/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00154: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0752 - val_mean_absolute_error: 2.0752\n",
      "Epoch 155/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00155: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0699 - val_mean_absolute_error: 2.0699\n",
      "Epoch 156/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00156: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0710 - val_mean_absolute_error: 2.0710\n",
      "Epoch 157/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00157: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 158/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00158: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 159/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00159: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 160/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00160: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0763 - val_mean_absolute_error: 2.0763\n",
      "Epoch 161/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00161: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0739 - val_mean_absolute_error: 2.0739\n",
      "Epoch 162/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00162: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 163/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00163: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 164/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00164: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 165/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00165: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0901 - val_mean_absolute_error: 2.0901\n",
      "Epoch 166/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00166: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0809 - val_mean_absolute_error: 2.0809\n",
      "Epoch 167/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00167: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.1085 - val_mean_absolute_error: 2.1085\n",
      "Epoch 168/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00168: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 169/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00169: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.1047 - val_mean_absolute_error: 2.1047\n",
      "Epoch 170/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00170: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0906 - val_mean_absolute_error: 2.0906\n",
      "Epoch 171/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00171: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 172/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00172: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0809 - val_mean_absolute_error: 2.0809\n",
      "Epoch 173/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00173: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0791 - val_mean_absolute_error: 2.0791\n",
      "Epoch 174/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00174: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.1080 - val_mean_absolute_error: 2.1080\n",
      "Epoch 175/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00175: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.1454 - val_mean_absolute_error: 2.1454\n",
      "Epoch 176/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00176: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 177/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00177: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 178/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00178: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00179: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0730 - val_mean_absolute_error: 2.0730\n",
      "Epoch 180/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00180: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 181/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00181: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 182/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00182: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.1066 - val_mean_absolute_error: 2.1066\n",
      "Epoch 183/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00183: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 184/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00184: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0921 - val_mean_absolute_error: 2.0921\n",
      "Epoch 185/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00185: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 186/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00186: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0968 - val_mean_absolute_error: 2.0968\n",
      "Epoch 187/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00187: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0873 - val_mean_absolute_error: 2.0873\n",
      "Epoch 188/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00188: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0696 - val_mean_absolute_error: 2.0696\n",
      "Epoch 189/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00189: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 190/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780- ETA: 1s - loss: 2.\n",
      "Epoch 00190: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 191/500\n",
      "27669/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00191: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0833 - val_mean_absolute_error: 2.0833\n",
      "Epoch 192/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00192: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.1049 - val_mean_absolute_error: 2.1049\n",
      "Epoch 193/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00193: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.1006 - val_mean_absolute_error: 2.1006\n",
      "Epoch 194/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00194: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0690 - val_mean_absolute_error: 2.0690\n",
      "Epoch 195/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00195: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 196/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00196: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.1020 - val_mean_absolute_error: 2.1020\n",
      "Epoch 197/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00197: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0792 - val_mean_absolute_error: 2.0792\n",
      "Epoch 198/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0767 - mean_absolute_error: 2.0767\n",
      "Epoch 00198: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0904 - val_mean_absolute_error: 2.0904\n",
      "Epoch 199/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00199: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 200/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00200: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 201/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00201: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 202/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00202: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.1052 - val_mean_absolute_error: 2.1052\n",
      "Epoch 203/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00203: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0821 - val_mean_absolute_error: 2.0821\n",
      "Epoch 204/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00204: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0721 - val_mean_absolute_error: 2.0721\n",
      "Epoch 205/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00205: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.1106 - val_mean_absolute_error: 2.1106\n",
      "Epoch 206/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00206: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 207/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00207: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 208/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00208: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0878 - val_mean_absolute_error: 2.0878\n",
      "Epoch 209/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00209: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 210/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00210: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 211/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00211: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0718 - val_mean_absolute_error: 2.0718\n",
      "Epoch 212/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00212: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 213/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00213: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0732 - val_mean_absolute_error: 2.0732\n",
      "Epoch 214/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00214: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.1056 - val_mean_absolute_error: 2.1056\n",
      "Epoch 215/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00215: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.1007 - val_mean_absolute_error: 2.1007\n",
      "Epoch 216/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00216: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 217/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00217: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 218/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00218: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0846 - val_mean_absolute_error: 2.0846\n",
      "Epoch 219/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00219: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 220/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00220: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0896 - val_mean_absolute_error: 2.0896\n",
      "Epoch 221/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00221: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 222/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00222: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 223/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00223: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0765 - val_mean_absolute_error: 2.0765\n",
      "Epoch 224/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00224: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0786 - val_mean_absolute_error: 2.0786\n",
      "Epoch 225/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00225: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 226/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00226: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 227/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00227: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 228/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00228: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1278 - val_mean_absolute_error: 2.1278\n",
      "Epoch 229/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00229: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0710 - val_mean_absolute_error: 2.0710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00230: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 231/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00231: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0711 - val_mean_absolute_error: 2.0711\n",
      "Epoch 232/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00232: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0975 - val_mean_absolute_error: 2.0975\n",
      "Epoch 233/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00233: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0906 - val_mean_absolute_error: 2.0906\n",
      "Epoch 234/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00234: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 235/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00235: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0842 - val_mean_absolute_error: 2.0842\n",
      "Epoch 236/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00236: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 237/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00237: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0802 - val_mean_absolute_error: 2.0802\n",
      "Epoch 238/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00238: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0705 - val_mean_absolute_error: 2.0705\n",
      "Epoch 239/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00239: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0734 - val_mean_absolute_error: 2.0734\n",
      "Epoch 240/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00240: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 241/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00241: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0939 - val_mean_absolute_error: 2.0939\n",
      "Epoch 242/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0767 - mean_absolute_error: 2.0767\n",
      "Epoch 00242: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 243/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00243: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 244/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00244: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0848 - val_mean_absolute_error: 2.0848\n",
      "Epoch 245/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00245: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0895 - val_mean_absolute_error: 2.0895\n",
      "Epoch 246/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00246: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0729 - val_mean_absolute_error: 2.0729\n",
      "Epoch 247/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00247: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 248/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00248: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 249/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00249: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 250/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00250: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0947 - val_mean_absolute_error: 2.0947\n",
      "Epoch 251/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00251: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1004 - val_mean_absolute_error: 2.1004\n",
      "Epoch 252/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00252: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 253/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00253: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 254/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00254: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 255/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00255: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00256: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1094 - val_mean_absolute_error: 2.1094\n",
      "Epoch 257/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00257: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 258/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00258: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 259/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00259: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 260/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00260: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 261/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00261: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 262/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00262: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0701 - val_mean_absolute_error: 2.0701\n",
      "Epoch 263/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00263: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0752 - val_mean_absolute_error: 2.0752\n",
      "Epoch 264/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00264: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.1183 - val_mean_absolute_error: 2.1183\n",
      "Epoch 265/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00265: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0875 - val_mean_absolute_error: 2.0875\n",
      "Epoch 266/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00266: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0998 - val_mean_absolute_error: 2.0998\n",
      "Epoch 267/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00267: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0745 - val_mean_absolute_error: 2.0745\n",
      "Epoch 268/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00268: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1036 - val_mean_absolute_error: 2.1036\n",
      "Epoch 269/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00269: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.1232 - val_mean_absolute_error: 2.1232\n",
      "Epoch 270/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00270: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n",
      "Epoch 271/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00271: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0871 - val_mean_absolute_error: 2.0871\n",
      "Epoch 272/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00272: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0735 - val_mean_absolute_error: 2.0735\n",
      "Epoch 273/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00273: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0747 - mean_absolute_error: 2.0747 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 274/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00274: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0890 - val_mean_absolute_error: 2.0890\n",
      "Epoch 275/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00275: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0758 - val_mean_absolute_error: 2.0758\n",
      "Epoch 276/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00276: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.1024 - val_mean_absolute_error: 2.1024\n",
      "Epoch 277/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00277: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0955 - val_mean_absolute_error: 2.0955\n",
      "Epoch 278/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00278: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 279/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00279: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 280/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00280: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 281/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00281: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.1173 - val_mean_absolute_error: 2.1173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00282: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 283/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00283: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 284/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00284: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0921 - val_mean_absolute_error: 2.0921\n",
      "Epoch 285/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00285: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 286/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00286: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 287/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00287: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 288/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00288: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0907 - val_mean_absolute_error: 2.0907\n",
      "Epoch 289/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00289: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0746 - val_mean_absolute_error: 2.0746\n",
      "Epoch 290/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00290: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 291/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00291: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 292/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00292: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0844 - val_mean_absolute_error: 2.0844\n",
      "Epoch 293/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00293: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0854 - val_mean_absolute_error: 2.0854\n",
      "Epoch 294/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00294: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 295/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00295: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 296/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00296: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 297/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00297: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0922 - val_mean_absolute_error: 2.0922\n",
      "Epoch 298/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00298: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 299/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00299: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0980 - val_mean_absolute_error: 2.0980\n",
      "Epoch 300/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00300: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0804 - val_mean_absolute_error: 2.0804\n",
      "Epoch 301/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00301: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.1788 - val_mean_absolute_error: 2.1788\n",
      "Epoch 302/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00302: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.1505 - val_mean_absolute_error: 2.1505\n",
      "Epoch 303/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00303: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0926 - val_mean_absolute_error: 2.0926\n",
      "Epoch 304/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00304: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0905 - val_mean_absolute_error: 2.0905\n",
      "Epoch 305/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00305: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 306/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00306: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0743 - val_mean_absolute_error: 2.0743\n",
      "Epoch 307/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00307: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0834 - val_mean_absolute_error: 2.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 308/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00308: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.0789 - val_mean_absolute_error: 2.0789\n",
      "Epoch 309/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00309: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1291 - val_mean_absolute_error: 2.1291\n",
      "Epoch 310/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00310: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0813 - val_mean_absolute_error: 2.0813\n",
      "Epoch 311/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00311: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 312/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00312: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0952 - val_mean_absolute_error: 2.0952\n",
      "Epoch 313/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00313: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0855 - val_mean_absolute_error: 2.0855\n",
      "Epoch 314/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00314: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.0731 - val_mean_absolute_error: 2.0731\n",
      "Epoch 315/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00315: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0989 - val_mean_absolute_error: 2.0989\n",
      "Epoch 316/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00316: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0743 - mean_absolute_error: 2.0743 - val_loss: 2.0884 - val_mean_absolute_error: 2.0884\n",
      "Epoch 317/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00317: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 318/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00318: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 319/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00319: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 320/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00320: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0968 - val_mean_absolute_error: 2.0968\n",
      "Epoch 321/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00321: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0892 - val_mean_absolute_error: 2.0892\n",
      "Epoch 322/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00322: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.1138 - val_mean_absolute_error: 2.1138\n",
      "Epoch 323/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00323: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.1135 - val_mean_absolute_error: 2.1135\n",
      "Epoch 324/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00324: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 325/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00325: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0871 - val_mean_absolute_error: 2.0871\n",
      "Epoch 326/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00326: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 327/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00327: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 328/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00328: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0885 - val_mean_absolute_error: 2.0885\n",
      "Epoch 329/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00329: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 330/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00330: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n",
      "Epoch 331/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00331: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 332/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00332: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 333/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00333: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0735 - val_mean_absolute_error: 2.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00334: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0943 - val_mean_absolute_error: 2.0943\n",
      "Epoch 335/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00335: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.1489 - val_mean_absolute_error: 2.1489\n",
      "Epoch 336/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00336: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 337/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00337: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0840 - mean_absolute_error: 2.0840 - val_loss: 2.1090 - val_mean_absolute_error: 2.1090\n",
      "Epoch 338/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00338: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0839 - mean_absolute_error: 2.0839 - val_loss: 2.0978 - val_mean_absolute_error: 2.0978\n",
      "Epoch 339/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0821 - mean_absolute_error: 2.0821\n",
      "Epoch 00339: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0820 - mean_absolute_error: 2.0820 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 340/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0820 - mean_absolute_error: 2.0820\n",
      "Epoch 00340: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0819 - mean_absolute_error: 2.0819 - val_loss: 2.0761 - val_mean_absolute_error: 2.0761\n",
      "Epoch 341/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00341: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0725 - val_mean_absolute_error: 2.0725\n",
      "Epoch 342/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00342: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.1031 - val_mean_absolute_error: 2.1031\n",
      "Epoch 343/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0732 - mean_absolute_error: 2.0732\n",
      "Epoch 00343: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0732 - mean_absolute_error: 2.0732 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 344/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00344: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.1001 - val_mean_absolute_error: 2.1001\n",
      "Epoch 345/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0732 - mean_absolute_error: 2.0732\n",
      "Epoch 00345: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0732 - mean_absolute_error: 2.0732 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 346/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0740 - mean_absolute_error: 2.0740\n",
      "Epoch 00346: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0756 - val_mean_absolute_error: 2.0756\n",
      "Epoch 347/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0735 - mean_absolute_error: 2.0735\n",
      "Epoch 00347: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0737 - mean_absolute_error: 2.0737 - val_loss: 2.0774 - val_mean_absolute_error: 2.0774\n",
      "Epoch 348/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00348: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0729 - mean_absolute_error: 2.0729 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 349/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00349: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0730 - mean_absolute_error: 2.0730 - val_loss: 2.0713 - val_mean_absolute_error: 2.0713\n",
      "Epoch 350/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0735 - mean_absolute_error: 2.0735\n",
      "Epoch 00350: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0734 - mean_absolute_error: 2.0734 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 351/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0733 - mean_absolute_error: 2.0733\n",
      "Epoch 00351: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0734 - mean_absolute_error: 2.0734 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 352/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0727 - mean_absolute_error: 2.0727\n",
      "Epoch 00352: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0726 - mean_absolute_error: 2.0726 - val_loss: 2.0808 - val_mean_absolute_error: 2.0808\n",
      "Epoch 353/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00353: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 354/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0736 - mean_absolute_error: 2.0736\n",
      "Epoch 00354: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0736 - mean_absolute_error: 2.0736 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 355/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0733 - mean_absolute_error: 2.0733\n",
      "Epoch 00355: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0733 - mean_absolute_error: 2.0733 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 356/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0738 - mean_absolute_error: 2.0738\n",
      "Epoch 00356: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0736 - mean_absolute_error: 2.0736 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 357/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00357: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0730 - mean_absolute_error: 2.0730 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 358/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0727 - mean_absolute_error: 2.0727\n",
      "Epoch 00358: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0726 - mean_absolute_error: 2.0726 - val_loss: 2.0913 - val_mean_absolute_error: 2.0913\n",
      "Epoch 359/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00359: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 360/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0737 - mean_absolute_error: 2.0737\n",
      "Epoch 00360: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.0892 - val_mean_absolute_error: 2.0892\n",
      "Epoch 361/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00361: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0733 - mean_absolute_error: 2.0733 - val_loss: 2.0692 - val_mean_absolute_error: 2.0692\n",
      "Epoch 362/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00362: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0876 - val_mean_absolute_error: 2.0876\n",
      "Epoch 363/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0883 - mean_absolute_error: 2.0883\n",
      "Epoch 00363: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0884 - mean_absolute_error: 2.0884 - val_loss: 2.0942 - val_mean_absolute_error: 2.0942\n",
      "Epoch 364/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0879 - mean_absolute_error: 2.0879\n",
      "Epoch 00364: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0879 - mean_absolute_error: 2.0879 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 365/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0927 - mean_absolute_error: 2.0927\n",
      "Epoch 00365: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0927 - mean_absolute_error: 2.0927 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 366/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0861 - mean_absolute_error: 2.0861\n",
      "Epoch 00366: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0859 - mean_absolute_error: 2.0859 - val_loss: 2.0762 - val_mean_absolute_error: 2.0762\n",
      "Epoch 367/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0811 - mean_absolute_error: 2.0811\n",
      "Epoch 00367: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0810 - mean_absolute_error: 2.0810 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 368/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00368: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0855 - val_mean_absolute_error: 2.0855\n",
      "Epoch 369/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00369: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0829 - val_mean_absolute_error: 2.0829\n",
      "Epoch 370/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00370: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 371/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00371: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 372/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00372: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 373/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00373: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0954 - val_mean_absolute_error: 2.0954\n",
      "Epoch 374/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00374: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 375/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00375: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.1068 - val_mean_absolute_error: 2.1068\n",
      "Epoch 376/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00376: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0832 - val_mean_absolute_error: 2.0832\n",
      "Epoch 377/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00377: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 378/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00378: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0857 - val_mean_absolute_error: 2.0857\n",
      "Epoch 379/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00379: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.0867 - val_mean_absolute_error: 2.0867\n",
      "Epoch 380/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00380: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0846 - mean_absolute_error: 2.0846 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 381/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0823 - mean_absolute_error: 2.0823\n",
      "Epoch 00381: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0823 - mean_absolute_error: 2.0823 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 382/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00382: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0893 - val_mean_absolute_error: 2.0893\n",
      "Epoch 383/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00383: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1413 - val_mean_absolute_error: 2.1413\n",
      "Epoch 384/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00384: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00385: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0897 - val_mean_absolute_error: 2.0897\n",
      "Epoch 386/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00386: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0743 - mean_absolute_error: 2.0743 - val_loss: 2.0850 - val_mean_absolute_error: 2.0850\n",
      "Epoch 387/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00387: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0954 - val_mean_absolute_error: 2.0954\n",
      "Epoch 388/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00388: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.1260 - val_mean_absolute_error: 2.1260\n",
      "Epoch 389/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00389: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.1178 - val_mean_absolute_error: 2.1178\n",
      "Epoch 390/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00390: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0802 - mean_absolute_error: 2.0802 - val_loss: 2.0863 - val_mean_absolute_error: 2.0863\n",
      "Epoch 391/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00391: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0903 - val_mean_absolute_error: 2.0903\n",
      "Epoch 392/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00392: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 393/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0801 - mean_absolute_error: 2.0801\n",
      "Epoch 00393: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n",
      "Epoch 394/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00394: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0981 - val_mean_absolute_error: 2.0981\n",
      "Epoch 395/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00395: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.1024 - val_mean_absolute_error: 2.1024\n",
      "Epoch 396/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00396: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0793 - val_mean_absolute_error: 2.0793\n",
      "Epoch 397/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00397: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0948 - val_mean_absolute_error: 2.0948\n",
      "Epoch 398/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00398: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.1461 - val_mean_absolute_error: 2.1461\n",
      "Epoch 399/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00399: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0702 - val_mean_absolute_error: 2.0702\n",
      "Epoch 400/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00400: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.1308 - val_mean_absolute_error: 2.1308\n",
      "Epoch 401/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00401: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0935 - val_mean_absolute_error: 2.0935\n",
      "Epoch 402/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00402: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 403/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00403: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0846 - val_mean_absolute_error: 2.0846\n",
      "Epoch 404/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0738 - mean_absolute_error: 2.0738\n",
      "Epoch 00404: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.0744 - val_mean_absolute_error: 2.0744\n",
      "Epoch 405/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00405: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 406/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00406: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 407/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00407: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.0934 - val_mean_absolute_error: 2.0934\n",
      "Epoch 408/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00408: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.1097 - val_mean_absolute_error: 2.1097\n",
      "Epoch 409/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0742 - mean_absolute_error: 2.0742\n",
      "Epoch 00409: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 410/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00410: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0723 - val_mean_absolute_error: 2.0723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00411: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.1044 - val_mean_absolute_error: 2.1044\n",
      "Epoch 412/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00412: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 413/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00413: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0808 - val_mean_absolute_error: 2.0808\n",
      "Epoch 414/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00414: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 415/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766- ETA: 1s - loss: 2.0768 - \n",
      "Epoch 00415: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0765 - val_mean_absolute_error: 2.0765\n",
      "Epoch 416/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00416: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0749 - val_mean_absolute_error: 2.0749\n",
      "Epoch 417/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00417: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0716 - val_mean_absolute_error: 2.0716\n",
      "Epoch 418/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00418: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 419/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00419: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0766 - val_mean_absolute_error: 2.0766\n",
      "Epoch 420/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00420: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0742 - mean_absolute_error: 2.0742 - val_loss: 2.0747 - val_mean_absolute_error: 2.0747\n",
      "Epoch 421/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0737 - mean_absolute_error: 2.0737\n",
      "Epoch 00421: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.1264 - val_mean_absolute_error: 2.1264\n",
      "Epoch 422/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00422: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0786 - val_mean_absolute_error: 2.0786\n",
      "Epoch 423/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00423: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0747 - mean_absolute_error: 2.0747 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 424/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00424: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0845 - val_mean_absolute_error: 2.0845\n",
      "Epoch 425/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00425: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0741 - mean_absolute_error: 2.0741 - val_loss: 2.0725 - val_mean_absolute_error: 2.0725\n",
      "Epoch 426/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0740 - mean_absolute_error: 2.0740\n",
      "Epoch 00426: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0932 - val_mean_absolute_error: 2.0932\n",
      "Epoch 427/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00427: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 428/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00428: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 429/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00429: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 430/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00430: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 431/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00431: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.1013 - val_mean_absolute_error: 2.1013\n",
      "Epoch 432/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00432: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0877 - val_mean_absolute_error: 2.0877\n",
      "Epoch 433/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00433: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 434/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00434: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 435/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00435: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 436/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00436: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 437/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00437: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 438/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00438: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0728 - val_mean_absolute_error: 2.0728\n",
      "Epoch 439/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00439: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0881 - val_mean_absolute_error: 2.0881\n",
      "Epoch 440/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00440: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1149 - val_mean_absolute_error: 2.1149\n",
      "Epoch 441/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00441: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 442/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00442: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.1261 - val_mean_absolute_error: 2.1261\n",
      "Epoch 443/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00443: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 444/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00444: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0897 - val_mean_absolute_error: 2.0897\n",
      "Epoch 445/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00445: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 446/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00446: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.1037 - val_mean_absolute_error: 2.1037\n",
      "Epoch 447/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00447: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 448/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00448: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 449/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00449: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 450/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00450: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.1015 - val_mean_absolute_error: 2.1015\n",
      "Epoch 451/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00451: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0749 - val_mean_absolute_error: 2.0749\n",
      "Epoch 452/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00452: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0953 - val_mean_absolute_error: 2.0953\n",
      "Epoch 453/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00453: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0925 - val_mean_absolute_error: 2.0925\n",
      "Epoch 454/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00454: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 455/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00455: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 456/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00456: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 457/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00457: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0961 - val_mean_absolute_error: 2.0961\n",
      "Epoch 458/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00458: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0825 - val_mean_absolute_error: 2.0825\n",
      "Epoch 459/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00459: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0918 - val_mean_absolute_error: 2.0918\n",
      "Epoch 460/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00460: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0969 - val_mean_absolute_error: 2.0969\n",
      "Epoch 461/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00461: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0808 - mean_absolute_error: 2.0808 - val_loss: 2.0794 - val_mean_absolute_error: 2.0794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0816 - mean_absolute_error: 2.0816\n",
      "Epoch 00462: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 1ms/step - loss: 2.0816 - mean_absolute_error: 2.0816 - val_loss: 2.0936 - val_mean_absolute_error: 2.0936\n",
      "Epoch 463/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0826 - mean_absolute_error: 2.0826\n",
      "Epoch 00463: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0826 - mean_absolute_error: 2.0826 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 464/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00464: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0756 - val_mean_absolute_error: 2.0756\n",
      "Epoch 465/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00465: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0809 - mean_absolute_error: 2.0809 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 466/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00466: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0803 - mean_absolute_error: 2.0803 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 467/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00467: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 468/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00468: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 469/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0806 - mean_absolute_error: 2.0806\n",
      "Epoch 00469: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0833 - val_mean_absolute_error: 2.0833\n",
      "Epoch 470/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00470: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 471/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00471: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 472/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00472: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 473/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00473: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.1079 - val_mean_absolute_error: 2.1079\n",
      "Epoch 474/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00474: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 475/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00475: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0830 - mean_absolute_error: 2.0830 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 476/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00476: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 477/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00477: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 478/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00478: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 479/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00479: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.1298 - val_mean_absolute_error: 2.1298\n",
      "Epoch 480/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00480: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 481/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00481: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0893 - val_mean_absolute_error: 2.0893\n",
      "Epoch 482/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00482: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0823 - val_mean_absolute_error: 2.0823\n",
      "Epoch 483/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00483: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.1137 - val_mean_absolute_error: 2.1137\n",
      "Epoch 484/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00484: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0739 - val_mean_absolute_error: 2.0739\n",
      "Epoch 485/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00485: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0859 - val_mean_absolute_error: 2.0859\n",
      "Epoch 486/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00486: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.1217 - val_mean_absolute_error: 2.1217\n",
      "Epoch 487/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00487: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0830 - val_mean_absolute_error: 2.0830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00488: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.1058 - val_mean_absolute_error: 2.1058\n",
      "Epoch 489/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00489: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 490/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0813 - mean_absolute_error: 2.0813\n",
      "Epoch 00490: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0813 - mean_absolute_error: 2.0813 - val_loss: 2.1054 - val_mean_absolute_error: 2.1054\n",
      "Epoch 491/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00491: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 47s 2ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 492/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00492: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 47s 2ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0996 - val_mean_absolute_error: 2.0996\n",
      "Epoch 493/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00493: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 494/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00494: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0741 - val_mean_absolute_error: 2.0741\n",
      "Epoch 495/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00495: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0757 - val_mean_absolute_error: 2.0757\n",
      "Epoch 496/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00496: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0932 - val_mean_absolute_error: 2.0932\n",
      "Epoch 497/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00497: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 498/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00498: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0828 - val_mean_absolute_error: 2.0828\n",
      "Epoch 499/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00499: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 500/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00500: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0742 - mean_absolute_error: 2.0742 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dd473be490>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'Weights-076--2.06861.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.418053 ],\n",
       "       [10.769339 ],\n",
       "       [ 4.0847797],\n",
       "       ...,\n",
       "       [55.06902  ],\n",
       "       [22.621426 ],\n",
       "       [ 6.283396 ]], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def knn_model(x_train, x_val, y_train, y_val, neighbors):\n",
    "    min_rmse = 1000\n",
    "    for n in neighbors:\n",
    "        knn = KNeighborsRegressor(n_neighbors=n)\n",
    "        knn.fit(x_train, y_train)\n",
    "        pred = knn.predict(x_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        if rmse < min_rmse:\n",
    "            min_rmse = rmse\n",
    "            model = knn\n",
    "            best_pred = pred\n",
    "        print('Neighbours', n, 'RMSE', rmse)\n",
    "    return model, min_rmse, best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbours 18 RMSE 5.1377800001928735\n",
      "Neighbours 24 RMSE 5.12771491074715\n",
      "Neighbours 30 RMSE 5.127487618943662\n",
      "Neighbours 40 RMSE 5.130315314887339\n"
     ]
    }
   ],
   "source": [
    "k_choices = [18,24,30,40]\n",
    "knn_final_model, knn_final_rmse, knn_final_pred = knn_model(X_train, X_val, y_train, y_val, k_choices)\n",
    "knn_test_pred = knn_final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
