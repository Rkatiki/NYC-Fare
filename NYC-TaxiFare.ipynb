{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-24 11:42:00.000000113</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2013-11-24 11:42:00 UTC</td>\n",
       "      <td>-73.957740</td>\n",
       "      <td>40.773617</td>\n",
       "      <td>-73.977087</td>\n",
       "      <td>40.758465</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-06-17 16:12:58.0000001</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2010-06-17 16:12:58 UTC</td>\n",
       "      <td>-73.966473</td>\n",
       "      <td>40.757500</td>\n",
       "      <td>-73.993336</td>\n",
       "      <td>40.745051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-04 12:11:19.0000003</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-11-04 12:11:19 UTC</td>\n",
       "      <td>-73.975189</td>\n",
       "      <td>40.759095</td>\n",
       "      <td>-73.980510</td>\n",
       "      <td>40.767610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-31 19:20:00.00000027</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2012-07-31 19:20:00 UTC</td>\n",
       "      <td>-74.002707</td>\n",
       "      <td>40.726640</td>\n",
       "      <td>-73.991825</td>\n",
       "      <td>40.727220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-03-03 23:30:00.00000072</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2014-03-03 23:30:00 UTC</td>\n",
       "      <td>-73.981627</td>\n",
       "      <td>40.780420</td>\n",
       "      <td>-74.008465</td>\n",
       "      <td>40.734012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0  2013-11-24 11:42:00.000000113          9.0  2013-11-24 11:42:00 UTC   \n",
       "1    2010-06-17 16:12:58.0000001         10.5  2010-06-17 16:12:58 UTC   \n",
       "2    2014-11-04 12:11:19.0000003          8.0  2014-11-04 12:11:19 UTC   \n",
       "3   2012-07-31 19:20:00.00000027          4.9  2012-07-31 19:20:00 UTC   \n",
       "4   2014-03-03 23:30:00.00000072         13.5  2014-03-03 23:30:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.957740        40.773617         -73.977087         40.758465   \n",
       "1        -73.966473        40.757500         -73.993336         40.745051   \n",
       "2        -73.975189        40.759095         -73.980510         40.767610   \n",
       "3        -74.002707        40.726640         -73.991825         40.727220   \n",
       "4        -73.981627        40.780420         -74.008465         40.734012   \n",
       "\n",
       "   passenger_count  \n",
       "0                6  \n",
       "1                1  \n",
       "2                1  \n",
       "3                2  \n",
       "4                2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "skiprows = np.random.rand(55 * 10 ** 7) > 0.02\n",
    "skiprows[0] = False\n",
    "df = pd.read_csv('train.csv', skiprows=lambda x: skiprows[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1109086, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.973320</td>\n",
       "      <td>40.763805</td>\n",
       "      <td>-73.981430</td>\n",
       "      <td>40.743835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.986862</td>\n",
       "      <td>40.719383</td>\n",
       "      <td>-73.998886</td>\n",
       "      <td>40.739201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>2011-10-08 11:53:44 UTC</td>\n",
       "      <td>-73.982524</td>\n",
       "      <td>40.751260</td>\n",
       "      <td>-73.979654</td>\n",
       "      <td>40.746139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.981160</td>\n",
       "      <td>40.767807</td>\n",
       "      <td>-73.990448</td>\n",
       "      <td>40.751635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.966046</td>\n",
       "      <td>40.789775</td>\n",
       "      <td>-73.988565</td>\n",
       "      <td>40.744427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key          pickup_datetime  pickup_longitude  \\\n",
       "0  2015-01-27 13:08:24.0000002  2015-01-27 13:08:24 UTC        -73.973320   \n",
       "1  2015-01-27 13:08:24.0000003  2015-01-27 13:08:24 UTC        -73.986862   \n",
       "2  2011-10-08 11:53:44.0000002  2011-10-08 11:53:44 UTC        -73.982524   \n",
       "3  2012-12-01 21:12:12.0000002  2012-12-01 21:12:12 UTC        -73.981160   \n",
       "4  2012-12-01 21:12:12.0000003  2012-12-01 21:12:12 UTC        -73.966046   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.763805         -73.981430         40.743835                1  \n",
       "1        40.719383         -73.998886         40.739201                1  \n",
       "2        40.751260         -73.979654         40.746139                1  \n",
       "3        40.767807         -73.990448         40.751635                1  \n",
       "4        40.789775         -73.988565         40.744427                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],utc=True)\n",
    "df1['pickup_datetime'] = pd.to_datetime(df1['pickup_datetime'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.drop(df.index[(df.pickup_longitude < -75) | \n",
    "           (df.pickup_longitude > -72) | \n",
    "           (df.pickup_latitude < 40) | \n",
    "           (df.pickup_latitude > 42)],inplace=True)\n",
    "df.drop(df.index[(df.dropoff_longitude < -75) | \n",
    "           (df.dropoff_longitude > -72) | \n",
    "           (df.dropoff_latitude < 40) | \n",
    "           (df.dropoff_latitude > 42)],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,2:]\n",
    "Y_train = df['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df1.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['pickup_datetime'][3].dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement holidays==9.1.2 (from versions: 0.1, 0.2, 0.3, 0.3.1, 0.4, 0.4.1, 0.5, 0.6, 0.7, 0.8, 0.8.1, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 0.9.9, 0.9.10, 0.9.11, 0.9.12, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5.2, 0.11.1)\n",
      "ERROR: No matching distribution found for holidays==9.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install holidays==9.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "us_holidays = holidays.US()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/1109086 [00:00<?, ?it/s]<ipython-input-10-5c51f4faf8d5>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['distance_travelled'][i] = hs.haversine(df['pickup_location'][i],df['dropoff_location'][i])\n",
      "  0%|                                                                                      | 0/1109086 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'us_holidays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c51f4faf8d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'distance_travelled'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhaversine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pickup_location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dropoff_location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_a_holiday'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pickup_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mus_holidays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_a_holiday'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'us_holidays' is not defined"
     ]
    }
   ],
   "source": [
    "import haversine as hs\n",
    "from tqdm import tqdm\n",
    "df['distance_travelled'] = None\n",
    "df['pickup_location'] = list(zip(df.pickup_latitude, df.pickup_longitude))\n",
    "df['dropoff_location'] = list(zip(df.dropoff_latitude, df.dropoff_longitude))\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    df['distance_travelled'][i] = hs.haversine(df['pickup_location'][i],df['dropoff_location'][i])\n",
    "    df['is_a_holiday'][i] = df['pickup_datetime'][i] in us_holidays\n",
    "df['is_a_holiday'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haversine as hs\n",
    "from tqdm import tqdm\n",
    "df['distance'] = list(zip(list(zip(df.pickup_latitude, df.pickup_longitude)),list(zip(df.dropoff_latitude, df.dropoff_longitude))))\n",
    "df['distance_travelled'] = df['distance'].apply(lambda x: hs.haversine(x[0],x[1]))\n",
    "df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "df['month'] = df['pickup_datetime'].apply(lambda x:x.month)\n",
    "df['day'] = df['pickup_datetime'].apply(lambda x: x.dayofweek)\n",
    "df[\"befor_shock\"] = ((df[\"year\"] <= 2011) | ((df[\"year\"] <= 2012) & (df[\"month\"] <= 8))).apply(int)\n",
    "df['is_a_holiday'] = (df['pickup_datetime'].apply(lambda x: x in us_holidays)).apply(int)\n",
    "df['passenger_count'] = (df['passenger_count']<5).apply(int)\n",
    "df[\"time\"] = df[\"pickup_datetime\"].apply(lambda x: x.hour * 60 + x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['distance'] = list(zip(list(zip(df1.pickup_latitude, df1.pickup_longitude)),list(zip(df1.dropoff_latitude, df1.dropoff_longitude))))\n",
    "df1['distance_travelled'] = df1['distance'].apply(lambda x: hs.haversine(x[0],x[1]))\n",
    "df1['year'] = df1['pickup_datetime'].apply(lambda x: x.year)\n",
    "df1['month'] = df1['pickup_datetime'].apply(lambda x:x.month)\n",
    "df1['day'] = df1['pickup_datetime'].apply(lambda x: x.dayofweek)\n",
    "df1[\"befor_shock\"] = ((df1[\"year\"] <= 2011) | ((df1[\"year\"] <= 2012) & (df1[\"month\"] <= 8))).apply(int)\n",
    "df1['is_a_holiday'] = (df1['pickup_datetime'].apply(lambda x: x in us_holidays)).apply(int)\n",
    "df1['passenger_count'] = (df1['passenger_count']<5).apply(int)\n",
    "df1[\"time\"] = df1[\"pickup_datetime\"].apply(lambda x: x.hour * 60 + x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "def geodesic_dist(trip):\n",
    "    pickup_lat = trip['pickup_latitude']\n",
    "    pickup_long = trip['pickup_longitude']\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    distance = geopy.distance.geodesic((pickup_lat, pickup_long), \n",
    "                                       (dropoff_lat, dropoff_long)).miles\n",
    "    try:\n",
    "        return distance\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "def circle_dist(trip):\n",
    "    pickup_lat = trip['pickup_latitude']\n",
    "    pickup_long = trip['pickup_longitude']\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    distance = geopy.distance.great_circle((pickup_lat, pickup_long), \n",
    "                                       (dropoff_lat, dropoff_long)).miles\n",
    "    try:\n",
    "        return distance\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jfk_dist(trip):\n",
    "    jfk_lat = 40.6413\n",
    "    jfk_long = -73.7781\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    jfk_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (jfk_lat, jfk_long)).miles\n",
    "    return jfk_distance\n",
    "\n",
    "def lga_dist(trip):\n",
    "    lga_lat = 40.7769\n",
    "    lga_long = -73.8740\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    lga_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (lga_lat, lga_long)).miles\n",
    "    return lga_distance\n",
    "\n",
    "def ewr_dist(trip):\n",
    "    ewr_lat = 40.6895\n",
    "    ewr_long = -74.1745\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    ewr_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (ewr_lat, ewr_long)).miles\n",
    "    return ewr_distance\n",
    "\n",
    "def tsq_dist(trip):\n",
    "    tsq_lat = 40.7580\n",
    "    tsq_long = -73.9855\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    tsq_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (tsq_lat, tsq_long)).miles\n",
    "    return tsq_distance\n",
    "def cpk_dist(trip):\n",
    "    cpk_lat = 40.7812\n",
    "    cpk_long = -73.9665\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    cpk_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (cpk_lat, cpk_long)).miles\n",
    "    return cpk_distance\n",
    "\n",
    "def lib_dist(trip):\n",
    "    lib_lat = 40.6892\n",
    "    lib_long = -74.0445\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    lib_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (lib_lat, lib_long)).miles\n",
    "    return lib_distance\n",
    "\n",
    "def gct_dist(trip):\n",
    "    gct_lat = 40.7527\n",
    "    gct_long = -73.9772\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    gct_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (gct_lat, gct_long)).miles\n",
    "    return gct_distance\n",
    "\n",
    "def met_dist(trip):\n",
    "    met_lat = 40.7794\n",
    "    met_long = -73.9632\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    met_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (met_lat, met_long)).miles\n",
    "    return met_distance\n",
    "\n",
    "def wtc_dist(trip):\n",
    "    wtc_lat = 40.7126\n",
    "    wtc_long = -74.0099\n",
    "    dropoff_lat = trip['dropoff_latitude']\n",
    "    dropoff_long = trip['dropoff_longitude']\n",
    "    wtc_distance = geopy.distance.geodesic((dropoff_lat, dropoff_long), (wtc_lat, wtc_long)).miles\n",
    "    return wtc_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_floats(df):\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_ints(df):\n",
    "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
    "    return df\n",
    "\n",
    "def optimize(df):\n",
    "    return optimize_floats(optimize_ints(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dists(df):\n",
    "    df['geodesic'] = df.apply(lambda x: geodesic_dist(x), axis = 1 )\n",
    "    df['circle'] = df.apply(lambda x: circle_dist(x), axis = 1 )\n",
    "    df['jfk'] = df.apply(lambda x: jfk_dist(x), axis = 1 )\n",
    "    df['lga'] = df.apply(lambda x: lga_dist(x), axis = 1 )\n",
    "    df['ewr'] = df.apply(lambda x: ewr_dist(x), axis = 1 )\n",
    "    df['tsq'] = df.apply(lambda x: tsq_dist(x), axis = 1 )\n",
    "    df['cpk'] = df.apply(lambda x: cpk_dist(x), axis = 1 )\n",
    "    df['lib'] = df.apply(lambda x: lib_dist(x), axis = 1 )\n",
    "    df['gct'] = df.apply(lambda x: gct_dist(x), axis = 1 )\n",
    "    df['met'] = df.apply(lambda x: met_dist(x), axis = 1 )\n",
    "    df['wtc'] = df.apply(lambda x: wtc_dist(x), axis = 1 )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calc_dists(df)\n",
    "df1 = calc_dists(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.drop('fare_amount', axis=1).corr(), square=True)\n",
    "plt.suptitle('Pearson Correlation Heatmap')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-2fa116424e69>:10: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  f.show();\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1368x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "(mu, sigma) = norm.fit(df['geodesic'])\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(19, 5))\n",
    "ax1 = sns.distplot(df['geodesic'] , fit=norm, ax=ax1)\n",
    "ax1.legend([f'Normal distribution ($\\mu=$ {mu:.3f} and $\\sigma=$ {sigma:.3f})'], loc='best')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distance Distribution')\n",
    "ax2 = stats.probplot(df['geodesic'], plot=plt)\n",
    "f.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-a570f7b5a9db>:8: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  f.show();\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAFOCAYAAADesvuBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde7yVZZnw8d+1NyqiGGpYykFQCTmIiAQ2puVrKpmJmhW8msdibHDGmcneNJvqrSibHC0q87XRUlPRSU0ypTxUaiMaKJongjyBOomYhwQPG673j/VsWmz2Ye3lWmz38vf9fNZnr+d+7vte17M+W/bj9dyHyEwkSZIkSZLU85p6OgBJkiRJkiSVmKiRJEmSJEl6kzBRI0mSJEmS9CZhokaSJEmSJOlNwkSNJEmSJEnSm4SJGkmSJEmSpDcJEzWS3nQi4vMR8Z817O+vEbFT8f7HEfG1GvZ9XkT8W636kyRJjSEiMiJ2qbLtYxHxgQ7O7RMRi9urW+t7qE7ie39ELK/350hvVSZqpLew4g/76iKR0fraoc6f+ZuIeCUiXoqIFyNiYUScFhGbtdbJzK9n5icr7KvLepm5ZWY+UoPYj4uI29v0fVJmfvWN9i1Jknpem3ujP0fEjyJiy56Oq1xm3paZIzs4t+4eKiKGFcmiPtV8TnHfs6b4Ll6MiEURcUgV/dT0IZn0VmCiRtKHi0RG6+upShtGSTX/jpycmf2B7YHPANOA6yMiquirs/iqujGRJElvaR/OzC2BCcC7gS+0rfAWuse4o/guBgAXAFdGxDY9HJPU8EzUSFpPRGwdEddFxIqI+EvxfnDZ+d9ExKyI+B2wCtgpInaNiBsj4rmIWBwRH6vkszLz5cz8DXAo8B7gQ8VnfDkiflK87xsRP4mIlRHxfET8PiLeERGzgH2A7xVPer5X1M+ImBkRS4AlZWXlQ4/fXsT7UkT8NiJ2LOpt8OSpddRORIwCzgPeU3ze88X59Z4SRcSnImJp8V3MLR+hVPR9UkQsKb7b79c6OSVJkmojM58EbgDGQof3GB3+3S8cHBGPRMSzEfGt1gdcEbFzRNxS3N88GxGXRsSANm3fHREPFvcMP4qIvkXbDqcdld9DAbcWP58v7l3eV8S5W1n97YoRRAO7+C7WAhcCmwM7tfO5o4p7pucj4oGIOLQonwEcBfyfIoafd/Y5kkpM1Ehqqwn4EbAjMBRYDXyvTZ1PADOA/sAK4EbgMmA7YDpwbkSMqfQDM/MJYAGlxEtbxwJvA4YA2wInAasz8wzgNkqjc7bMzJPL2hwGTAZGd/CRRwFfBd4OLAIurSDGh4rPvqP4vLY3U0TE/wK+AXyM0mihx4E5baodQunp3O5FvYO6+mxJkrTxRcQQ4GDgnrLidfcYFf7dPxyYSGl0zlTghNbui7Y7AKMo3ed8uU3boyjdJ+wMvIt2RvZ0Yd/i54Di3uW3RXxHl9WZDtyUmSs666h4iPVJ4K8USaqyc5sAPwd+Rele8B+BSyNiZGaeT+k+69+LGD7czWuQ3pJM1Ej6WfH04/mI+FlmrszMqzJzVWa+BMwC3temzY8z84HMbAGmAI9l5o8ysyUz7wauAo7sZhxPAe0NpX2dUoJml8xck5kLM/PFLvr6RmY+l5mrOzj/i8y8NTNfBc6gNEpmSDfjbc9RwIWZeXfR9+lF38PK6pyZmc8XyalfA+Nr8LmSJKl2flaMnL0d+C3w9bJz5fcYlfzd/2ZR/wng25QSI2Tm0sy8MTNfLZIkZ7Ph/db3MnNZZj5H6X5seg2u7SLgf5dNXf8EcEkn9fcqvov/KT7/8Mx8oW0dYEtK9zivZeYtwHU1ild6S3qrzK2U1LHDMvOm1oOI6AecQykBs3VR3D8imjNzTXG8rKz9jsDk1qlAhT50/ke/PYOA/26n/BJKT5nmFEOCfwKckZmvd9LXsk7OrXc+M/8aEc9ReqL15+6FvIEdgLvb9L2S0rU9VhT/T1n9VZRubCRJ0pvHevdGbZTfY1Tyd7+8/uNFGyJiO2A2pdHE/Sk9QP9LJ5+1ru0bkZl3RsTLwPsi4mlgF2BuJ03mZ+Z7u+h2B2BZMT2q1eOUvgdJVXBEjaS2PgOMBCZn5lb8bdhs+VoqWfZ+GfDbzBxQ9toyMz9d6QcWo1n2pDSVaT2Z+Xpm/t/MHA38HaWpQ8e0E8d6zbr4yHWjZ6K0k8M2lEb0vFwU9yur+85u9PsUpcRVa99bUBoN9GQX7SRJUu9Qfi9Qyd/98hG7Q4s2UJr2lMC44n7raNa/1+qsbTWxlruo+LxPAD/NzFe62W9bTwFDYv0NJobyt++hq/snSW2YqJHUVn9K69I8H6VV/b/URf3rgHdFxCciYpPi9e5i8d1ORUS/iHgfcC1wF3B9O3X2i4jdIqIZeJHSVKjWkT1/pp0F7SpwcES8NyI2pbRWzZ3F0OIVlG4qjo6I5og4gdK88FZ/BgYX7dpzGXB8RIyP0nbjXy/6fqyKGCVJ0ptbJX/3PxuljRqGAKcAVxTl/Smt9/J8RAwCPttO/zMjYnBxP/b5sraVWgGsZcN7pUsorZ1zNHBxN/tsz52UHnb9n+I+8P3Ah/nbej3V3q9Jb1kmaiS19W1KK/o/C8wH5nVWuVjH5kBKW2w/RWlqzzeBzTpp9r2IeInSH+5vU1rTZkqbIbOt3gn8lFKS5iFKc8VbdzP4DnBksRvC7IquruQySgmo5yiN5Dmq7NynKN0srQTGsP50rFuAB4D/iYhn23aamTcD/1Zcz9OUkjzTuhGXJEnqJSr8u38tsJDS5gW/oLTFNcD/pbTA8AtF+dXtfMRllBbofaR4fa2dOp3Ft4rS2ja/K9Yi3KsoX05pylbSzmjm7srM1yjt4PlBSveP5wLHZObDRZULKC2+/HxE/OyNfp70VhCZjkSTJEmSpLeKiLgQeCozu7uTlKSNwMWEJUmSJOktotiV6ghgj56NRFJH6jr1KSKmRMTiiFgaEae1cz4iYnZx/r6ImNBV24j4alF3UUT8KiJaV04fFhGri/JFEXFePa9NkiRJknqTiPgqcD/wrcx8tKfjkdS+uk19Khb+/CNwALAc+D0wPTMfLKtzMPCPwMHAZOA7mTm5s7YRsVVmvli0/ydgdGaeVGSGr8vMsXW5IEmSJEmSpDqr54iaScDSzHykWGBqDjC1TZ2pwMVZMh8YEBHbd9a2NUlT2AK3e5MkSZIkSQ2inomaQcCysuPlRVkldTptGxGzImIZpZ1avlhWb3hE3BMRv42Ifd74JUiSJEmSJG089VxMONopazv6paM6nbbNzDOAMyLidOBkStvsPg0MzcyVEbEn8LOIGNNmBA4RMQOYAbDFFlvsueuuu1Z6PZIkqYYWLlz4bGYO7Ok43sre/va357Bhw3o6DEmS3pI6uheqZ6JmOTCk7Hgw8FSFdTatoC3AZcAvgC9l5qvAqwCZuTAi/gS8C1hQ3iAzzwfOB5g4cWIuWLAASZK08UXE4z0dw1vdsGHD8F5IkqSe0dG9UD2nPv0eGBERwyNiU2AaMLdNnbnAMcXuT3sBL2Tm0521jYgRZe0PBR4uygcWixATETsBI4BH6nd5kiRJkiRJtVW3ETWZ2RIRJwO/BJqBCzPzgYg4qTh/HnA9pR2flgKrgOM7a1t0fWZEjATWAo8DJxXl+wJfiYgWYA1wUmY+V6/rkyRJkiRJqrV6Tn0iM6+nlIwpLzuv7H0CMyttW5R/pIP6VwFXvZF4JUmSJEmSelJdEzWS3vxef/11li9fziuvvNLToUhqUH379mXw4MFssskmPR2KJEnSm56JGuktbvny5fTv359hw4YR0d6Ga5JUvcxk5cqVLF++nOHDh/d0OJIkSW969VxMWFIv8Morr7DtttuapJFUFxHBtttu66g9SZKkCpmokWSSRlJd+W+MJElS5UzUSJIkSZIktXHppTBsGDQ1lX5eeunG+VzXqJEkSZIkSSpz6aUwYwasWlU6fvzx0jHAUUfV97MdUSPpTSEi+MxnPrPu+KyzzuLLX/7yRo1hyy237LLOl7/8Zc466ywA/u7v/q7Des8//zznnntup321tn/ssccYO3ZsNyJtv//O4umO1atX8773vY81a9bUpL+OnHDCCWy33XYbXHtH5eVeeeUVJk2axO67786YMWP40pe+tO7cOeecw5gxYxg7dizTp09ftzbK4sWLGT9+/LrXVlttxbe//e2aX1f578jG8J3vfIexY8cyZsyY9a5n3rx5jBw5kl122YUzzzyzw/bPP/88Rx55JLvuuiujRo3ijjvu6PK7GjZsGLvtthvjx49n4sSJALz22mvsu+++tLS01O9iJUmSNpIzzvhbkqbVqlWl8nozUVNj11xzDZ/+9Kd7Ogyp19lss824+uqrefbZZ7vdNjNZu3ZtHaLq3H//9393eK6zRE1rvJ2170p7/b+R/spdeOGFHHHEETQ3N9ekv44cd9xxzJs3r+Lycpttthm33HIL9957L4sWLWLevHnMnz+fJ598ktmzZ7NgwQLuv/9+1qxZw5w5cwAYOXIkixYtYtGiRSxcuJB+/fpx+OGH1+XaNpb777+fH/7wh9x1113ce++9XHfddSxZsoQ1a9Ywc+ZMbrjhBh588EEuv/xyHnzwwXb7OOWUU5gyZQoPP/ww9957L6NGjarou/r1r3/NokWLWLBgAQCbbrop+++/P1dccUXdr1uSJKnenniie+W1ZKKmxh5//HFuv/32ng5D6nX69OnDjBkzOOecczY4d/bZZzN27FjGjh277qn+Y489xqhRo/iHf/gHJkyYwG233cauu+7KJz/5ScaOHctRRx3FTTfdxN57782IESO466671vV32GGHseeeezJmzBjOP//8LmObNWsWI0eO5AMf+ACLFy9eV946Aufll1/mQx/6ELvvvjtjx47liiuu4LTTTuNPf/oT48eP57Of/ewG8S5btmy9ETwtLS0ce+yxjBs3jiOPPJJVRfq+7Wib1pFGbfsvj6er7+xTn/oUY8aM4cADD2T16tUbXO+ll17K1KlT1x2///3vX3fdK1eu7Pbon47su+++bLPNNhWXl4uIddf7+uuv8/rrr69bsLalpYXVq1fT0tLCqlWr2GGHHTZof/PNN7Pzzjuz4447bnCuvd+Prr67jn5Hyt17773su+++jB49mqamJiJivZFA1XjooYfYa6+96NevH3369OF973sf11xzDXfddRe77LILO+20E5tuuinTpk3j2muv3aD9iy++yK233sqJJ54IlJItAwYMWK9OZ99VW4cddhiXbqzJ25IkSXU0dGj3ymvJRE2NRUSPPNmXGsHMmTO59NJLeeGFF9aVLVy4kB/96EfceeedzJ8/nx/+8Ifcc889QGkqyzHHHMM999zDjjvuyNKlSznllFO47777ePjhh7nsssu4/fbbOeuss/j617++rs8LL7yQhQsXsmDBAmbPns3KlSs7jGnhwoXMmTOHe+65h6uvvprf//73G9SZN28eO+ywA/feey/3338/U6ZM4cwzz2TnnXdm0aJFfOtb32o33nKLFy9mxowZ3HfffWy11VZdTptqr/9KvrMlS5Ywc+ZMHnjgAQYMGMBVV121XtvXXnuNRx55hGHDhq0rW7p0KSNGjADgvvvuY7fddluvzT777LPeNJnW10033dTpNbxRa9asYfz48Wy33XYccMABTJ48mUGDBnHqqacydOhQtt9+e972trdx4IEHbtB2zpw5TJ8+vd1+O/r96Oi7q+R35JVXXuHjH/84Z511Fg8++CBnnHEGp5566nrT+6r5HseOHcutt97KypUrWbVqFddffz3Lli3jySefZMiQIevqDR48mCeffHKD9o888ggDBw7k+OOPZ4899uCTn/wkL7/8cpffVURw4IEHsueee66X7Bw7dmy71y9JktTbzJoF/fqtX9avX6m83lxMuMaamppYu3Ytmel2pOp1vv71r/Pwww/XtM9dd92Vz3/+8xXV3WqrrTjmmGOYPXs2m2++OQC33347hx9+OFtssQUARxxxBLfddhuHHnooO+64I3vttde69sOHD1+XRBgzZgz7778/EcFuu+3GY489tq7e7NmzueaaawBYtmwZS5YsYdttt203pttuu43DDz+cfsW/0oceeugGdXbbbTdOPfVUPve5z3HIIYewzz778Je//GWDem3jLTdkyBD23ntvAI4++mhmz57Nqaee2un31ZHOvrPhw4czfvx4APbcc8/1vheAZ599dr0RFY8//jiDBg2iqamU17/vvvsYN27cem1uu+22quJ8o5qbm1m0aBHPP/88hx9+OPfffz+DBg3i2muv5dFHH2XAgAF89KMf5Sc/+QlHH330unavvfYac+fO5Rvf+Ea7/bb3+/HOd76zw++ukt+Rm266iQkTJjBp0iQAxo0bx7x589b7O1HN9zhq1Cg+97nPccABB7Dllluy++6706dPHzJzg7rt/U1qaWnh7rvv5rvf/S6TJ0/mlFNO4cwzz+SrX/0q0PF39bvf/Y4ddtiBZ555hgMOOIBdd92Vfffdl+bmZjbddFNeeukl+vfv3+3rkSRJerNoXTD4jDNK052GDi0laeq9kDA4oqbmWv9npr2bZEld++d//mcuuOCCdU/1O/tvqTUR0WqzzTZb976pqWndcVNT07oFTn/zm99w0003cccdd3Dvvfeyxx57rFtstiNdJV3f9a53sXDhQnbbbTdOP/10vvKVr1QUb2ef0Xrcp0+f9UbpdRUrdP6dlX9Hzc3NGyz8uvnmm6/3GYsWLVovMbNw4cINEjU9NaKm1YABA3j/+9/PvHnzuOmmmxg+fDgDBw5kk0024Ygjjthg7Z4bbriBCRMm8I53vGODvjr7/ejsu+vqd+T+++9fbyTS3XffzYQJE9arU+33eOKJJ3L33Xdz6623ss022zBixAgGDx7MsmXL1tVZvnx5u1PABg8ezODBg5k8eTIARx55JHffffe68x19V619bbfddhx++OHrTS189dVX6du3b6cxS5Ik9QZHHQWPPQZr15Z+bowkDTiipuZaEzVr165d917qLSod+VJP22yzDR/72Me44IILOOGEE9h333057rjjOO2008hMrrnmGi655JKq+3/hhRfYeuut6devHw8//DDz58/vtH7557e0tPDzn/+cv//7v1+vzlNPPcU222zD0UcfzZZbbsmPf/xjZs6cyUsvvVRxXE888QR33HEH73nPe7j88st573vfC8A73vEOnnnmGVauXMmWW27Jddddx5QpU+jfv3+H/b+R72zrrbdmzZo1vPLKK/Tt25d77713XaJiyZIlXHvttXzta19br01PjKhZsWIFm2yyCQMGDGD16tXcdNNNfO5zn2PgwIHMnz+fVatWsfnmm3PzzTev25Wo1eWXX97htKfu/n5AZb8j2267LbfccgsAf/zjH7n66qs3SCBV+z0+88wzbLfddjzxxBNcffXV3HHHHfTv358lS5bw6KOPMmjQIObMmcNll122Qdt3vvOdDBkyhMWLFzNy5EhuvvlmRo8eve58e9/Vyy+/zNq1a+nfvz8vv/wyv/rVr/jiF78IlNYwak2SSZIkqTpmEmqsPFEjqTqf+cxn1u3+NGHCBI477jgmTZrE5MmT+eQnP8kee+xRdd9TpkyhpaWFcePG8W//9m8dTkVqNWHCBD7+8Y8zfvx4PvKRj7DPPvtsUOcPf/gDkyZNYvz48cyaNYsvfOELbLvttuy9996MHTt23WK/nRk1ahQXXXQR48aN47nnnlu3e9wmm2zCF7/4RSZPnswhhxzCrrvuCtBp/2/0OzvwwAPXLYq+aNEi1q5dy+67785XvvKVdXHWwvTp03nPe97D4sWLGTx4MBdccEGn5QAHH3wwTz31FE8//TT77bcf48aN493vfjcHHHAAhxxyCJMnT+bII49kwoQJ7Lbbbqxdu5YZM2asa79q1SpuvPFGjjjiiHZj6u7vB1T2OzJ9+nT++te/MnbsWGbMmMHll1/e4XS77vrIRz7C6NGj+fCHP8z3v/99tt56a/r06cP3vvc9DjroIEaNGsXHPvYxxowZs65N6/cI8N3vfpejjjqKcePGsWjRonUJ246+qz//+c+8973vZffdd2fSpEl86EMfYsqUKUBpJ6iDDz64JtclSZL0VhVv5Sk6EydOzNZtRWvlhz/8IWeffTaLFi1ab5i89Gb10EMPMWrUqJ4OQ28i99xzD2effTaXXHIJu+yyC/fcc4/rjagiRxxxBN/4xjcYOXLkBufa+7cmIhZm5sQNKmujqce9kCRJqkxH90KOqKmx1nUKHFEjqbfaY4892G+//XjhhRdoamoySaOKvPbaaxx22GHtJmkkSZJUOdeoqTGnPklqBCeccAJQWk9FqsSmm27KMccc09NhSJIk9XqOqKkxEzWSJEmSJKlaJmpqzESNJEmSJEmqlomaGjNRI0mSJEmSqmWipsZaFxN+K++mpd7H31dJ9eS/MZIkSZUzUVNjrSNq1qxZ08ORSJXp27cvK1eu9H+kJNVFZrJy5Ur69u3b06FIkiT1Cu76VGPNzc2ATw/VewwePJjly5ezYsWKng5FUoPq27cvgwcP7ukwJEmSegUTNTXWOvXJNWrUW2yyySYMHz68p8OQJEmSJOHUp5pzMWFJkiRJklQtEzU11pqoceqTJEmSJEnqLhM1NebUJ0mSJEmSVC0TNTXmrk+SJEmSJKlaJmpqzF2fJEmSJElStUzU1JhTnyRJkiRJUrXqmqiJiCkRsTgilkbEae2cj4iYXZy/LyImdNU2Ir5a1F0UEb+KiB3Kzp1e1F8cEQfV89o64q5PkiRJkiSpWnVL1EREM/B94IPAaGB6RIxuU+2DwIjiNQP4QQVtv5WZ4zJzPHAd8MWizWhgGjAGmAKcW/SzUbnrkyRJkiRJqlY9R9RMApZm5iOZ+RowB5japs5U4OIsmQ8MiIjtO2ubmS+Wtd8CyLK+5mTmq5n5KLC06Gejap365GLCkiRJkiSpu+qZqBkELCs7Xl6UVVKn07YRMSsilgFHUYyoqfDz6s6pT5IkSZIkqVr1TNREO2Vt5wN1VKfTtpl5RmYOAS4FTu7G5xERMyJiQUQsWLFiRbuBvxFOfZIkSZIkSdWqZ6JmOTCk7Hgw8FSFdSppC3AZ8JFufB6ZeX5mTszMiQMHDqzgMrrHETWSJEmSJKla9UzU/B4YERHDI2JTSgv9zm1TZy5wTLH7017AC5n5dGdtI2JEWftDgYfL+poWEZtFxHBKCxTfVa+L64iJGkmSJEmSVK0+9eo4M1si4mTgl0AzcGFmPhARJxXnzwOuBw6mtPDvKuD4ztoWXZ8ZESOBtcDjQGt/D0TElcCDQAswMzM3+oq+Tn2SJEmSJEnVqluiBiAzr6eUjCkvO6/sfQIzK21blH+kneqt52YBs6qNtxbc9UmSJEmSJFWrnlOf3pKam5sBpz5JkiRJkqTuM1FTY60japz6JEmSWkXEv0TEAxFxf0RcHhF9I2KbiLgxIpYUP7cuq396RCyNiMURcVBZ+Z4R8Yfi3OwobjyKNfquKMrvjIhhG/8qJUlSLZioqTEXE5YkSeUiYhDwT8DEzBxLaf29acBpwM2ZOQK4uTgmIkYX58cAU4BzI6K56O4HwAxKmyaMKM4DnAj8JTN3Ac4BvrkRLk2SJNWBiZoaM1EjSZLa0QfYPCL6AP2Ap4CpwEXF+YuAw4r3U4E5mflqZj5KadOFSRGxPbBVZt5RrPN3cZs2rX39FNi/dbSNJEnqXUzU1FjrPZGJGkmSBJCZTwJnAU8ATwMvZOavgHdk5tNFnaeB7Yomg4BlZV0sL8oGFe/blq/XJjNbgBeAbetxPZIkqb5M1NSYI2okSVK5Yu2ZqcBwYAdgi4g4urMm7ZRlJ+WdtWkvnhkRsSAiFqxYsaKTMCRJUk8wUVNj7vokSZLa+ADwaGauyMzXgauBvwP+XExnovj5TFF/OTCkrP1gSlOllhfv25av16aYXvU24Ln2gsnM8zNzYmZOHDhwYA0uT5Ik1ZKJmhpz1ydJktTGE8BeEdGvWDdmf+AhYC5wbFHnWODa4v1cYFqxk9NwSosG31VMj3opIvYq+jmmTZvWvo4EbklvRiRJ6pX69HQAjcapT5IkqVxm3hkRPwXuBlqAe4DzgS2BKyPiRErJnI8W9R+IiCuBB4v6MzNzTdHdp4EfA5sDNxQvgAuASyJiKaWRNNM2wqVJkqQ6MFFTYyZqJElSW5n5JeBLbYpfpTS6pr36s4BZ7ZQvAMa2U/4KRaJHkiT1bk59qjF3fZIkSZIkSdUyUVNjjqiRJEmSJEnVMlFTY627Prl+nyRJkiRJ6i4TNTXm1CdJkiRJklQtEzU15tQnSZIkSZJULRM1NdaaqHHqkyRJkiRJ6i4TNTXWOvVpzZo1PRyJJEmSJEnqbUzU1FjrYsJOfZIkSZIkSd1loqbGnPokSZIkSZKqZaKmxtz1SZIkSZIkVctETY2565MkSZIkSaqWiZoaM1EjSZIkSZKqZaKmxpz6JEmSJEmSqmWipsbc9UmSJEmSJFXLRE2NueuTJEmSJEmqlomaGnPqkyRJkiRJqpaJmhpzMWFJkiRJklQtEzU1ZqJGkiRJkiRVy0RNjTn1SZIkSZIkVctETY2565MkSZIkSapWXRM1ETElIhZHxNKIOK2d8xERs4vz90XEhK7aRsS3IuLhov41ETGgKB8WEasjYlHxOq+e19aR1hE17vokSZIkSZK6q26JmohoBr4PfBAYDUyPiNFtqn0QGFG8ZgA/qKDtjcDYzBwH/BE4vay/P2Xm+OJ1Un2urHMRQUQ4okaSJEmSJHVbPUfUTAKWZuYjmfkaMAeY2qbOVODiLJkPDIiI7Ttrm5m/ysyWov18YHAdr6EqTU1NJmokSZIkSVK31TNRMwhYVna8vCirpE4lbQFOAG4oOx4eEfdExG8jYp9qA3+jTNRIkiRJkqRq9Klj39FOWduFWzqq02XbiDgDaAEuLYqeBoZm5sqI2BP4WUSMycwX27SbQWmaFUOHDu3yIqphokaSJEmSJFWjniNqlgNDyo4HA09VWKfTthFxLHAIcFQWq/Zm5quZubJ4vxD4E/Cutsqf7+IAACAASURBVEFl5vmZOTEzJw4cOLDKS+tcU1OTiwlLkiRJkqRuq2ei5vfAiIgYHhGbAtOAuW3qzAWOKXZ/2gt4ITOf7qxtREwBPgccmpmrWjuKiIHFIsRExE6UFih+pI7X1yEXE5YkSZIkSdWo29SnzGyJiJOBXwLNwIWZ+UBEnFScPw+4HjgYWAqsAo7vrG3R9feAzYAbi62w5xc7PO0LfCUiWoA1wEmZ+Vy9rq8zTn2SJEmSJEnVqOcaNWTm9ZSSMeVl55W9T2BmpW2L8l06qH8VcNUbibdWTNRIkiRJkqRq1HPq01uWU58kSZIkSVI1TNTUQXNzs4kaSZIkSZLUbSZq6sBdnyRJkiRJUjVM1NSBU58kSZIkSVI1TNTUgYsJS5IkSZKkapioqQMTNZIkSZIkqRomaurAqU+SJEmSJKkaJmrqwF2fJEmSJElSNUzU1EFEuOuTJEmSJEnqNhM1deAaNZIkSZIkqRomaurARI0kSZIkSaqGiZo6cDFhSZIkSZJUDRM1deBiwpIkSZIkqRomauqgqanJxYQlSZIkSVK3maipA6c+SZIkSZKkapioqQMXE5YkqfFFRFNEbFVh3QER8dOIeDgiHoqI90TENhFxY0QsKX5uXVb/9IhYGhGLI+KgsvI9I+IPxbnZERFF+WYRcUVRfmdEDKv19UqSpI3DRE0dmKiRJKkxRcRlEbFVRGwBPAgsjojPVtD0O8C8zNwV2B14CDgNuDkzRwA3F8dExGhgGjAGmAKcGxHNRT8/AGYAI4rXlKL8ROAvmbkLcA7wzTd8sZIkqUeYqKkDpz5JktSwRmfmi8BhwPXAUOATnTUoRt3sC1wAkJmvZebzwFTgoqLaRUWfFOVzMvPVzHwUWApMiojtga0y844sLYZ3cZs2rX39FNi/dbSNJEnqXUzU1IG7PkmS1LA2iYhNKCVIrs3M14GudhDYCVgB/Cgi7omI/yxG5LwjM58GKH5uV9QfBCwra7+8KBtUvG9bvl6bzGwBXgC2re4SJUlSTzJRUwfu+iRJUsP6f8BjwBbArRGxI/BiF236ABOAH2TmHsDLFNOcOtDeSJjspLyzNht2HjEjIhZExIIVK1Z0EoYkSeoJJmrqwKlPkiQ1psycnZmDMvPgLHkc2K+LZsuB5Zl5Z3H8U0qJmz8X05kofj5TVn9IWfvBwFNF+eB2ytdrExF9gLcBz3VwDedn5sTMnDhw4MAur1mSJG1cJmrqoKmpiTVr1vR0GJIkqcYi4h0RcUFE3FAcjwaO7axNZv4PsCwiRhZF+1NaiHhuWdtjgWuL93OBacVOTsMpLRp8VzE96qWI2KtYf+aYNm1a+zoSuCUd3itJUq/Up6cDaERNTU20tLT0dBiSJKn2fgz8CDijOP4jcAXFQsGd+Efg0ojYFHgEOJ7SA7MrI+JE4AngowCZ+UBEXEkpmdMCzMzM1idAny5i2By4oXhRfP4lEbGU0kiaaW/oKiVJUo8xUVMHTn2SJKlhvT0zr4yI06G0cG9EdDmMNjMXARPbObV/B/VnAbPaKV8AjG2n/BWKRI8kSerdnPpUB+76JElSw3o5IralWKg3IvaitMOSJElSTTiipg4iwl2fJElqTP9KaT2YnSPid8BASmvCSJIk1YSJmjpoampyRI0kSQ0oM++OiPcBIyltib04M1/v4bAkSVIDMVFTByZqJElqTBFxTJuiCcVI2ot7JCBJktRwTNTUgYkaSZIa1rvL3veltBjw3YCJGkmSVBMVJWoiYmxm3l/vYBqFiRpJkhpTZv5j+XFEvA24pIfCkSRJDajSXZ/Oi4i7IuIfImJApZ1HxJSIWBwRSyPitHbOR0TMLs7fFxETumobEd+KiIeL+teUxxMRpxf1F0fEQZXGWWsmaiRJestYBYzo6SAkSVLjqChRk5nvBY4ChgALIuKyiDigszYR0Qx8H/ggMBqYHhGj21T7IKWbmxHADOAHFbS9ERibmeOAPwKnF21GA9OAMcAU4Nyin43OXZ8kSWpMEfHziJhbvK4DFgPX9nRckiSpcVS8Rk1mLomILwALgNnAHhERwOcz8+p2mkwClmbmIwARMQeYCjxYVmcqcHGWshrzI2JARGwPDOuobWb+qqz9fP62JeZUYE5mvgo8GhFLixjuqPQaa8URNZIkNayzyt63AI9n5vKeCkaSJDWeSteoGQccD3yI0oiWDxfbU+5AKRHSXqJmELCs7Hg5MLmCOoMqbAtwAnBFWV/z2+lrozNRI0lSY8rM3/Z0DJIkqbFVOqLme8APKY2eWd1amJlPFaNs2hPtlLWdD9RRnS7bRsQZlJ5kXdqNzyMiZlCaZsXQoUPbafLGRYSJGkmSGkhEvEQ79xWU7j8yM7fayCFJkqQGVWmi5mBgdWauAYiIJqBvZq7KzI52OlhOaU2bVoOBpyqss2lnbSPiWOAQYP/822IwlXwemXk+cD7AxIkT67KQTHNzs4kaSZIaSGb27+kYJEnSW0Oluz7dBGxedtyvKOvM74ERETE8IjaltNDv3DZ15gLHFLs/7QW8kJlPd9Y2IqYAnwMOzcxVbfqaFhGbRcRwSgsU31Xh9dVUU1OTiwlLktTAImK7iBja+urpeCRJUuOodERN38z8a+tBZv41Ivp11iAzWyLiZOCXQDNwYWY+EBEnFefPA66nNFpnKaXtLY/vrG3R9feAzYAbS2sZMz8zTyr6vpLSYsUtwMzWEUAbm1OfJElqTBFxKPAfwA7AM8COwEOUdp2UJEl6wypN1LwcERMy826AiNgTWN1FGzLzekrJmPKy88reJzCz0rZF+S6dfN4sYFZXcdVbU1MTa9b0SI5IkiTV11eBvYCbMnOPiNgPmN7DMUmSpAZSaaLmn4H/iojWNV+2Bz5en5B6P6c+SZLUsF7PzJUR0RQRTZn564j4Zk8HJUmSGkdFiZrM/H1E7AqMpLS7wcOZ+XpdI+vFnPokSVLDej4itgRuBS6NiGcoTbmWJEmqiUoXEwZ4NzAO2AOYHhHH1Cek3s9dnyRJaiwRcWRE9AWmUlpX71+AecCfgA/3ZGySJKmxVDSiJiIuAXYGFgGti68kcHGd4urVnPokSVLDOQo4l1Jy5nLgV5l5Uc+GJEmSGlGla9RMBEan2YeKOPVJkqTGkpmHR8RWwOHAPwEXRMS1wOWZeWvPRidJkhpJpVOf7gfeWc9AGklTU5OJGkmSGkxmvpiZF2XmB4HdKI00/m5ELOvh0CRJUgOpdETN24EHI+Iu4NXWwsw8tC5R9XImaiRJalwRsTVwBKUdMLcBrurZiCRJUiOpNFHz5XoG0WhM1EiS1Fgioj9wGDAdmADMBb4G/Nqp4ZIkqZYq3Z77txGxIzAiM2+KiH5Ac31D671cTFiSpIbzKPBL4AfAvMx8vYfjkSRJDarSXZ8+BcygNLx3Z2AQcB6wf/1C671cTFiSpIYzNDNX9XQQkiSp8VW6mPBMYG/gRYDMXAJsV6+gerumpibWrFnTdUVJktQrmKSRJEkbS6WJmlcz87XWg4joAzi3pwNNTaWv1elPkiRJkiSpOypN1Pw2Ij4PbB4RBwD/Bfy8fmH1bhEB4PQnSZIkSZLULZXu+nQacCLwB+DvgeuB/6xXUL1dc3NpneW1a9euey9JknqviPg5nYwmzsxDN2I4kiSpgVW669Na4IfFS11w6pMkSQ3nrOLnEcA7gZ8Ux9OBx3oiIEmS1Jgq3fXpUdp5ipSZO9U8ogbg1CdJkhpLZv4WICK+mpn7lp36eUTc2kNhSZKkBlTp1KeJZe/7Ah+ltFW32tE6osadnyRJajgDI2KnzHwEICKGAwN7OCZJktRAKp36tLJN0bcj4nbgi7UPqfdz6pMkSQ3rX4DfRMQjxfEwSuv3SZIk1USlU58mlB02URph078uETUApz5JktSYMnNeRIwAdi2KHs7MV3syJkmS1Fgqnfr0H2XvWygtmvexmkfTIMp3fZIkSY0jIvoB/wrsmJmfiogRETEyM6/r6dgkSVJjqHTq0371DqSROPVJkqSG9SNgIfCe4ng58F+AiRpJklQTlU59+tfOzmfm2bUJpzG0Tn1yMWFJkhrOzpn58YiYDpCZq6P1D78kSVINdGfXp3cDc4vjDwO3AsvqEVRv1zqixqlPkiQ1nNciYnMgASJiZ8A1aiRJUs1Umqh5OzAhM18CiIgvA/+VmZ+sV2C9mVOfJElqWF8C5gFDIuJSYG/guB6NSJIkNZRKEzVDgdfKjl+jtB2l2uGIGkmSGk9ENAFbA0cAewEBnJKZz/ZoYJIkqaFUmqi5BLgrIq6hNNT3cODiukXVy5mokSSp8WTm2og4OTOvBH7R0/FIkqTGVOmuT7Mi4gZgn6Lo+My8p35h9W5OfZIkqWHdGBGnAlcAL7cWZuZzPReSJElqJJWOqAHoB7yYmT+KiIERMTwzH61XYL2Zuz5JktSwTih+ziwrS2CnHohFkiQ1oKZKKkXEl4DPAacXRZsAP6lXUL2dU58kSWpMmTm8nVdFSZqIaI6IeyLiuuJ4m4i4MSKWFD+3Lqt7ekQsjYjFEXFQWfmeEfGH4tzs1q3BI2KziLiiKL8zIobV9solSdLGUlGihtKaNIdSDPHNzKeA/vUKqrdz6pMkSY0pIvpFxBci4vzieEREHFJh81OAh8qOTwNuzswRwM3FMRExGpgGjAGmAOdGRHPR5gfADGBE8ZpSlJ8I/CUzdwHOAb5Z5SVKkqQeVmmi5rUsZR0SICK2qKRRREwpngQtjYjT2jkfxdOgpRFxX0RM6KptRHw0Ih6IiLURMbGsfFhErI6IRcXrvAqvreYcUSNJUsP6EaXdL/+uOF4OfK2rRhExGPgQ8J9lxVOBi4r3FwGHlZXPycxXi2nmS4FJEbE9sFVm3lHcl13cpk1rXz8F9m8dbSNJknqXShM1V0bE/wMGRMSngJuAH3bWoHjy833gg8BoYHrxhKjcB/nbE6EZlJ4SddX2fkrbYt7azsf+KTPHF6+TKry2mjNRI0lSw9o5M/8deB0gM1dT2qa7K98G/g9QfnPwjsx8uujnaWC7onwQsKys3vKibFDxvm35em0yswV4Adi24quSJElvGl0uJlw8jbkC2BV4ERgJfDEzb+yi6SRgaWY+UvQzh9LTngfL6kwFLi6eCs2PiAHF06JhHbXNzIeKsoovcmNrjc2pT5IkNZzXImJz/jbKeGfg1c4aFFOjnsnMhRHx/go+o72bnOykvLM27cUzg9IDMoYOHVpBOJIkaWPqMlGTmRkRP8vMPYGukjPl2nsaNLmCOoM6KG/btj3DI+IeSgmlL2TmbW0rbIybk9YRNe76JElSw/kSMA8YEhGXAnsDx3XRZm/g0Ig4GOgLbBURPwH+HBHbZ+bTxYOqZ4r6y4EhZe0HA08V5YPbKS9vszwi+gBvA9rdMjwzzwfOB5g4caJPlSRJepOpdOrT/Ih4dzf7ruTJTjVPjDryNDA0M/cA/hW4LCK22qCTzPMzc2JmThw4cGAXXVbHxYQlSWpMxYjiIyglZy4HJmbmb7poc3pmDs7MYZQWCb4lM48G5gLHFtWOBa4t3s8FphU7OQ2nNEX8rmJ61EsRsVcx4vmYNm1a+zqy+AxvRCRJ6oW6HFFT2A84KSIeo7TzU1AabDOukzYdPQ2qpM6mFbRdT2a+SjH0uBha/CfgXcCCztrVQ+vUJ9eokSSpMZRveFB4uvg5NCKGZubdVXR7JqV1AE8EngA+CpCZD0TElZSmi7cAMzOzdZjup4EfA5sDNxQvgAuASyJiKaWRNNOqiEeSJL0JdJqoKW48nqC0qG93/R4YUTwJepLSDcP/blNnLnBysQbNZOCFYvjvigrato11IPBcZq6JiJ0oPX16pIq437Dm5tIOmiZqJElqGP9R/OwLTATupfTgahxwJ/DeSjopRt/8pni/Eti/g3qzgFntlC8AxrZT/gpFokeSJPVuXY2o+RkwITMfj4irMvMjlXacmS0RcTLwS6AZuLB4QnRScf484HrgYErbTq4Cju+sLUBEHA58FxgI/CIiFmXmQcC+wFciogVYA5yUme3Oza43d32SJKmxZOZ+sG6DgxmZ+YfieCxwak/GJkmSGktXiZrytWJ26m7nmXk9pWRMedl5Ze8TmFlp26L8GuCadsqvAq7qboz14NQnSZIa1q6tSRqAzLw/Isb3ZECSJKmxdJWoyQ7eqxOOqJEkqWE9HBH/CfyE0r3R0cBDPRuSJElqJF0lanaPiBcpjazZvHgPf1tMeINdleSuT5IkNbDjKC3oe0pxfCvwgx6LRpIkNZxOEzWZ2byxAmkkjqiRJKnxREQzcF1mfgA4p6fjkSRJjamppwNoRCZqJElqPMUW2asi4m09HYskSWpcXU19UhWc+iRJUsN6BfhDRNwIvNxamJn/1HMhSZKkRmKipg5ad31as2ZND0ciSZJq7BfFS5IkqS5M1NSBU58kSWpYVwC7UNrx6U+Z+UoPxyNJkhqMa9TUgVOfJElqLBHRJyL+HVgOXERpe+5lEfHvEbFJz0YnSZIaiYmaOnBEjSRJDedbwDbA8MzcMzP3AHYGBgBn9WhkkiSpoZioqQMTNZIkNZxDgE9l5kutBZn5IvBp4OAei0qSJDUcEzV1YKJGkqSGk9nOnOZiy27nOkuSpJoxUVMHrbs+maiRJKlhPBgRx7QtjIijgYd7IB5JktSg3PWpDhxRI0lSw5kJXB0RJwALKY2ieTewOXB4TwYmSZIai4maOnDXJ0mSGktmPglMjoj/BYwBArghM2/u2cgkSVKjMVFTB46okSSpMWXmLcAtPR2HJElqXK5RUwcmaiRJkiRJUjVM1NSBiRpJkiRJklQNEzV14K5PkiRJkiSpGiZq6sARNZIkSZIkqRomaurAXZ8kSZIkSVI1TNTUgSNqJEmSJElSNUzU1IGJGkmSJEmSVA0TNXXgYsKSJEmSJKkaJmrqwBE1kiRJkiSpGiZq6sDFhCVJkiRJUjVM1NSBI2okSZIkSVI1TNTUgYkaSZIkSZJUDRM1dWCiRpIkSZIkVcNETR2465MkSZIkSapGXRM1ETElIhZHxNKIOK2d8xERs4vz90XEhK7aRsRHI+KBiFgbERPb9Hd6UX9xRBxUz2vrjCNqJEmSJElSNeqWqImIZuD7wAeB0cD0iBjdptoHgRHFawbwgwra3g8cAdza5vNGA9OAMcAU4Nyin43OXZ8kSZIkSVI16jmiZhKwNDMfyczXgDnA1DZ1pgIXZ8l8YEBEbN9Z28x8KDMXt/N5U4E5mflqZj4KLC362egcUSNJkiRJkqpRz0TNIGBZ2fHyoqySOpW0rebzNpqmpiYTNZIkSZIkqVvqmaiJdsrazgXqqE4lbav5PCJiRkQsiIgFK1as6KLL6pmokSRJkiRJ3VXPRM1yYEjZ8WDgqQrrVNK2ms8jM8/PzImZOXHgwIFddFm9iDBRI0mSJEmSuqWeiZrfAyMiYnhEbEppod+5berMBY4pdn/aC3ghM5+usG1bc4FpEbFZRAyntEDxXbW8oO5wRI0kSZIkSequPvXqODNbIuJk4JdAM3BhZj4QEScV588DrgcOprTw7yrg+M7aAkTE4cB3gYHALyJiUWYeVPR9JfAg0ALMzMw19bq+rjQ3N7vrkyRJkiRJ6pa6JWoAMvN6SsmY8rLzyt4nMLPStkX5NcA1HbSZBcx6AyHXjFOfJEmSJElSd9Vz6tNbmlOfJEmSJElSd5moqRMTNZIkSZIkqbtM1NSJU58kSZIkSVJ3maipE0fUSJIkSZKk7jJRUydNTU3u+iRJkoiIIRHx64h4KCIeiIhTivJtIuLGiFhS/Ny6rM3pEbE0IhZHxEFl5XtGxB+Kc7MjIoryzSLiiqL8zogYtrGvU5Ik1YaJmjpxRI0kSSq0AJ/JzFHAXsDMiBgNnAbcnJkjgJuLY4pz04AxwBTg3IhoLvr6ATADGFG8phTlJwJ/ycxdgHOAb26MC5MkSbVnoqZOTNRIkiSAzHw6M+8u3r8EPAQMAqYCFxXVLgIOK95PBeZk5quZ+SiwFJgUEdsDW2XmHVkatntxmzatff0U2L91tI0kSepdTNTUiYsJS5KktoopSXsAdwLvyMynoZTMAbYrqg0ClpU1W16UDSrety1fr01mtgAvANvW4xokSVJ9maipE0fUSJKkchGxJXAV8M+Z+WJnVdspy07KO2vTXhwzImJBRCxYsWJFZyFLkqQeYKKmTlxMWJIktYqITSglaS7NzKuL4j8X05kofj5TlC8HhpQ1Hww8VZQPbqd8vTYR0Qd4G/Bce7Fk5vmZOTEzJw4cOPCNXpokSaoxEzV14ogaSZIEUKwVcwHwUGaeXXZqLnBs8f5Y4Nqy8mnFTk7DKS0afFcxPeqliNir6POYNm1a+zoSuCV9YiRJUq/Up6cDaFQmaiRJUmFv4BPAHyJiUVH2eeBM4MqIOBF4AvgoQGY+EBFXAg9S2jFqZmauKdp9GvgxsDlwQ/GCUiLokohYSmkkzbR6X5QkSaoPEzV1YqJGkiQBZObttL+GDMD+HbSZBcxqp3wBMLad8lcoEj2SJKl3c+pTnbjrkyRJkiRJ6i4TNXXiiBpJkiRJktRdJmrqpLm52V2fJEmSJElSt5ioqROnPkmSJEmSpO4yUVMnTU1NrFmzpuuKkiRJkiRJBRM1ddLU1OTUJ0mSJEmS1C0maurEqU+SJEmSJKm7TNTUibs+SZIkSZKk7jJRUydOfZIkSZIkSd1loqZOHFEjSZIkSZK6y0RNnZiokSRJkiRJ3WWipk5cTFiSJEmSJHWXiZo6cUSNJEmSJEnqLhM1deJiwpIkSZIkqbtM1NSJI2okSZKk/9/e3Ud5UtV3Hn9/egABH3jGJcDMoDuagAriiKgnLBHQwbgMrnoYwyoCR5YsIGZDIso5OWezwaCsMbCgLEFWYCcSRBNms6yAAz5sXB7kWZ7C8DSMjKLDgrogODPf/aOqmd80/Qhdv+7pfr/O6fOrulW36tbt6fnd+ta9tyRJE2WgpiMDAwOsW7duqoshSZIkSZI2IQZqOuLQJ0mSJEmSNFGdBmqSLEpyX5IVSU4dZnuSnN1uvyPJvmPlTbJ9kmuS3N9+btemz0/yTJLb2p/zury2sfjWJ0mSJEmSNFGdBWqSzAHOBQ4F9gQ+nGTPIbsdCixof44DvjyOvKcCy6tqAbC8XR/0QFXt0/4c382VjY9z1EiSJEmSpInarMNj7wesqKoHAZJcCiwG7u7ZZzFwcTVjhK5Psm2SXYD5o+RdDBzY5r8I+A7wqQ6vY8L+9oaVrHry1zz59HP87Q0rh93nD942t8+lkiRJkiRJ012XQ592BR7tWV/Vpo1nn9HyvrqqVgO0nzv37LdHkluTfDfJ7770S3jxkgHKHjWSJEmSJGkCuuxRk2HShs6uO9I+48k71GpgblWtSfIW4B+S7FVVv9johMlxNMOsmDu3u14tGQhVBmokSZIkSdL4ddmjZhWwe8/6bsBj49xntLw/bYdH0X4+DlBVz1bVmnb5ZuAB4HVDC1VV51fVwqpauNNOO73ISxtb06PGtz5JkiRJkqTx6zJQcxOwIMkeSbYAlgDLhuyzDPho+/an/YGn2uFMo+VdBhzVLh8FXAGQZKd2EmKSvIZmguIHu7u80SX2qJEkSZIkSRPT2dCnqlqb5ETgKmAOcGFV3ZXk+Hb7ecCVwHuBFcDTwNGj5W0PfQZwWZJjgZXAh9r0A4A/T7IWWAccX1VPdHV9Y0kGaOZIliRJkiRJGp8u56ihqq6kCcb0pp3Xs1zACePN26avAQ4aJv0bwDdeYpEnzcDAgD1qJEmSJEnShHQ59GlWS+IcNZIkSZIkaUIM1HQkAwOs9/XckiRJkiRpAgzUdMTJhCVJkiRJ0kQZqOmIkwlLkiRJkqSJMlDTEXvUSJIkSZKkiTJQ05EMDDiZsCRJkiRJmhADNR2Jr+eWJEmSJEkTZKCmI83ruQ3USJIkSZKk8TNQ0xEnE5YkSZIkSRNloKYjzWTCBmokSZIkSdL4GajpiHPUSJIkSZKkiTJQ05HEtz5JkiRJkqSJMVDTkQyEqvUOf5IkSZIkSeNmoKYjSVu1BmokSZIkSdI4GajpSBIAe9RIkiRJkqRxM1DTkQ2BGicUliRJkiRJ42OgpiMDA3MAnFBYkiRJkiSNm4GajmSg6VGz3h41kiRJkiRNqaVLYf58GBhoPpcuHT5tOthsqgswUz0/9Gm9gRpJktQfSRYBZwFzgAuq6owpLtKoli6F006DlSth++2btDVrYM4cWLduw2fi+xkkSeMzMADr14/+XfLII3D00U3ac89tSDvuuGb5yCOnpuyD7FHTkcG3PjmZsCRJ6ockc4BzgUOBPYEPJ9mzn2UY68lk7/Ydd4RjjmkaxlVNgGbNmma/des2/rQ5JUkar8G+EmN9l/zmNxuCNIOefrp5gDDV7FHTEScTliRJfbYfsKKqHgRIcimwGLi7HydfuhRO+fhTLHjmduYCPAIXHQs73wuHHALXXAMXn1nMexbmAawZ/jhh9KjMpr59OpTBa/QaJ2P7dCiD1+g1jmf7RI+RR4AvDNlh0SLYa68xzzNZDNR0JAP2qJEkSX21K/Boz/oq4G1Dd0pyHHAcwNy5cyft5KedBq955kd8j3+1IfFZ4C+an0NofiRJmvZOGbK+444GamaC54c+OUeNJEnqjwyT9oInRlV1PnA+wMKFCyftidLKlfAke/Eulr+gUMuXw0EHNYWpYYs5pIxj7LOpb58OZfAavcbJ2D4dyuA1eo3j2T7cPptv1s5R85sN27feCs4+G444YkjmLbcc8/iTyUBNRwbf+mSgRpIk9ckqYPee9d2Ax/p18rlz4ZFHtuU63rVR+rx5wLvggXnNfDSSJE2VwQmF582D009v0gYntZ87t0k7YoonEgYnE+6MkwlLkqQ+uwlYkGSPJFsAS4Bl/Tr56afD1ltvnLb11hsawsNtm5ivzwAADtdJREFU33xz2GGHpuG8ww7NDzRv6Oj9zNgPSiVJApoJ6+GF3yXz5sEllzSBmocfbt7sdOSRzfL69RvSpgN71HTEyYQlSVI/VdXaJCcCV9G8nvvCqrqrX+cfbNwOfTI5mD7WdkmS1DBQ0xF71EiSpH6rqiuBK6fq/INPJ1/sdkmS5NCnzgwMOJmwJEmSJEmaGAM1XWknE17v0CdJkiRJkjROBmo6suH13A59kiRJkiRJ49NpoCbJoiT3JVmR5NRhtifJ2e32O5LsO1beJNsnuSbJ/e3ndj3bPt3uf1+S93R5bWNxMmFJkiRJkjRRnQVqkswBzgUOBfYEPpxkzyG7HQosaH+OA748jrynAsuragGwvF2n3b4E2AtYBHypPc6UcDJhSZIkSZI0UV32qNkPWFFVD1bVc8ClwOIh+ywGLq7G9cC2SXYZI+9i4KJ2+SLg8J70S6vq2ap6CFjRHmdKZMAeNZIkSZIkaWK6fD33rsCjPeurgLeNY59dx8j76qpaDVBVq5Ps3HOs64c5Vl9ddtOjfOHq+1i78icAnP8XfwoDQzv2hM+mdzUv2P4CwyRJktQv3//WFWy33XZj7yhJkqSXpMtAzXChhaHjgEbaZzx5X8z5SHIczTArgF8luW+M474YOwI/7+C4Gp313n/Wef9Z5/1nnQPbb799F4ed18VBNX4333zzz5M8MtXl6CP/nvvDeu4f67o/rOf+mI31PGxbqMtAzSpg95713YDHxrnPFqPk/WmSXdreNLsAj0/gfFTV+cD5E7uUiUnyw6pa2OU59ELWe/9Z5/1nnfefda6ZrKp2muoy9JN/z/1hPfePdd0f1nN/WM8bdDlHzU3AgiR7JNmCZqLfZUP2WQZ8tH370/7AU+2wptHyLgOOapePAq7oSV+S5GVJ9qCZoPjGri5OkiRJkiRpsnXWo6aq1iY5EbgKmANcWFV3JTm+3X4ecCXwXpqJf58Gjh4tb3voM4DLkhwLrAQ+1Oa5K8llwN3AWuCEqlrX1fVJkiRJkiRNti6HPlFVV9IEY3rTzutZLuCE8eZt09cAB42Q53Tg9JdQ5MnS6dAqjch67z/rvP+s8/6zzqWZw7/n/rCe+8e67g/ruT+s51aaWIkkSZIkSZKmWpdz1EiSJEmSJGkCDNRMsiSLktyXZEWSU6e6PJuSJLsnuS7JPUnuSnJym759kmuS3N9+bteT59NtXd+X5D096W9Jcme77ewkadNfluTv2vQbkszv93VOR0nmJLk1yT+269Z5x5Jsm+TyJPe2/+bfbr13K8kftf+3/CjJ15JsaZ1Ls1OSU5JUkh2nuiwzVZIz2++4O5L8fZJtp7pMM4n3HP0x0v2JujH0nmQ2M1AziZLMAc4FDgX2BD6cZM+pLdUmZS3wx1X1O8D+wAlt/Z0KLK+qBcDydp122xJgL2AR8KX2dwDwZeA4mrd/LWi3AxwL/N+q+pfAF4HP9ePCNgEnA/f0rFvn3TsL+FZV/TawN039W+8dSbIr8AlgYVW9gWai+iVY59Ksk2R34BCal1KoO9cAb6iqNwH/DHx6isszY3jP0Vcj3Z+oG0PvSWYtAzWTaz9gRVU9WFXPAZcCi6e4TJuMqlpdVbe0y7+k+SPdlaYOL2p3uwg4vF1eDFxaVc9W1UM0bw/bL8kuwKuq6v+0E1ZfPCTP4LEuBw4afBo+WyXZDfh94IKeZOu8Q0leBRwAfAWgqp6rqiex3ru2GbBVks2ArYHHsM6l2eiLwJ8CTtTYoaq6uqrWtqvXA7tNZXlmGO85+mSU+xNNshHuSWYtAzWTa1fg0Z71VfiH/KK0QwbeDNwAvLqqVkPznyWwc7vbSPW9a7s8NH2jPG3j4Slghy6uYRPy1zQN1vU9adZ5t14D/Az4b233zguSvBzrvTNV9WPgP9M8QV8NPFVVV2OdS7NKksOAH1fV7VNdllnmGOB/TXUhZhDvOabAkPsTTb7h7klmrU5fzz0LDffk1Kc1E5TkFcA3gE9W1S9GeSA9Un2P9nvwd9QjyfuAx6vq5iQHjifLMGnW+cRtBuwLnFRVNyQ5i3bIzQis95eonXtmMbAH8CTw9ST/drQsw6RZ59ImIMm3gX8xzKbTgM8A7+5viWau0eq6qq5o9zmNZvjI0n6WbYbz+6bPht6fTHV5ZpoXcU8y4xmomVyrgN171nej6VqvcUqyOc1/gkur6ptt8k+T7FJVq9thB4+36SPV9yo27l7b+3sYzLOqHf6wDfBEJxezaXgncFiS9wJbAq9K8t+xzru2ClhVVYNPZC6nCdRY7905GHioqn4GkOSbwDuwzqUZp6oOHi49yRtpgrW3tw+BdgNuSbJfVf2kj0WcMUaq60FJjgLeBxzUDhfV5PCeo49GuD/R5Br2nqSqRnuoNqM59Gly3QQsSLJHki1oJqJcNsVl2mS0czl8Bbinqv6qZ9My4Kh2+Sjgip70Je2bVvagmdTzxnb4wi+T7N8e86ND8gwe64PAtbO54VBVn66q3apqPs2/12vb/xCt8w61NwSPJnl9m3QQcDfWe5dWAvsn2bqtq4Noxplb59IsUVV3VtXOVTW//d5bBexrkKYbSRYBnwIOq6qnp7o8M4z3HH0yyv2JJtEo9ySzlj1qJlFVrU1yInAVzRtFLqyqu6a4WJuSdwIfAe5Mclub9hngDOCyJMfS3Gx9CKCq7kpyGc0N7lrghKpa1+b7Q+CrwFY0Y6IHx0V/BbgkyQqaJ91Lur6oTZR13r2TgKVtA+tB4Gia4Ln13oF2iNnlwC00dXgrcD7wCqxzSerCOcDLgGvaHkzXV9XxU1ukmcF7jr4a9v6kqq6cwjJpFogP+yRJkiRJkqYHhz5JkiRJkiRNEwZqJEmSJEmSpgkDNZIkSZIkSdOEgRpJkiRJkqRpwkCNJEmSJEnSNGGgRpIkSdKMl2SHJLe1Pz9J8uN2+ckkd/e5LIcn2bNn/c+THPwijjM/yY8mt3QTOv9nhqz/oP2c0nJJmzoDNZJesiSfSHJPkqVTXZYuJflkkq2nuhySJGniqmpNVe1TVfsA5wFfbJf3AdZP9vmSbDbK5sOB5wM1VfVnVfXtyS5DH2wUqKmqd0xVQaSZxECNpMnw74H3VtWRY+04RqNluvskYKBGkqSZZ06Sv0lyV5Krk2wFkOS1Sb6V5OYk30/y2236vCTLk9zRfs5t07+a5K+SXAd8brj8Sd4BHAac2fboeW2b74PtMd6a5AdJbk9yY5JXtj1Uvp/klvZn1IBIGuckuTvJ/0xyZc/xH06yY7u8MMl32uX92vPe2n6+vk3/WJJvttdxf5LPt+lnAFu117C0TfvVMGWZk+TMJDe19fXv2vRdknyvzf+jJL/7En+H0oxhoEbSS5LkPOA1wLIknxrlC/7rSf4HcHWSlye5sP3CvjXJ4lGOP2zDJMmBSb6b5LIk/5zkjCRHtg2aO5O8tt1vtIbUB3vO86ue434nyeVJ7k2ytG3sfAL4LeC6tvElSZJmjgXAuVW1F/Ak8IE2/XzgpKp6C3AK8KU2/Rzg4qp6E7AUOLvnWK8DDq6qPx4uf1X9AFgG/Enbw+eBwYxJtgD+Dji5qvYGDgaeAR4HDqmqfYEjhpxvOO8HXg+8Efg4MJ6eLvcCB1TVm4E/Az7bs22f9rxvBI5IsntVnQo8017DaA/rjgWeqqq3Am8FPp5kD+APgKvaXk17A7eNo4zSrLApP9mWNA1U1fFJFgG/BzwHfKGq1qYZZ/1ZNjR03g68qaqeSPJZ4NqqOibJtsCNSb5dVf9vmFMMNkx+nWQB8DVgYbttb+B3gCeAB4ELqmq/JCcDJ9H0gBlsSF2U5Biahs3hY1zWm4G9gMeAfwLeWVVnJ/kPwO9V1c8nWk+SJGlae6iqBgMFNwPzk7yCJsDx9SSD+72s/Xw78G/a5UuAz/cc6+tVtW6M/CN5PbC6qm4CqKpfACR5OXBOkn2AdTTBoNEcAHytqtYBjyW5doz9AbYBLmrbWwVs3rNteVU91ZblbmAe8Og4jgnwbuBNPQ/ItqEJjN0EXJhkc+AfeupfmvUM1EiaTKN9wV9TVU+0y+8GDktySru+JTAXuGeYY27OyA2Tm6pqNUCSB4Cr2/Q7aQJHMHpDaiQ3VtWq9ri3AfOB/z2OfJIkadP0bM/yOmArmtEHT7Y9PsZSPcuDD54mkn9Qhhxr0B8BP6V5SDUA/HqCZeq1lg0jK7bsSf9PwHVV9f4k84Hv9GwbWj8TuY8MTa+iq16wITkA+H3gkiRnVtXFEziuNGM59EnSZBr8gn8D8K/Z+Mu/t7dMgA8MTuhXVXOrarggDWzcMFkIbNGzrbfRsL5nfT0jNyAGGy3PN1LSPOYa6bgTbYxIkqQZoO3N8lCSD8Hz877s3W7+AbCkXT6SYR7ojJH/l8ArhzntvcBvJXlrm+eVaeb324amp8164CPAnDGK/z1gSTs/zC5seIAF8DDwlnb5Az3p2wA/bpc/NsbxB/2m7REzmquAPxzcL8nr2mHw84DHq+pvgK8A+47znNKMZ6BG0mQa7xf8VcBJbYCEJG8e45gTaZgMNVJD6mE2NFIWs3Hvn5GM1KiSJEkz05HAsUluB+6iaTMAfAI4OskdNO2TkyeY/1LgT9q5+l47uHNVPUczF8x/afNcQ/Pg60vAUUmup+ldPNxw8V5/D9xP08v4y8B3e7b9R+CsJN+neSA16PPAXyb5J8bf3jofuCOjv/nzAuBu4JY0r+z+rzQPwQ4EbktyK03A6KxxnlOa8VI1Uo84SRqfJA/T9HZZAFwE/Ay4FvhIVc1P8jFgYVWd2O6/FfDXNOO2AzxcVe8b4dgLgG8ATwPX0XSdfUWSA4FTBvOleWPBKVX1w95tbdfdC4Ed23IdXVUrk7wauIImYL18lOOeA/ywqr6a5CTgBJrAUe+TKUmSpGkryVeBf6yqy6e6LJLGZqBGkiRJkmYwAzXSpsVAjSRJkiRJ0jThBJmSpoUk7wE+NyT5oap6/1SUR5IkSZKmgj1qJEmSJEmSpgnf+iRJkiRJkjRNGKiRJEmSJEmaJgzUSJIkSZIkTRMGaiRJkiRJkqYJAzWSJEmSJEnTxP8Hziyce5Ba0zYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1368x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(mu, sigma) = norm.fit(df['fare_amount'])\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(19, 5))\n",
    "ax1 = sns.distplot(df['fare_amount'] , fit=norm, ax=ax1)\n",
    "ax1.legend([f'Normal distribution ($\\mu=$ {mu:.3f} and $\\sigma=$ {sigma:.3f})'], loc='best')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Fare Distribution')\n",
    "ax2 = stats.probplot(df['fare_amount'], plot=plt)\n",
    "f.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>distance</th>\n",
       "      <th>distance_travelled</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>befor_shock</th>\n",
       "      <th>is_a_holiday</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-12 10:19:05.0000001</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2015-04-12 10:19:05+00:00</td>\n",
       "      <td>-73.979279</td>\n",
       "      <td>40.723438</td>\n",
       "      <td>-74.004608</td>\n",
       "      <td>40.746948</td>\n",
       "      <td>0</td>\n",
       "      <td>((40.72343826293945, -73.97927856445312), (40....</td>\n",
       "      <td>3.374718</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-05 17:39:00.00000040</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2011-01-05 17:39:00+00:00</td>\n",
       "      <td>-73.966957</td>\n",
       "      <td>40.761268</td>\n",
       "      <td>-73.967912</td>\n",
       "      <td>40.765535</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.761268, -73.966957), (40.765535, -73.9679...</td>\n",
       "      <td>0.481238</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-17 04:22:00.0000006</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2013-09-17 04:22:00+00:00</td>\n",
       "      <td>-73.987210</td>\n",
       "      <td>40.729325</td>\n",
       "      <td>-73.931985</td>\n",
       "      <td>40.697207</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.729325, -73.98721), (40.697207, -73.931985))</td>\n",
       "      <td>5.866839</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-09 22:21:25.0000001</td>\n",
       "      <td>9.7</td>\n",
       "      <td>2011-03-09 22:21:25+00:00</td>\n",
       "      <td>-73.977829</td>\n",
       "      <td>40.788979</td>\n",
       "      <td>-73.967935</td>\n",
       "      <td>40.760508</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.788979, -73.977829), (40.760508, -73.9679...</td>\n",
       "      <td>3.273626</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-10-16 10:32:00.000000191</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2009-10-16 10:32:00+00:00</td>\n",
       "      <td>-73.990575</td>\n",
       "      <td>40.746117</td>\n",
       "      <td>-74.003227</td>\n",
       "      <td>40.751447</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.746117, -73.990575), (40.751447, -74.0032...</td>\n",
       "      <td>1.219496</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108480</th>\n",
       "      <td>2009-04-30 21:37:00.00000040</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2009-04-30 21:37:00+00:00</td>\n",
       "      <td>-73.997763</td>\n",
       "      <td>40.741155</td>\n",
       "      <td>-73.984755</td>\n",
       "      <td>40.717655</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.741155, -73.997763), (40.717655, -73.9847...</td>\n",
       "      <td>2.833663</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108481</th>\n",
       "      <td>2012-08-03 23:31:00.000000184</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2012-08-03 23:31:00+00:00</td>\n",
       "      <td>-73.967117</td>\n",
       "      <td>40.756930</td>\n",
       "      <td>-73.973147</td>\n",
       "      <td>40.755955</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.75693, -73.967117), (40.755955, -73.973147))</td>\n",
       "      <td>0.519345</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108482</th>\n",
       "      <td>2011-02-20 14:03:00.00000051</td>\n",
       "      <td>15.3</td>\n",
       "      <td>2011-02-20 14:03:00+00:00</td>\n",
       "      <td>-73.991902</td>\n",
       "      <td>40.738360</td>\n",
       "      <td>-73.967693</td>\n",
       "      <td>40.792793</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.73836, -73.991902), (40.792792999999996, ...</td>\n",
       "      <td>6.386844</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108483</th>\n",
       "      <td>2011-08-06 18:36:47.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2011-08-06 18:36:47+00:00</td>\n",
       "      <td>-73.992143</td>\n",
       "      <td>40.770590</td>\n",
       "      <td>-73.988171</td>\n",
       "      <td>40.758195</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.770590000000006, -73.992143), (40.758195,...</td>\n",
       "      <td>1.418278</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108484</th>\n",
       "      <td>2011-04-02 22:04:24.0000004</td>\n",
       "      <td>14.1</td>\n",
       "      <td>2011-04-02 22:04:24+00:00</td>\n",
       "      <td>-73.970505</td>\n",
       "      <td>40.752325</td>\n",
       "      <td>-73.960537</td>\n",
       "      <td>40.797342</td>\n",
       "      <td>1</td>\n",
       "      <td>((40.752325, -73.970505), (40.797342, -73.9605...</td>\n",
       "      <td>5.075555</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1108485 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  fare_amount           pickup_datetime  \\\n",
       "0          2015-04-12 10:19:05.0000001         11.5 2015-04-12 10:19:05+00:00   \n",
       "1         2011-01-05 17:39:00.00000040          3.7 2011-01-05 17:39:00+00:00   \n",
       "2          2013-09-17 04:22:00.0000006         19.0 2013-09-17 04:22:00+00:00   \n",
       "3          2011-03-09 22:21:25.0000001          9.7 2011-03-09 22:21:25+00:00   \n",
       "4        2009-10-16 10:32:00.000000191          7.3 2009-10-16 10:32:00+00:00   \n",
       "...                                ...          ...                       ...   \n",
       "1108480   2009-04-30 21:37:00.00000040         10.5 2009-04-30 21:37:00+00:00   \n",
       "1108481  2012-08-03 23:31:00.000000184          3.3 2012-08-03 23:31:00+00:00   \n",
       "1108482   2011-02-20 14:03:00.00000051         15.3 2011-02-20 14:03:00+00:00   \n",
       "1108483    2011-08-06 18:36:47.0000001          7.7 2011-08-06 18:36:47+00:00   \n",
       "1108484    2011-04-02 22:04:24.0000004         14.1 2011-04-02 22:04:24+00:00   \n",
       "\n",
       "         pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0              -73.979279        40.723438         -74.004608   \n",
       "1              -73.966957        40.761268         -73.967912   \n",
       "2              -73.987210        40.729325         -73.931985   \n",
       "3              -73.977829        40.788979         -73.967935   \n",
       "4              -73.990575        40.746117         -74.003227   \n",
       "...                   ...              ...                ...   \n",
       "1108480        -73.997763        40.741155         -73.984755   \n",
       "1108481        -73.967117        40.756930         -73.973147   \n",
       "1108482        -73.991902        40.738360         -73.967693   \n",
       "1108483        -73.992143        40.770590         -73.988171   \n",
       "1108484        -73.970505        40.752325         -73.960537   \n",
       "\n",
       "         dropoff_latitude  passenger_count  \\\n",
       "0               40.746948                0   \n",
       "1               40.765535                1   \n",
       "2               40.697207                1   \n",
       "3               40.760508                1   \n",
       "4               40.751447                1   \n",
       "...                   ...              ...   \n",
       "1108480         40.717655                1   \n",
       "1108481         40.755955                1   \n",
       "1108482         40.792793                1   \n",
       "1108483         40.758195                1   \n",
       "1108484         40.797342                1   \n",
       "\n",
       "                                                  distance  \\\n",
       "0        ((40.72343826293945, -73.97927856445312), (40....   \n",
       "1        ((40.761268, -73.966957), (40.765535, -73.9679...   \n",
       "2        ((40.729325, -73.98721), (40.697207, -73.931985))   \n",
       "3        ((40.788979, -73.977829), (40.760508, -73.9679...   \n",
       "4        ((40.746117, -73.990575), (40.751447, -74.0032...   \n",
       "...                                                    ...   \n",
       "1108480  ((40.741155, -73.997763), (40.717655, -73.9847...   \n",
       "1108481  ((40.75693, -73.967117), (40.755955, -73.973147))   \n",
       "1108482  ((40.73836, -73.991902), (40.792792999999996, ...   \n",
       "1108483  ((40.770590000000006, -73.992143), (40.758195,...   \n",
       "1108484  ((40.752325, -73.970505), (40.797342, -73.9605...   \n",
       "\n",
       "         distance_travelled  year  month  day  befor_shock  is_a_holiday  time  \n",
       "0                  3.374718  2015      4    6            0             0   619  \n",
       "1                  0.481238  2011      1    2            1             0  1059  \n",
       "2                  5.866839  2013      9    1            0             0   262  \n",
       "3                  3.273626  2011      3    2            1             0  1341  \n",
       "4                  1.219496  2009     10    4            1             0   632  \n",
       "...                     ...   ...    ...  ...          ...           ...   ...  \n",
       "1108480            2.833663  2009      4    3            1             0  1297  \n",
       "1108481            0.519345  2012      8    4            1             0  1411  \n",
       "1108482            6.386844  2011      2    6            1             0   843  \n",
       "1108483            1.418278  2011      8    5            1             0  1116  \n",
       "1108484            5.075555  2011      4    5            1             0  1324  \n",
       "\n",
       "[1108485 rows x 16 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dummy=pd.get_dummies(df['day'])\n",
    "\n",
    "#df=pd.concat([df,train_dummy],axis=1)\n",
    "#holiday_dummy = pd.get_dummies(df['is_a_holiday'])\n",
    "#passenger_dummy = pd.get_dummies(df['passeger_count'])\n",
    "#df=pd.concat([df,day_dummy,holiday_dummy, passenger_dummy],axis=1)\n",
    "df=pd.concat([df,day_dummy],axis=1)\n",
    "\n",
    "day1_dummy=pd.get_dummies(df1['day'])\n",
    "#df=pd.concat([df,train_dummy],axis=1)\n",
    "#holiday_dummy = pd.get_dummies(df['is_a_holiday'])\n",
    "#passenger_dummy = pd.get_dummies(df['passeger_count'])\n",
    "#df=pd.concat([df,day_dummy,holiday_dummy, passenger_dummy],axis=1)\n",
    "df1=pd.concat([df1,day1_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 16 columns before encoding categorical features\n",
      "There are 42 columns after encoding categorical features\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "        #drop the encoded column\n",
    "        df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "    \n",
    "cat_cols = ['is_a_holiday','befor_shock','day','month','year','passenger_count']\n",
    "print('There were {} columns before encoding categorical features'.format(df.shape[1]))\n",
    "df = oneHotEncode(df, cat_cols)\n",
    "df1 = oneHotEncode(df1, cat_cols)\n",
    "print('There are {} columns after encoding categorical features'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns = ['key','fare_amount','distance','pickup_datetime'])\n",
    "y_train = df['fare_amount']\n",
    "X_test = df1.drop(columns = ['key','distance','pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          11.5\n",
       "1           3.7\n",
       "2          19.0\n",
       "3           9.7\n",
       "4           7.3\n",
       "           ... \n",
       "1108480    10.5\n",
       "1108481     3.3\n",
       "1108482    15.3\n",
       "1108483     7.7\n",
       "1108484    14.1\n",
       "Name: fare_amount, Length: 1108472, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = df1.iloc[:,-13:]\n",
    "dftest.drop(columns = ['day','dropoff_location','pickup_location'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(dftrain,df['fare_amount'])\n",
    "y_pred = regr.predict(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key  fare_amount\n",
       "0  2015-01-27 13:08:24.0000002        11.35\n",
       "1  2015-01-27 13:08:24.0000003        11.35\n",
       "2  2011-10-08 11:53:44.0000002        11.35\n",
       "3  2012-12-01 21:12:12.0000002        11.35\n",
       "4  2012-12-01 21:12:12.0000003        11.35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('sample_submission.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3711217078712763"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission=pd.DataFrame(data=y_pred,columns=['fare_amount'])\n",
    "Submission['key']=df1['key']\n",
    "Submission=Submission[['key','fare_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = Submission.set_index('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.to_csv('y1_knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-27 13:08:24.0000002</th>\n",
       "      <td>11.292572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-27 13:08:24.0000003</th>\n",
       "      <td>10.649523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-08 11:53:44.0000002</th>\n",
       "      <td>4.736473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-01 21:12:12.0000002</th>\n",
       "      <td>10.116229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-01 21:12:12.0000003</th>\n",
       "      <td>15.810888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             fare_amount\n",
       "key                                     \n",
       "2015-01-27 13:08:24.0000002    11.292572\n",
       "2015-01-27 13:08:24.0000003    10.649523\n",
       "2011-10-08 11:53:44.0000002     4.736473\n",
       "2012-12-01 21:12:12.0000002    10.116229\n",
       "2012-12-01 21:12:12.0000003    15.810888"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(max_depth=30, n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               4992      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 169,857\n",
      "Trainable params: 169,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0851 - mean_absolute_error: 2.0851\n",
      "Epoch 00001: val_loss improved from inf to 2.10555, saving model to Weights-001--2.10555.hdf5\n",
      "27712/27712 [==============================] - 36s 1ms/step - loss: 2.0851 - mean_absolute_error: 2.0851 - val_loss: 2.1055 - val_mean_absolute_error: 2.1055\n",
      "Epoch 2/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0852 - mean_absolute_error: 2.0852\n",
      "Epoch 00002: val_loss did not improve from 2.10555\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0852 - mean_absolute_error: 2.0852 - val_loss: 2.1553 - val_mean_absolute_error: 2.1553\n",
      "Epoch 3/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0866 - mean_absolute_error: 2.0866\n",
      "Epoch 00003: val_loss improved from 2.10555 to 2.09226, saving model to Weights-003--2.09226.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0866 - mean_absolute_error: 2.0866 - val_loss: 2.0923 - val_mean_absolute_error: 2.0923\n",
      "Epoch 4/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0896 - mean_absolute_error: 2.0896\n",
      "Epoch 00004: val_loss did not improve from 2.09226\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0898 - mean_absolute_error: 2.0898 - val_loss: 2.0979 - val_mean_absolute_error: 2.0979\n",
      "Epoch 5/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0881 - mean_absolute_error: 2.0881\n",
      "Epoch 00005: val_loss improved from 2.09226 to 2.08297, saving model to Weights-005--2.08297.hdf5\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0882 - mean_absolute_error: 2.0882 - val_loss: 2.0830 - val_mean_absolute_error: 2.0830\n",
      "Epoch 6/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0889 - mean_absolute_error: 2.0889\n",
      "Epoch 00006: val_loss did not improve from 2.08297\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0891 - mean_absolute_error: 2.0891 - val_loss: 2.0989 - val_mean_absolute_error: 2.0989\n",
      "Epoch 7/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0882 - mean_absolute_error: 2.0882\n",
      "Epoch 00007: val_loss improved from 2.08297 to 2.07841, saving model to Weights-007--2.07841.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0883 - mean_absolute_error: 2.0883 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n",
      "Epoch 8/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0866 - mean_absolute_error: 2.0866\n",
      "Epoch 00008: val_loss did not improve from 2.07841\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0867 - mean_absolute_error: 2.0867 - val_loss: 2.1217 - val_mean_absolute_error: 2.1217\n",
      "Epoch 9/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0819 - mean_absolute_error: 2.0819\n",
      "Epoch 00009: val_loss did not improve from 2.07841\n",
      "27712/27712 [==============================] - 42s 1ms/step - loss: 2.0817 - mean_absolute_error: 2.0817 - val_loss: 2.1020 - val_mean_absolute_error: 2.1020\n",
      "Epoch 10/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00010: val_loss improved from 2.07841 to 2.07691, saving model to Weights-010--2.07691.hdf5\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 11/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00011: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0830 - mean_absolute_error: 2.0830 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 12/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0825 - mean_absolute_error: 2.0825\n",
      "Epoch 00012: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0824 - mean_absolute_error: 2.0824 - val_loss: 2.1270 - val_mean_absolute_error: 2.1270\n",
      "Epoch 13/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00013: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.1002 - val_mean_absolute_error: 2.1002\n",
      "Epoch 14/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0845 - mean_absolute_error: 2.0845\n",
      "Epoch 00014: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0845 - mean_absolute_error: 2.0845 - val_loss: 2.1035 - val_mean_absolute_error: 2.1035\n",
      "Epoch 15/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0853 - mean_absolute_error: 2.0853\n",
      "Epoch 00015: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0855 - mean_absolute_error: 2.0855 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 16/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00016: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0847 - mean_absolute_error: 2.0847 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 17/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0853 - mean_absolute_error: 2.0853\n",
      "Epoch 00017: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0855 - mean_absolute_error: 2.0855 - val_loss: 2.1032 - val_mean_absolute_error: 2.1032\n",
      "Epoch 18/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0871 - mean_absolute_error: 2.0871\n",
      "Epoch 00018: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0871 - mean_absolute_error: 2.0871 - val_loss: 2.1052 - val_mean_absolute_error: 2.1052\n",
      "Epoch 19/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00019: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0864 - val_mean_absolute_error: 2.0864\n",
      "Epoch 20/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0859 - mean_absolute_error: 2.0859\n",
      "Epoch 00020: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0858 - mean_absolute_error: 2.0858 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 21/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0860 - mean_absolute_error: 2.0860\n",
      "Epoch 00021: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0860 - mean_absolute_error: 2.0860 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 22/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0872 - mean_absolute_error: 2.0872\n",
      "Epoch 00022: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0871 - mean_absolute_error: 2.0871 - val_loss: 2.1062 - val_mean_absolute_error: 2.1062\n",
      "Epoch 23/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00023: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0848 - mean_absolute_error: 2.0848 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 24/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0863 - mean_absolute_error: 2.0863\n",
      "Epoch 00024: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0863 - mean_absolute_error: 2.0863 - val_loss: 2.0924 - val_mean_absolute_error: 2.0924\n",
      "Epoch 25/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00025: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0896 - val_mean_absolute_error: 2.0896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00026: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.1512 - val_mean_absolute_error: 2.1512\n",
      "Epoch 27/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0860 - mean_absolute_error: 2.0860\n",
      "Epoch 00027: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0860 - mean_absolute_error: 2.0860 - val_loss: 2.1021 - val_mean_absolute_error: 2.1021\n",
      "Epoch 28/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0852 - mean_absolute_error: 2.0852\n",
      "Epoch 00028: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0852 - mean_absolute_error: 2.0852 - val_loss: 2.1117 - val_mean_absolute_error: 2.1117\n",
      "Epoch 29/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0844 - mean_absolute_error: 2.0844\n",
      "Epoch 00029: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0844 - mean_absolute_error: 2.0844 - val_loss: 2.1010 - val_mean_absolute_error: 2.1010\n",
      "Epoch 30/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0859 - mean_absolute_error: 2.0859\n",
      "Epoch 00030: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0859 - mean_absolute_error: 2.0859 - val_loss: 2.0815 - val_mean_absolute_error: 2.0815\n",
      "Epoch 31/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0871 - mean_absolute_error: 2.0871\n",
      "Epoch 00031: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0873 - mean_absolute_error: 2.0873 - val_loss: 2.1015 - val_mean_absolute_error: 2.1015\n",
      "Epoch 32/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0841 - mean_absolute_error: 2.0841\n",
      "Epoch 00032: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0841 - mean_absolute_error: 2.0841 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 33/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00033: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 34/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00034: val_loss did not improve from 2.07691\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0774 - val_mean_absolute_error: 2.0774\n",
      "Epoch 35/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00035: val_loss improved from 2.07691 to 2.07578, saving model to Weights-035--2.07578.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0758 - val_mean_absolute_error: 2.0758\n",
      "Epoch 36/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0851 - mean_absolute_error: 2.0851\n",
      "Epoch 00036: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0849 - mean_absolute_error: 2.0849 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 37/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0833 - mean_absolute_error: 2.0833\n",
      "Epoch 00037: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0831 - mean_absolute_error: 2.0831 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 38/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0843 - mean_absolute_error: 2.0843\n",
      "Epoch 00038: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0844 - mean_absolute_error: 2.0844 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 39/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0848 - mean_absolute_error: 2.0848\n",
      "Epoch 00039: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0848 - mean_absolute_error: 2.0848 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 40/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0833 - mean_absolute_error: 2.0833\n",
      "Epoch 00040: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0832 - mean_absolute_error: 2.0832 - val_loss: 2.1318 - val_mean_absolute_error: 2.1318\n",
      "Epoch 41/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00041: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 42/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0836 - mean_absolute_error: 2.0836\n",
      "Epoch 00042: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0837 - mean_absolute_error: 2.0837 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 43/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0855 - mean_absolute_error: 2.0855\n",
      "Epoch 00043: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0854 - mean_absolute_error: 2.0854 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 44/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00044: val_loss did not improve from 2.07578\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0843 - val_mean_absolute_error: 2.0843\n",
      "Epoch 45/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00045: val_loss improved from 2.07578 to 2.07546, saving model to Weights-045--2.07546.hdf5\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0829 - mean_absolute_error: 2.0829 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 46/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00046: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0838 - mean_absolute_error: 2.0838 - val_loss: 2.1350 - val_mean_absolute_error: 2.1350\n",
      "Epoch 47/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0835 - mean_absolute_error: 2.0835\n",
      "Epoch 00047: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0834 - mean_absolute_error: 2.0834 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 48/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0827 - mean_absolute_error: 2.0827\n",
      "Epoch 00048: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.1220 - val_mean_absolute_error: 2.1220\n",
      "Epoch 49/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00049: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.1425 - val_mean_absolute_error: 2.1425\n",
      "Epoch 50/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0836 - mean_absolute_error: 2.0836\n",
      "Epoch 00050: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.1112 - val_mean_absolute_error: 2.1112\n",
      "Epoch 51/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0821 - mean_absolute_error: 2.0821\n",
      "Epoch 00051: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0823 - mean_absolute_error: 2.0823 - val_loss: 2.0992 - val_mean_absolute_error: 2.0992\n",
      "Epoch 52/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0837 - mean_absolute_error: 2.0837\n",
      "Epoch 00052: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0817 - val_mean_absolute_error: 2.0817\n",
      "Epoch 53/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0827 - mean_absolute_error: 2.0827\n",
      "Epoch 00053: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0859 - val_mean_absolute_error: 2.0859\n",
      "Epoch 54/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00054: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0827 - val_mean_absolute_error: 2.0827\n",
      "Epoch 55/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00055: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0950 - val_mean_absolute_error: 2.0950\n",
      "Epoch 56/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00056: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0839 - val_mean_absolute_error: 2.0839\n",
      "Epoch 57/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00057: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0934 - val_mean_absolute_error: 2.0934\n",
      "Epoch 58/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00058: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0815 - val_mean_absolute_error: 2.0815\n",
      "Epoch 59/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00059: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 60/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00060: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.0931 - val_mean_absolute_error: 2.0931\n",
      "Epoch 61/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00061: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 62/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00062: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 63/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00063: val_loss did not improve from 2.07546\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 64/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00064: val_loss improved from 2.07546 to 2.07296, saving model to Weights-064--2.07296.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0730 - val_mean_absolute_error: 2.0730\n",
      "Epoch 65/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00065: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0977 - val_mean_absolute_error: 2.0977\n",
      "Epoch 66/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00066: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 67/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00067: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 68/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00068: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0800 - mean_absolute_error: 2.0800 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 69/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00069: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0731 - val_mean_absolute_error: 2.0731\n",
      "Epoch 70/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00070: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0790 - mean_absolute_error: 2.0790 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 71/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00071: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0736 - val_mean_absolute_error: 2.0736\n",
      "Epoch 72/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00072: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.1021 - val_mean_absolute_error: 2.1021\n",
      "Epoch 73/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00073: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 74/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00074: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0794 - val_mean_absolute_error: 2.0794\n",
      "Epoch 75/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00075: val_loss did not improve from 2.07296\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0799 - mean_absolute_error: 2.0799 - val_loss: 2.0852 - val_mean_absolute_error: 2.0852\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00076: val_loss improved from 2.07296 to 2.06861, saving model to Weights-076--2.06861.hdf5\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0686 - val_mean_absolute_error: 2.0686\n",
      "Epoch 77/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00077: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0704 - val_mean_absolute_error: 2.0704\n",
      "Epoch 78/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00078: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 79/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0814 - mean_absolute_error: 2.0814\n",
      "Epoch 00079: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0811 - mean_absolute_error: 2.0811 - val_loss: 2.1209 - val_mean_absolute_error: 2.1209\n",
      "Epoch 80/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0801 - mean_absolute_error: 2.0801\n",
      "Epoch 00080: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0754 - val_mean_absolute_error: 2.0754\n",
      "Epoch 81/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00081: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 82/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00082: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 83/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00083: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 84/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00084: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 85/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00085: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1072 - val_mean_absolute_error: 2.1072\n",
      "Epoch 86/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00086: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0958 - val_mean_absolute_error: 2.0958\n",
      "Epoch 87/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.0799\n",
      "Epoch 00087: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0799 - mean_absolute_error: 2.0799 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 88/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00088: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0933 - val_mean_absolute_error: 2.0933\n",
      "Epoch 89/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00089: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 90/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00090: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0884 - val_mean_absolute_error: 2.0884\n",
      "Epoch 91/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00091: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 92/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00092: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0860 - val_mean_absolute_error: 2.0860\n",
      "Epoch 93/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0797 - mean_absolute_error: 2.0797\n",
      "Epoch 00093: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 94/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0807 - mean_absolute_error: 2.0807\n",
      "Epoch 00094: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0808 - mean_absolute_error: 2.0808 - val_loss: 2.0712 - val_mean_absolute_error: 2.0712\n",
      "Epoch 95/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00095: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0809 - mean_absolute_error: 2.0809 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 96/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00096: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 97/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00097: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0743 - val_mean_absolute_error: 2.0743\n",
      "Epoch 98/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00098: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0890 - val_mean_absolute_error: 2.0890\n",
      "Epoch 99/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00099: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.1049 - val_mean_absolute_error: 2.1049\n",
      "Epoch 100/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00100: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0875 - val_mean_absolute_error: 2.0875\n",
      "Epoch 101/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00101: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0721 - val_mean_absolute_error: 2.0721\n",
      "Epoch 102/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00102: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 103/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00103: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0757 - val_mean_absolute_error: 2.0757\n",
      "Epoch 104/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00104: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 105/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00105: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 106/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00106: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0902 - val_mean_absolute_error: 2.0902\n",
      "Epoch 107/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00107: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0828 - val_mean_absolute_error: 2.0828\n",
      "Epoch 108/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00108: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0800 - val_mean_absolute_error: 2.0800\n",
      "Epoch 109/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00109: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 110/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0802 - mean_absolute_error: 2.0802\n",
      "Epoch 00110: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0800 - mean_absolute_error: 2.0800 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 111/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00111: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0701 - val_mean_absolute_error: 2.0701\n",
      "Epoch 112/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00112: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 113/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00113: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0761 - val_mean_absolute_error: 2.0761\n",
      "Epoch 114/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00114: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0840 - val_mean_absolute_error: 2.0840\n",
      "Epoch 115/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00115: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 116/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00116: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1087 - val_mean_absolute_error: 2.1087\n",
      "Epoch 117/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00117: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0848 - val_mean_absolute_error: 2.0848\n",
      "Epoch 118/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00118: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0876 - val_mean_absolute_error: 2.0876\n",
      "Epoch 119/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00119: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 120/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00120: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.0877 - val_mean_absolute_error: 2.0877\n",
      "Epoch 121/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00121: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.1455 - val_mean_absolute_error: 2.1455\n",
      "Epoch 122/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00122: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 123/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00123: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0975 - val_mean_absolute_error: 2.0975\n",
      "Epoch 124/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00124: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 125/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00125: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 126/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00126: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0762 - val_mean_absolute_error: 2.0762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00127: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 128/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00128: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0880 - val_mean_absolute_error: 2.0880\n",
      "Epoch 129/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00129: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0996 - val_mean_absolute_error: 2.0996\n",
      "Epoch 130/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00130: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 131/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00131: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 132/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00132: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 133/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00133: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 134/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00134: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 135/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00135: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 136/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00136: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0803 - val_mean_absolute_error: 2.0803\n",
      "Epoch 137/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00137: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 138/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00138: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 139/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00139: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0880 - val_mean_absolute_error: 2.0880\n",
      "Epoch 140/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0793 - mean_absolute_error: 2.0793\n",
      "Epoch 00140: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0792 - mean_absolute_error: 2.0792 - val_loss: 2.0943 - val_mean_absolute_error: 2.0943\n",
      "Epoch 141/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00141: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 142/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00142: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0805 - val_mean_absolute_error: 2.0805\n",
      "Epoch 143/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00143: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 144/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00144: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.1101 - val_mean_absolute_error: 2.1101\n",
      "Epoch 145/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0798 - mean_absolute_error: 2.0798\n",
      "Epoch 00145: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 146/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00146: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 147/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00147: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0964 - val_mean_absolute_error: 2.0964\n",
      "Epoch 148/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0789 - mean_absolute_error: 2.0789\n",
      "Epoch 00148: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0788 - mean_absolute_error: 2.0788 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 149/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00149: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0835 - val_mean_absolute_error: 2.0835\n",
      "Epoch 150/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00150: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0788 - val_mean_absolute_error: 2.0788\n",
      "Epoch 151/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0787 - mean_absolute_error: 2.0787\n",
      "Epoch 00151: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0823 - val_mean_absolute_error: 2.0823\n",
      "Epoch 152/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00152: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0775 - val_mean_absolute_error: 2.0775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00153: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0817 - val_mean_absolute_error: 2.0817\n",
      "Epoch 154/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00154: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0752 - val_mean_absolute_error: 2.0752\n",
      "Epoch 155/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00155: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0699 - val_mean_absolute_error: 2.0699\n",
      "Epoch 156/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00156: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0710 - val_mean_absolute_error: 2.0710\n",
      "Epoch 157/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00157: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 158/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00158: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 159/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00159: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 160/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00160: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0787 - mean_absolute_error: 2.0787 - val_loss: 2.0763 - val_mean_absolute_error: 2.0763\n",
      "Epoch 161/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00161: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0739 - val_mean_absolute_error: 2.0739\n",
      "Epoch 162/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00162: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 163/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00163: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 164/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00164: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 165/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00165: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0901 - val_mean_absolute_error: 2.0901\n",
      "Epoch 166/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00166: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0809 - val_mean_absolute_error: 2.0809\n",
      "Epoch 167/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00167: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.1085 - val_mean_absolute_error: 2.1085\n",
      "Epoch 168/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00168: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 169/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00169: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.1047 - val_mean_absolute_error: 2.1047\n",
      "Epoch 170/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00170: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0906 - val_mean_absolute_error: 2.0906\n",
      "Epoch 171/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00171: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 172/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00172: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0809 - val_mean_absolute_error: 2.0809\n",
      "Epoch 173/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00173: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0791 - val_mean_absolute_error: 2.0791\n",
      "Epoch 174/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00174: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.1080 - val_mean_absolute_error: 2.1080\n",
      "Epoch 175/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00175: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.1454 - val_mean_absolute_error: 2.1454\n",
      "Epoch 176/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00176: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 177/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00177: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 178/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00178: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00179: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0783 - mean_absolute_error: 2.0783 - val_loss: 2.0730 - val_mean_absolute_error: 2.0730\n",
      "Epoch 180/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00180: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 181/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00181: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 182/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00182: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.1066 - val_mean_absolute_error: 2.1066\n",
      "Epoch 183/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0794 - mean_absolute_error: 2.0794\n",
      "Epoch 00183: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 184/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00184: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0921 - val_mean_absolute_error: 2.0921\n",
      "Epoch 185/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00185: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 186/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00186: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0968 - val_mean_absolute_error: 2.0968\n",
      "Epoch 187/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00187: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0873 - val_mean_absolute_error: 2.0873\n",
      "Epoch 188/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00188: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0696 - val_mean_absolute_error: 2.0696\n",
      "Epoch 189/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00189: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 190/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780- ETA: 1s - loss: 2.\n",
      "Epoch 00190: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 191/500\n",
      "27669/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00191: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0833 - val_mean_absolute_error: 2.0833\n",
      "Epoch 192/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00192: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.1049 - val_mean_absolute_error: 2.1049\n",
      "Epoch 193/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00193: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.1006 - val_mean_absolute_error: 2.1006\n",
      "Epoch 194/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00194: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0690 - val_mean_absolute_error: 2.0690\n",
      "Epoch 195/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00195: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 196/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00196: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.1020 - val_mean_absolute_error: 2.1020\n",
      "Epoch 197/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00197: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0792 - val_mean_absolute_error: 2.0792\n",
      "Epoch 198/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0767 - mean_absolute_error: 2.0767\n",
      "Epoch 00198: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0904 - val_mean_absolute_error: 2.0904\n",
      "Epoch 199/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00199: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 200/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00200: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0769 - val_mean_absolute_error: 2.0769\n",
      "Epoch 201/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00201: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
      "Epoch 202/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00202: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.1052 - val_mean_absolute_error: 2.1052\n",
      "Epoch 203/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00203: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0821 - val_mean_absolute_error: 2.0821\n",
      "Epoch 204/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00204: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0721 - val_mean_absolute_error: 2.0721\n",
      "Epoch 205/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00205: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.1106 - val_mean_absolute_error: 2.1106\n",
      "Epoch 206/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00206: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 207/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00207: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 208/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00208: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0878 - val_mean_absolute_error: 2.0878\n",
      "Epoch 209/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00209: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 210/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00210: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 211/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00211: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.0718 - val_mean_absolute_error: 2.0718\n",
      "Epoch 212/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00212: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 213/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00213: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0732 - val_mean_absolute_error: 2.0732\n",
      "Epoch 214/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00214: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.1056 - val_mean_absolute_error: 2.1056\n",
      "Epoch 215/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00215: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.1007 - val_mean_absolute_error: 2.1007\n",
      "Epoch 216/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00216: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 217/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00217: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 218/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00218: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0846 - val_mean_absolute_error: 2.0846\n",
      "Epoch 219/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00219: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 220/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00220: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0896 - val_mean_absolute_error: 2.0896\n",
      "Epoch 221/500\n",
      "27671/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00221: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0742 - val_mean_absolute_error: 2.0742\n",
      "Epoch 222/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00222: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 223/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00223: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0765 - val_mean_absolute_error: 2.0765\n",
      "Epoch 224/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00224: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0786 - val_mean_absolute_error: 2.0786\n",
      "Epoch 225/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00225: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 226/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00226: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 227/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00227: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 228/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00228: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1278 - val_mean_absolute_error: 2.1278\n",
      "Epoch 229/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00229: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0710 - val_mean_absolute_error: 2.0710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00230: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 231/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00231: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0711 - val_mean_absolute_error: 2.0711\n",
      "Epoch 232/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00232: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0975 - val_mean_absolute_error: 2.0975\n",
      "Epoch 233/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00233: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0906 - val_mean_absolute_error: 2.0906\n",
      "Epoch 234/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00234: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 235/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00235: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0842 - val_mean_absolute_error: 2.0842\n",
      "Epoch 236/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00236: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0872 - val_mean_absolute_error: 2.0872\n",
      "Epoch 237/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00237: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0802 - val_mean_absolute_error: 2.0802\n",
      "Epoch 238/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00238: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0705 - val_mean_absolute_error: 2.0705\n",
      "Epoch 239/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00239: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0734 - val_mean_absolute_error: 2.0734\n",
      "Epoch 240/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00240: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0783 - val_mean_absolute_error: 2.0783\n",
      "Epoch 241/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00241: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0939 - val_mean_absolute_error: 2.0939\n",
      "Epoch 242/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0767 - mean_absolute_error: 2.0767\n",
      "Epoch 00242: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 243/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00243: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
      "Epoch 244/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00244: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0848 - val_mean_absolute_error: 2.0848\n",
      "Epoch 245/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00245: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0895 - val_mean_absolute_error: 2.0895\n",
      "Epoch 246/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00246: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0729 - val_mean_absolute_error: 2.0729\n",
      "Epoch 247/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00247: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0810 - val_mean_absolute_error: 2.0810\n",
      "Epoch 248/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00248: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 249/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00249: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 250/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00250: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0947 - val_mean_absolute_error: 2.0947\n",
      "Epoch 251/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00251: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1004 - val_mean_absolute_error: 2.1004\n",
      "Epoch 252/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00252: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0777 - val_mean_absolute_error: 2.0777\n",
      "Epoch 253/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00253: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 254/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00254: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 255/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00255: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00256: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1094 - val_mean_absolute_error: 2.1094\n",
      "Epoch 257/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00257: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 258/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00258: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 259/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00259: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 260/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00260: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 261/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00261: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 262/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00262: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0701 - val_mean_absolute_error: 2.0701\n",
      "Epoch 263/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00263: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0752 - val_mean_absolute_error: 2.0752\n",
      "Epoch 264/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00264: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.1183 - val_mean_absolute_error: 2.1183\n",
      "Epoch 265/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00265: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0875 - val_mean_absolute_error: 2.0875\n",
      "Epoch 266/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00266: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0998 - val_mean_absolute_error: 2.0998\n",
      "Epoch 267/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00267: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0745 - val_mean_absolute_error: 2.0745\n",
      "Epoch 268/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00268: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1036 - val_mean_absolute_error: 2.1036\n",
      "Epoch 269/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00269: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.1232 - val_mean_absolute_error: 2.1232\n",
      "Epoch 270/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00270: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n",
      "Epoch 271/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00271: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0871 - val_mean_absolute_error: 2.0871\n",
      "Epoch 272/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00272: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0735 - val_mean_absolute_error: 2.0735\n",
      "Epoch 273/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00273: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0747 - mean_absolute_error: 2.0747 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 274/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00274: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0890 - val_mean_absolute_error: 2.0890\n",
      "Epoch 275/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00275: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0758 - val_mean_absolute_error: 2.0758\n",
      "Epoch 276/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00276: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.1024 - val_mean_absolute_error: 2.1024\n",
      "Epoch 277/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00277: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0955 - val_mean_absolute_error: 2.0955\n",
      "Epoch 278/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00278: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0849 - val_mean_absolute_error: 2.0849\n",
      "Epoch 279/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00279: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 280/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00280: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 281/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00281: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.1173 - val_mean_absolute_error: 2.1173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00282: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 283/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00283: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0755 - val_mean_absolute_error: 2.0755\n",
      "Epoch 284/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00284: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0921 - val_mean_absolute_error: 2.0921\n",
      "Epoch 285/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00285: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 286/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00286: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0899 - val_mean_absolute_error: 2.0899\n",
      "Epoch 287/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00287: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
      "Epoch 288/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00288: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0907 - val_mean_absolute_error: 2.0907\n",
      "Epoch 289/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00289: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0746 - val_mean_absolute_error: 2.0746\n",
      "Epoch 290/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00290: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 291/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00291: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 292/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00292: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0844 - val_mean_absolute_error: 2.0844\n",
      "Epoch 293/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00293: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0854 - val_mean_absolute_error: 2.0854\n",
      "Epoch 294/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00294: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 295/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00295: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 296/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00296: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 297/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00297: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0922 - val_mean_absolute_error: 2.0922\n",
      "Epoch 298/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00298: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0748 - val_mean_absolute_error: 2.0748\n",
      "Epoch 299/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00299: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0980 - val_mean_absolute_error: 2.0980\n",
      "Epoch 300/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00300: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0804 - val_mean_absolute_error: 2.0804\n",
      "Epoch 301/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00301: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.1788 - val_mean_absolute_error: 2.1788\n",
      "Epoch 302/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00302: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.1505 - val_mean_absolute_error: 2.1505\n",
      "Epoch 303/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00303: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0926 - val_mean_absolute_error: 2.0926\n",
      "Epoch 304/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00304: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0905 - val_mean_absolute_error: 2.0905\n",
      "Epoch 305/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00305: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 306/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00306: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0743 - val_mean_absolute_error: 2.0743\n",
      "Epoch 307/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00307: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0834 - val_mean_absolute_error: 2.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 308/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00308: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.0789 - val_mean_absolute_error: 2.0789\n",
      "Epoch 309/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00309: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.1291 - val_mean_absolute_error: 2.1291\n",
      "Epoch 310/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00310: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0813 - val_mean_absolute_error: 2.0813\n",
      "Epoch 311/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00311: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 312/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00312: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0952 - val_mean_absolute_error: 2.0952\n",
      "Epoch 313/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00313: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0855 - val_mean_absolute_error: 2.0855\n",
      "Epoch 314/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00314: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.0731 - val_mean_absolute_error: 2.0731\n",
      "Epoch 315/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00315: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0989 - val_mean_absolute_error: 2.0989\n",
      "Epoch 316/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00316: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0743 - mean_absolute_error: 2.0743 - val_loss: 2.0884 - val_mean_absolute_error: 2.0884\n",
      "Epoch 317/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00317: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
      "Epoch 318/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0758 - mean_absolute_error: 2.0758\n",
      "Epoch 00318: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 319/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00319: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 320/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00320: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0968 - val_mean_absolute_error: 2.0968\n",
      "Epoch 321/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00321: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0892 - val_mean_absolute_error: 2.0892\n",
      "Epoch 322/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00322: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.1138 - val_mean_absolute_error: 2.1138\n",
      "Epoch 323/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00323: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.1135 - val_mean_absolute_error: 2.1135\n",
      "Epoch 324/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00324: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0886 - val_mean_absolute_error: 2.0886\n",
      "Epoch 325/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0780 - mean_absolute_error: 2.0780\n",
      "Epoch 00325: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.0871 - val_mean_absolute_error: 2.0871\n",
      "Epoch 326/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00326: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 327/500\n",
      "27673/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00327: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 328/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00328: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0885 - val_mean_absolute_error: 2.0885\n",
      "Epoch 329/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00329: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.0816 - val_mean_absolute_error: 2.0816\n",
      "Epoch 330/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00330: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0775 - mean_absolute_error: 2.0775 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n",
      "Epoch 331/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00331: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0724 - val_mean_absolute_error: 2.0724\n",
      "Epoch 332/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00332: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 333/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00333: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0735 - val_mean_absolute_error: 2.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00334: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0943 - val_mean_absolute_error: 2.0943\n",
      "Epoch 335/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00335: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.1489 - val_mean_absolute_error: 2.1489\n",
      "Epoch 336/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00336: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 2.0891 - val_mean_absolute_error: 2.0891\n",
      "Epoch 337/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00337: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0840 - mean_absolute_error: 2.0840 - val_loss: 2.1090 - val_mean_absolute_error: 2.1090\n",
      "Epoch 338/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0839 - mean_absolute_error: 2.0839\n",
      "Epoch 00338: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0839 - mean_absolute_error: 2.0839 - val_loss: 2.0978 - val_mean_absolute_error: 2.0978\n",
      "Epoch 339/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0821 - mean_absolute_error: 2.0821\n",
      "Epoch 00339: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0820 - mean_absolute_error: 2.0820 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 340/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0820 - mean_absolute_error: 2.0820\n",
      "Epoch 00340: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0819 - mean_absolute_error: 2.0819 - val_loss: 2.0761 - val_mean_absolute_error: 2.0761\n",
      "Epoch 341/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00341: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0725 - val_mean_absolute_error: 2.0725\n",
      "Epoch 342/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00342: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.1031 - val_mean_absolute_error: 2.1031\n",
      "Epoch 343/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0732 - mean_absolute_error: 2.0732\n",
      "Epoch 00343: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0732 - mean_absolute_error: 2.0732 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
      "Epoch 344/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00344: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0745 - mean_absolute_error: 2.0745 - val_loss: 2.1001 - val_mean_absolute_error: 2.1001\n",
      "Epoch 345/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0732 - mean_absolute_error: 2.0732\n",
      "Epoch 00345: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0732 - mean_absolute_error: 2.0732 - val_loss: 2.0709 - val_mean_absolute_error: 2.0709\n",
      "Epoch 346/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0740 - mean_absolute_error: 2.0740\n",
      "Epoch 00346: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0756 - val_mean_absolute_error: 2.0756\n",
      "Epoch 347/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0735 - mean_absolute_error: 2.0735\n",
      "Epoch 00347: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0737 - mean_absolute_error: 2.0737 - val_loss: 2.0774 - val_mean_absolute_error: 2.0774\n",
      "Epoch 348/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00348: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0729 - mean_absolute_error: 2.0729 - val_loss: 2.0737 - val_mean_absolute_error: 2.0737\n",
      "Epoch 349/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00349: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0730 - mean_absolute_error: 2.0730 - val_loss: 2.0713 - val_mean_absolute_error: 2.0713\n",
      "Epoch 350/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0735 - mean_absolute_error: 2.0735\n",
      "Epoch 00350: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0734 - mean_absolute_error: 2.0734 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 351/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0733 - mean_absolute_error: 2.0733\n",
      "Epoch 00351: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0734 - mean_absolute_error: 2.0734 - val_loss: 2.0764 - val_mean_absolute_error: 2.0764\n",
      "Epoch 352/500\n",
      "27693/27712 [============================>.] - ETA: 0s - loss: 2.0727 - mean_absolute_error: 2.0727\n",
      "Epoch 00352: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0726 - mean_absolute_error: 2.0726 - val_loss: 2.0808 - val_mean_absolute_error: 2.0808\n",
      "Epoch 353/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00353: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0862 - val_mean_absolute_error: 2.0862\n",
      "Epoch 354/500\n",
      "27678/27712 [============================>.] - ETA: 0s - loss: 2.0736 - mean_absolute_error: 2.0736\n",
      "Epoch 00354: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0736 - mean_absolute_error: 2.0736 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
      "Epoch 355/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0733 - mean_absolute_error: 2.0733\n",
      "Epoch 00355: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0733 - mean_absolute_error: 2.0733 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 356/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0738 - mean_absolute_error: 2.0738\n",
      "Epoch 00356: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0736 - mean_absolute_error: 2.0736 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 357/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00357: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0730 - mean_absolute_error: 2.0730 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 358/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0727 - mean_absolute_error: 2.0727\n",
      "Epoch 00358: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0726 - mean_absolute_error: 2.0726 - val_loss: 2.0913 - val_mean_absolute_error: 2.0913\n",
      "Epoch 359/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00359: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0757 - mean_absolute_error: 2.0757 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 360/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0737 - mean_absolute_error: 2.0737\n",
      "Epoch 00360: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.0892 - val_mean_absolute_error: 2.0892\n",
      "Epoch 361/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0730 - mean_absolute_error: 2.0730\n",
      "Epoch 00361: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0733 - mean_absolute_error: 2.0733 - val_loss: 2.0692 - val_mean_absolute_error: 2.0692\n",
      "Epoch 362/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00362: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0876 - val_mean_absolute_error: 2.0876\n",
      "Epoch 363/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0883 - mean_absolute_error: 2.0883\n",
      "Epoch 00363: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0884 - mean_absolute_error: 2.0884 - val_loss: 2.0942 - val_mean_absolute_error: 2.0942\n",
      "Epoch 364/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0879 - mean_absolute_error: 2.0879\n",
      "Epoch 00364: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0879 - mean_absolute_error: 2.0879 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 365/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0927 - mean_absolute_error: 2.0927\n",
      "Epoch 00365: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0927 - mean_absolute_error: 2.0927 - val_loss: 2.0874 - val_mean_absolute_error: 2.0874\n",
      "Epoch 366/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0861 - mean_absolute_error: 2.0861\n",
      "Epoch 00366: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0859 - mean_absolute_error: 2.0859 - val_loss: 2.0762 - val_mean_absolute_error: 2.0762\n",
      "Epoch 367/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0811 - mean_absolute_error: 2.0811\n",
      "Epoch 00367: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0810 - mean_absolute_error: 2.0810 - val_loss: 2.0785 - val_mean_absolute_error: 2.0785\n",
      "Epoch 368/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00368: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.0855 - val_mean_absolute_error: 2.0855\n",
      "Epoch 369/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0776 - mean_absolute_error: 2.0776\n",
      "Epoch 00369: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0829 - val_mean_absolute_error: 2.0829\n",
      "Epoch 370/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0790 - mean_absolute_error: 2.0790\n",
      "Epoch 00370: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0789 - mean_absolute_error: 2.0789 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 371/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0771 - mean_absolute_error: 2.0771\n",
      "Epoch 00371: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0841 - val_mean_absolute_error: 2.0841\n",
      "Epoch 372/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00372: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0796 - mean_absolute_error: 2.0796 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
      "Epoch 373/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00373: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0954 - val_mean_absolute_error: 2.0954\n",
      "Epoch 374/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0788 - mean_absolute_error: 2.0788\n",
      "Epoch 00374: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 375/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00375: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0773 - mean_absolute_error: 2.0773 - val_loss: 2.1068 - val_mean_absolute_error: 2.1068\n",
      "Epoch 376/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0765 - mean_absolute_error: 2.0765\n",
      "Epoch 00376: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0832 - val_mean_absolute_error: 2.0832\n",
      "Epoch 377/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00377: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0811 - val_mean_absolute_error: 2.0811\n",
      "Epoch 378/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00378: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.0857 - val_mean_absolute_error: 2.0857\n",
      "Epoch 379/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0828 - mean_absolute_error: 2.0828\n",
      "Epoch 00379: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0828 - mean_absolute_error: 2.0828 - val_loss: 2.0867 - val_mean_absolute_error: 2.0867\n",
      "Epoch 380/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0847 - mean_absolute_error: 2.0847\n",
      "Epoch 00380: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0846 - mean_absolute_error: 2.0846 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 381/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0823 - mean_absolute_error: 2.0823\n",
      "Epoch 00381: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0823 - mean_absolute_error: 2.0823 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 382/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0762 - mean_absolute_error: 2.0762\n",
      "Epoch 00382: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0893 - val_mean_absolute_error: 2.0893\n",
      "Epoch 383/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00383: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.1413 - val_mean_absolute_error: 2.1413\n",
      "Epoch 384/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00384: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0727 - val_mean_absolute_error: 2.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00385: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0897 - val_mean_absolute_error: 2.0897\n",
      "Epoch 386/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0744 - mean_absolute_error: 2.0744\n",
      "Epoch 00386: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0743 - mean_absolute_error: 2.0743 - val_loss: 2.0850 - val_mean_absolute_error: 2.0850\n",
      "Epoch 387/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0755 - mean_absolute_error: 2.0755\n",
      "Epoch 00387: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0954 - val_mean_absolute_error: 2.0954\n",
      "Epoch 388/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00388: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.1260 - val_mean_absolute_error: 2.1260\n",
      "Epoch 389/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0791 - mean_absolute_error: 2.0791\n",
      "Epoch 00389: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.1178 - val_mean_absolute_error: 2.1178\n",
      "Epoch 390/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0800 - mean_absolute_error: 2.0800\n",
      "Epoch 00390: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0802 - mean_absolute_error: 2.0802 - val_loss: 2.0863 - val_mean_absolute_error: 2.0863\n",
      "Epoch 391/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00391: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0903 - val_mean_absolute_error: 2.0903\n",
      "Epoch 392/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0779 - mean_absolute_error: 2.0779\n",
      "Epoch 00392: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 393/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0801 - mean_absolute_error: 2.0801\n",
      "Epoch 00393: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0801 - mean_absolute_error: 2.0801 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n",
      "Epoch 394/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
      "Epoch 00394: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0981 - val_mean_absolute_error: 2.0981\n",
      "Epoch 395/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00395: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.1024 - val_mean_absolute_error: 2.1024\n",
      "Epoch 396/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00396: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0793 - val_mean_absolute_error: 2.0793\n",
      "Epoch 397/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00397: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0758 - mean_absolute_error: 2.0758 - val_loss: 2.0948 - val_mean_absolute_error: 2.0948\n",
      "Epoch 398/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00398: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.1461 - val_mean_absolute_error: 2.1461\n",
      "Epoch 399/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0785 - mean_absolute_error: 2.0785\n",
      "Epoch 00399: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0786 - mean_absolute_error: 2.0786 - val_loss: 2.0702 - val_mean_absolute_error: 2.0702\n",
      "Epoch 400/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0752 - mean_absolute_error: 2.0752\n",
      "Epoch 00400: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.1308 - val_mean_absolute_error: 2.1308\n",
      "Epoch 401/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00401: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0760 - mean_absolute_error: 2.0760 - val_loss: 2.0935 - val_mean_absolute_error: 2.0935\n",
      "Epoch 402/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00402: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 403/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00403: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.0846 - val_mean_absolute_error: 2.0846\n",
      "Epoch 404/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0738 - mean_absolute_error: 2.0738\n",
      "Epoch 00404: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.0744 - val_mean_absolute_error: 2.0744\n",
      "Epoch 405/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766\n",
      "Epoch 00405: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0766 - mean_absolute_error: 2.0766 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 406/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00406: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0770 - mean_absolute_error: 2.0770 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 407/500\n",
      "27672/27712 [============================>.] - ETA: 0s - loss: 2.0764 - mean_absolute_error: 2.0764\n",
      "Epoch 00407: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0765 - mean_absolute_error: 2.0765 - val_loss: 2.0934 - val_mean_absolute_error: 2.0934\n",
      "Epoch 408/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00408: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.1097 - val_mean_absolute_error: 2.1097\n",
      "Epoch 409/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0742 - mean_absolute_error: 2.0742\n",
      "Epoch 00409: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 410/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00410: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0723 - val_mean_absolute_error: 2.0723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/500\n",
      "27686/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00411: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.1044 - val_mean_absolute_error: 2.1044\n",
      "Epoch 412/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00412: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0726 - val_mean_absolute_error: 2.0726\n",
      "Epoch 413/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00413: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0808 - val_mean_absolute_error: 2.0808\n",
      "Epoch 414/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0753 - mean_absolute_error: 2.0753\n",
      "Epoch 00414: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 415/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0766 - mean_absolute_error: 2.0766- ETA: 1s - loss: 2.0768 - \n",
      "Epoch 00415: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0764 - mean_absolute_error: 2.0764 - val_loss: 2.0765 - val_mean_absolute_error: 2.0765\n",
      "Epoch 416/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0760 - mean_absolute_error: 2.0760\n",
      "Epoch 00416: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0759 - mean_absolute_error: 2.0759 - val_loss: 2.0749 - val_mean_absolute_error: 2.0749\n",
      "Epoch 417/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00417: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0716 - val_mean_absolute_error: 2.0716\n",
      "Epoch 418/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00418: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0818 - val_mean_absolute_error: 2.0818\n",
      "Epoch 419/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00419: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0766 - val_mean_absolute_error: 2.0766\n",
      "Epoch 420/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00420: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0742 - mean_absolute_error: 2.0742 - val_loss: 2.0747 - val_mean_absolute_error: 2.0747\n",
      "Epoch 421/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0737 - mean_absolute_error: 2.0737\n",
      "Epoch 00421: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0738 - mean_absolute_error: 2.0738 - val_loss: 2.1264 - val_mean_absolute_error: 2.1264\n",
      "Epoch 422/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00422: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0786 - val_mean_absolute_error: 2.0786\n",
      "Epoch 423/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00423: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0747 - mean_absolute_error: 2.0747 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
      "Epoch 424/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0741 - mean_absolute_error: 2.0741\n",
      "Epoch 00424: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0740 - mean_absolute_error: 2.0740 - val_loss: 2.0845 - val_mean_absolute_error: 2.0845\n",
      "Epoch 425/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0743 - mean_absolute_error: 2.0743\n",
      "Epoch 00425: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0741 - mean_absolute_error: 2.0741 - val_loss: 2.0725 - val_mean_absolute_error: 2.0725\n",
      "Epoch 426/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0740 - mean_absolute_error: 2.0740\n",
      "Epoch 00426: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.0932 - val_mean_absolute_error: 2.0932\n",
      "Epoch 427/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00427: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 428/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00428: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 429/500\n",
      "27692/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00429: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 430/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00430: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
      "Epoch 431/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00431: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.1013 - val_mean_absolute_error: 2.1013\n",
      "Epoch 432/500\n",
      "27700/27712 [============================>.] - ETA: 0s - loss: 2.0773 - mean_absolute_error: 2.0773\n",
      "Epoch 00432: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0877 - val_mean_absolute_error: 2.0877\n",
      "Epoch 433/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.0745\n",
      "Epoch 00433: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0744 - mean_absolute_error: 2.0744 - val_loss: 2.0738 - val_mean_absolute_error: 2.0738\n",
      "Epoch 434/500\n",
      "27676/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00434: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0801 - val_mean_absolute_error: 2.0801\n",
      "Epoch 435/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0750 - mean_absolute_error: 2.0750\n",
      "Epoch 00435: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0814 - val_mean_absolute_error: 2.0814\n",
      "Epoch 436/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00436: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 437/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00437: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0778 - val_mean_absolute_error: 2.0778\n",
      "Epoch 438/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0749 - mean_absolute_error: 2.0749\n",
      "Epoch 00438: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0728 - val_mean_absolute_error: 2.0728\n",
      "Epoch 439/500\n",
      "27699/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00439: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0881 - val_mean_absolute_error: 2.0881\n",
      "Epoch 440/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00440: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0794 - mean_absolute_error: 2.0794 - val_loss: 2.1149 - val_mean_absolute_error: 2.1149\n",
      "Epoch 441/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00441: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0894 - val_mean_absolute_error: 2.0894\n",
      "Epoch 442/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0795 - mean_absolute_error: 2.0795\n",
      "Epoch 00442: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0795 - mean_absolute_error: 2.0795 - val_loss: 2.1261 - val_mean_absolute_error: 2.1261\n",
      "Epoch 443/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00443: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 444/500\n",
      "27674/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00444: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0762 - mean_absolute_error: 2.0762 - val_loss: 2.0897 - val_mean_absolute_error: 2.0897\n",
      "Epoch 445/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00445: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 446/500\n",
      "27685/27712 [============================>.] - ETA: 0s - loss: 2.0786 - mean_absolute_error: 2.0786\n",
      "Epoch 00446: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.1037 - val_mean_absolute_error: 2.1037\n",
      "Epoch 447/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00447: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0782 - val_mean_absolute_error: 2.0782\n",
      "Epoch 448/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00448: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0868 - val_mean_absolute_error: 2.0868\n",
      "Epoch 449/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0748 - mean_absolute_error: 2.0748\n",
      "Epoch 00449: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0749 - mean_absolute_error: 2.0749 - val_loss: 2.0781 - val_mean_absolute_error: 2.0781\n",
      "Epoch 450/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00450: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0793 - mean_absolute_error: 2.0793 - val_loss: 2.1015 - val_mean_absolute_error: 2.1015\n",
      "Epoch 451/500\n",
      "27679/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00451: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0749 - val_mean_absolute_error: 2.0749\n",
      "Epoch 452/500\n",
      "27708/27712 [============================>.] - ETA: 0s - loss: 2.0756 - mean_absolute_error: 2.0756\n",
      "Epoch 00452: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0755 - mean_absolute_error: 2.0755 - val_loss: 2.0953 - val_mean_absolute_error: 2.0953\n",
      "Epoch 453/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00453: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0925 - val_mean_absolute_error: 2.0925\n",
      "Epoch 454/500\n",
      "27675/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00454: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 38s 1ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 455/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00455: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 39s 1ms/step - loss: 2.0767 - mean_absolute_error: 2.0767 - val_loss: 2.0751 - val_mean_absolute_error: 2.0751\n",
      "Epoch 456/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0772 - mean_absolute_error: 2.0772\n",
      "Epoch 00456: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0772 - mean_absolute_error: 2.0772 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 457/500\n",
      "27698/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00457: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0752 - mean_absolute_error: 2.0752 - val_loss: 2.0961 - val_mean_absolute_error: 2.0961\n",
      "Epoch 458/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00458: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0827 - mean_absolute_error: 2.0827 - val_loss: 2.0825 - val_mean_absolute_error: 2.0825\n",
      "Epoch 459/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00459: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0836 - mean_absolute_error: 2.0836 - val_loss: 2.0918 - val_mean_absolute_error: 2.0918\n",
      "Epoch 460/500\n",
      "27680/27712 [============================>.] - ETA: 0s - loss: 2.0834 - mean_absolute_error: 2.0834\n",
      "Epoch 00460: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 2.0969 - val_mean_absolute_error: 2.0969\n",
      "Epoch 461/500\n",
      "27689/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00461: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0808 - mean_absolute_error: 2.0808 - val_loss: 2.0794 - val_mean_absolute_error: 2.0794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0816 - mean_absolute_error: 2.0816\n",
      "Epoch 00462: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 1ms/step - loss: 2.0816 - mean_absolute_error: 2.0816 - val_loss: 2.0936 - val_mean_absolute_error: 2.0936\n",
      "Epoch 463/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0826 - mean_absolute_error: 2.0826\n",
      "Epoch 00463: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0826 - mean_absolute_error: 2.0826 - val_loss: 2.0822 - val_mean_absolute_error: 2.0822\n",
      "Epoch 464/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00464: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0756 - val_mean_absolute_error: 2.0756\n",
      "Epoch 465/500\n",
      "27684/27712 [============================>.] - ETA: 0s - loss: 2.0809 - mean_absolute_error: 2.0809\n",
      "Epoch 00465: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 42s 2ms/step - loss: 2.0809 - mean_absolute_error: 2.0809 - val_loss: 2.0882 - val_mean_absolute_error: 2.0882\n",
      "Epoch 466/500\n",
      "27703/27712 [============================>.] - ETA: 0s - loss: 2.0804 - mean_absolute_error: 2.0804\n",
      "Epoch 00466: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0803 - mean_absolute_error: 2.0803 - val_loss: 2.0865 - val_mean_absolute_error: 2.0865\n",
      "Epoch 467/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00467: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0722 - val_mean_absolute_error: 2.0722\n",
      "Epoch 468/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0775 - mean_absolute_error: 2.0775\n",
      "Epoch 00468: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 43s 2ms/step - loss: 2.0777 - mean_absolute_error: 2.0777 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
      "Epoch 469/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0806 - mean_absolute_error: 2.0806\n",
      "Epoch 00469: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0805 - mean_absolute_error: 2.0805 - val_loss: 2.0833 - val_mean_absolute_error: 2.0833\n",
      "Epoch 470/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0792 - mean_absolute_error: 2.0792\n",
      "Epoch 00470: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0791 - mean_absolute_error: 2.0791 - val_loss: 2.0795 - val_mean_absolute_error: 2.0795\n",
      "Epoch 471/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0781 - mean_absolute_error: 2.0781\n",
      "Epoch 00471: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0767 - val_mean_absolute_error: 2.0767\n",
      "Epoch 472/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00472: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0759 - val_mean_absolute_error: 2.0759\n",
      "Epoch 473/500\n",
      "27683/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00473: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.1079 - val_mean_absolute_error: 2.1079\n",
      "Epoch 474/500\n",
      "27702/27712 [============================>.] - ETA: 0s - loss: 2.0805 - mean_absolute_error: 2.0805\n",
      "Epoch 00474: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0804 - mean_absolute_error: 2.0804 - val_loss: 2.0820 - val_mean_absolute_error: 2.0820\n",
      "Epoch 475/500\n",
      "27707/27712 [============================>.] - ETA: 0s - loss: 2.0829 - mean_absolute_error: 2.0829\n",
      "Epoch 00475: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0830 - mean_absolute_error: 2.0830 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
      "Epoch 476/500\n",
      "27696/27712 [============================>.] - ETA: 0s - loss: 2.0796 - mean_absolute_error: 2.0796\n",
      "Epoch 00476: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0797 - mean_absolute_error: 2.0797 - val_loss: 2.0851 - val_mean_absolute_error: 2.0851\n",
      "Epoch 477/500\n",
      "27709/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00477: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 478/500\n",
      "27697/27712 [============================>.] - ETA: 0s - loss: 2.0761 - mean_absolute_error: 2.0761\n",
      "Epoch 00478: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0761 - mean_absolute_error: 2.0761 - val_loss: 2.0812 - val_mean_absolute_error: 2.0812\n",
      "Epoch 479/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0769 - mean_absolute_error: 2.0769\n",
      "Epoch 00479: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.1298 - val_mean_absolute_error: 2.1298\n",
      "Epoch 480/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00480: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0753 - mean_absolute_error: 2.0753 - val_loss: 2.0773 - val_mean_absolute_error: 2.0773\n",
      "Epoch 481/500\n",
      "27681/27712 [============================>.] - ETA: 0s - loss: 2.0770 - mean_absolute_error: 2.0770\n",
      "Epoch 00481: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0769 - mean_absolute_error: 2.0769 - val_loss: 2.0893 - val_mean_absolute_error: 2.0893\n",
      "Epoch 482/500\n",
      "27704/27712 [============================>.] - ETA: 0s - loss: 2.0763 - mean_absolute_error: 2.0763\n",
      "Epoch 00482: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0763 - mean_absolute_error: 2.0763 - val_loss: 2.0823 - val_mean_absolute_error: 2.0823\n",
      "Epoch 483/500\n",
      "27710/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00483: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0784 - mean_absolute_error: 2.0784 - val_loss: 2.1137 - val_mean_absolute_error: 2.1137\n",
      "Epoch 484/500\n",
      "27705/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00484: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 45s 2ms/step - loss: 2.0778 - mean_absolute_error: 2.0778 - val_loss: 2.0739 - val_mean_absolute_error: 2.0739\n",
      "Epoch 485/500\n",
      "27695/27712 [============================>.] - ETA: 0s - loss: 2.0754 - mean_absolute_error: 2.0754\n",
      "Epoch 00485: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0754 - mean_absolute_error: 2.0754 - val_loss: 2.0859 - val_mean_absolute_error: 2.0859\n",
      "Epoch 486/500\n",
      "27712/27712 [==============================] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00486: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0768 - mean_absolute_error: 2.0768 - val_loss: 2.1217 - val_mean_absolute_error: 2.1217\n",
      "Epoch 487/500\n",
      "27682/27712 [============================>.] - ETA: 0s - loss: 2.0783 - mean_absolute_error: 2.0783\n",
      "Epoch 00487: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0830 - val_mean_absolute_error: 2.0830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0777 - mean_absolute_error: 2.0777\n",
      "Epoch 00488: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0779 - mean_absolute_error: 2.0779 - val_loss: 2.1058 - val_mean_absolute_error: 2.1058\n",
      "Epoch 489/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0782 - mean_absolute_error: 2.0782\n",
      "Epoch 00489: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0781 - mean_absolute_error: 2.0781 - val_loss: 2.0799 - val_mean_absolute_error: 2.0799\n",
      "Epoch 490/500\n",
      "27691/27712 [============================>.] - ETA: 0s - loss: 2.0813 - mean_absolute_error: 2.0813\n",
      "Epoch 00490: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 46s 2ms/step - loss: 2.0813 - mean_absolute_error: 2.0813 - val_loss: 2.1054 - val_mean_absolute_error: 2.1054\n",
      "Epoch 491/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0784 - mean_absolute_error: 2.0784\n",
      "Epoch 00491: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 47s 2ms/step - loss: 2.0785 - mean_absolute_error: 2.0785 - val_loss: 2.0861 - val_mean_absolute_error: 2.0861\n",
      "Epoch 492/500\n",
      "27711/27712 [============================>.] - ETA: 0s - loss: 2.0774 - mean_absolute_error: 2.0774\n",
      "Epoch 00492: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 47s 2ms/step - loss: 2.0774 - mean_absolute_error: 2.0774 - val_loss: 2.0996 - val_mean_absolute_error: 2.0996\n",
      "Epoch 493/500\n",
      "27701/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00493: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 44s 2ms/step - loss: 2.0750 - mean_absolute_error: 2.0750 - val_loss: 2.0847 - val_mean_absolute_error: 2.0847\n",
      "Epoch 494/500\n",
      "27690/27712 [============================>.] - ETA: 0s - loss: 2.0778 - mean_absolute_error: 2.0778\n",
      "Epoch 00494: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0776 - mean_absolute_error: 2.0776 - val_loss: 2.0741 - val_mean_absolute_error: 2.0741\n",
      "Epoch 495/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0768 - mean_absolute_error: 2.0768\n",
      "Epoch 00495: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0771 - mean_absolute_error: 2.0771 - val_loss: 2.0757 - val_mean_absolute_error: 2.0757\n",
      "Epoch 496/500\n",
      "27694/27712 [============================>.] - ETA: 0s - loss: 2.0751 - mean_absolute_error: 2.0751\n",
      "Epoch 00496: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 41s 1ms/step - loss: 2.0751 - mean_absolute_error: 2.0751 - val_loss: 2.0932 - val_mean_absolute_error: 2.0932\n",
      "Epoch 497/500\n",
      "27687/27712 [============================>.] - ETA: 0s - loss: 2.0747 - mean_absolute_error: 2.0747\n",
      "Epoch 00497: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0748 - mean_absolute_error: 2.0748 - val_loss: 2.0779 - val_mean_absolute_error: 2.0779\n",
      "Epoch 498/500\n",
      "27706/27712 [============================>.] - ETA: 0s - loss: 2.0746 - mean_absolute_error: 2.0746\n",
      "Epoch 00498: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0746 - mean_absolute_error: 2.0746 - val_loss: 2.0828 - val_mean_absolute_error: 2.0828\n",
      "Epoch 499/500\n",
      "27688/27712 [============================>.] - ETA: 0s - loss: 2.0757 - mean_absolute_error: 2.0757\n",
      "Epoch 00499: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0756 - mean_absolute_error: 2.0756 - val_loss: 2.0790 - val_mean_absolute_error: 2.0790\n",
      "Epoch 500/500\n",
      "27677/27712 [============================>.] - ETA: 0s - loss: 2.0739 - mean_absolute_error: 2.0739\n",
      "Epoch 00500: val_loss did not improve from 2.06861\n",
      "27712/27712 [==============================] - 40s 1ms/step - loss: 2.0742 - mean_absolute_error: 2.0742 - val_loss: 2.0784 - val_mean_absolute_error: 2.0784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dd473be490>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'Weights-076--2.06861.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.418053 ],\n",
       "       [10.769339 ],\n",
       "       [ 4.0847797],\n",
       "       ...,\n",
       "       [55.06902  ],\n",
       "       [22.621426 ],\n",
       "       [ 6.283396 ]], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def knn_model(x_train, x_val, y_train, y_val, neighbors):\n",
    "    min_rmse = 1000\n",
    "    for n in neighbors:\n",
    "        knn = KNeighborsRegressor(n_neighbors=n)\n",
    "        knn.fit(x_train, y_train)\n",
    "        pred = knn.predict(x_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        if rmse < min_rmse:\n",
    "            min_rmse = rmse\n",
    "            model = knn\n",
    "            best_pred = pred\n",
    "        print('Neighbours', n, 'RMSE', rmse)\n",
    "    return model, min_rmse, best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbours 18 RMSE 5.1377800001928735\n",
      "Neighbours 24 RMSE 5.12771491074715\n",
      "Neighbours 30 RMSE 5.127487618943662\n",
      "Neighbours 40 RMSE 5.130315314887339\n"
     ]
    }
   ],
   "source": [
    "k_choices = [18,24,30,40]\n",
    "knn_final_model, knn_final_rmse, knn_final_pred = knn_model(X_train, X_val, y_train, y_val, k_choices)\n",
    "knn_test_pred = knn_final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
